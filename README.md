# Quant-GRU-PyTorch

ä¸€ä¸ªé«˜æ€§èƒ½çš„é‡åŒ– GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰å®ç°ï¼ŒåŸºäº CUDA å’Œ PyTorchï¼Œæ”¯æŒè®­ç»ƒå’Œæ¨ç†çš„é‡åŒ–æ„ŸçŸ¥è®¡ç®—ã€‚

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªæ”¯æŒé‡åŒ–çš„ GRU ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œæ ¸å¿ƒä½¿ç”¨ CUDA ç¼–å†™ä»¥å®ç°é«˜æ€§èƒ½è®¡ç®—ï¼Œå¹¶é€šè¿‡ PyBind11 æä¾› PyTorch æ¥å£ã€‚é¡¹ç›®æ”¯æŒï¼š

- **æµ®ç‚¹å’Œé‡åŒ–ä¸¤ç§æ¨¡å¼**ï¼šå¯åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶è‡ªç”±åˆ‡æ¢
- **çµæ´»çš„é‡åŒ–é…ç½®**ï¼šæ”¯æŒ 8/16 ä½é‡åŒ–ï¼Œå¯é…ç½®å¯¹ç§°/éå¯¹ç§°é‡åŒ–
- **ä¸¤ç§æ ¡å‡†æ–¹æ³•**ï¼šHistogramï¼ˆAIMET é£æ ¼ï¼Œé»˜è®¤ï¼Œé«˜ç²¾åº¦ï¼‰å’Œ MinMaxï¼ˆå¿«é€Ÿï¼‰
- **åŒå‘ GRU**ï¼šå®Œæ•´æ”¯æŒ bidirectional æ¨¡å¼
- **ä¸ PyTorch å…¼å®¹**ï¼š`QuantGRU` æ¥å£ä¸ `nn.GRU` ä¸€è‡´ï¼Œå¯æ— ç¼æ›¿æ¢
- **ONNX å¯¼å‡º**ï¼šæ”¯æŒ QDQ æ ¼å¼å¯¼å‡ºï¼Œä¾¿äºéƒ¨ç½²åˆ°å„ç±»æ¨ç†å¼•æ“

## ğŸ”§ ç¯å¢ƒè¦æ±‚

- **Python** >= 3.9
- **PyTorch** >= 2.0ï¼ˆæ”¯æŒ CUDAï¼‰
- **CUDA Toolkit** >= 11.0ï¼ˆå« cuBLASï¼‰
- **C++17** ç¼–è¯‘å™¨ï¼ˆGCC 7+ æˆ– Clang 5+ï¼‰
- **CMake** >= 3.18
- **OpenMP**

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¼–è¯‘ C++ åº“

```bash
# åˆ›å»ºæ„å»ºç›®å½•
mkdir build && cd build

# é…ç½® CMake
cmake ..

# ç¼–è¯‘
make -j$(nproc)
```

ç¼–è¯‘å®Œæˆåä¼šç”Ÿæˆï¼š
- `pytorch/lib/libgru_quant_static.a` - é™æ€åº“
- `pytorch/lib/libgru_quant_shared.so` - åŠ¨æ€åº“
- `gru_example` - C++ ç¤ºä¾‹ç¨‹åº

### 2. ç¼–è¯‘ Python æ‰©å±•

```bash
cd pytorch

# å®‰è£… Python æ‰©å±•ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
pip install -e .
```

### 3. éªŒè¯å®‰è£…

```bash
# C++ æµ‹è¯•
./build/gru_example

# Python æµ‹è¯•
cd pytorch
python test_quant_gru.py
```

---

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨ï¼ˆæµ®ç‚¹æ¨¡å¼ï¼‰

```python
from quant_gru import QuantGRU
import torch

# åˆ›å»ºæ¨¡å‹ï¼ˆä¸ nn.GRU æ¥å£ä¸€è‡´ï¼‰
gru = QuantGRU(
    input_size=64,
    hidden_size=128,
    batch_first=True,
    bidirectional=False
).cuda()

# å‰å‘ä¼ æ’­
input_data = torch.randn(32, 50, 64).cuda()  # [batch, seq_len, input_size]
output, h_n = gru(input_data)
# output: [32, 50, 128], h_n: [1, 32, 128]
```

### é‡åŒ–æ¨ç†

```python
from quant_gru import QuantGRU
import torch

# 1. åˆ›å»ºæ¨¡å‹
gru = QuantGRU(
    input_size=64,
    hidden_size=128,
    batch_first=True
).cuda()

# 2. åŠ è½½ä½å®½é…ç½®ï¼ˆäºŒé€‰ä¸€ï¼‰
# æ–¹å¼ä¸€ï¼šä»é…ç½®æ–‡ä»¶åŠ è½½
gru.load_bitwidth_config("pytorch/config/gru_quant_bitwidth_config.json", verbose=True)
# æ–¹å¼äºŒï¼šç›´æ¥è®¾ç½®ç»Ÿä¸€ä½å®½ï¼ˆ8/16ä½ï¼Œis_symmetricæ§åˆ¶å¯¹ç§°é‡åŒ–ï¼‰
# gru.set_all_bitwidth(bitwidth=8, is_symmetric=True, verbose=True)
# gru.use_quantization = True  # å¯ç”¨é‡åŒ–

# 3. ä½¿ç”¨æ ¡å‡†æ•°æ®è¿›è¡Œé‡åŒ–æ ¡å‡†
for batch in calibration_loader:
    gru.calibrate(batch.cuda())

# 4. æ¨ç†ï¼ˆé¦–æ¬¡å‰å‘æ—¶ä¼šè‡ªåŠ¨å®Œæˆæ ¡å‡†ï¼‰
output, h_n = gru(input_data)
```

> ğŸ’¡ **é‡åŒ–å¼€å…³**ï¼šé…ç½®æ–‡ä»¶ä¸­çš„ `disable_quantization` æ§åˆ¶æ˜¯å¦å¯ç”¨é‡åŒ–ï¼š
> - `false`ï¼ˆé»˜è®¤ï¼‰ï¼šå¯ç”¨é‡åŒ–æ¨ç†
> - `true`ï¼šä½¿ç”¨æµ®ç‚¹æ¨ç†

### é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)

```python
from quant_gru import QuantGRU
import torch

gru = QuantGRU(input_size=64, hidden_size=128, batch_first=True).cuda()
gru.load_bitwidth_config("pytorch/config/gru_quant_bitwidth_config.json")

# æ ¡å‡†
for batch in calibration_loader:
    gru.calibrate(batch.cuda())

# è®­ç»ƒå¾ªç¯ï¼ˆå‰å‘ä½¿ç”¨é‡åŒ–ï¼Œåå‘ä½¿ç”¨æµ®ç‚¹ï¼‰
optimizer = torch.optim.Adam(gru.parameters(), lr=0.001)
criterion = torch.nn.MSELoss()
gru.train()

for epoch in range(num_epochs):
    for x, target in train_loader:
        optimizer.zero_grad()
        output, _ = gru(x.cuda())
        loss = criterion(output, target.cuda())
        loss.backward()
        optimizer.step()
```

### æ ¡å‡†æ–¹æ³•é€‰æ‹©

```python
# AIMET é£æ ¼ç›´æ–¹å›¾æ ¡å‡†ï¼ˆé»˜è®¤ï¼Œç²¾åº¦é«˜ï¼Œæ¨èç”¨äºç”Ÿäº§éƒ¨ç½²ï¼‰
gru.calibration_method = 'histogram'

# MinMax æ ¡å‡†ï¼ˆé€Ÿåº¦å¿«ï¼Œé€‚åˆå¿«é€ŸåŸå‹éªŒè¯ï¼‰
gru.calibration_method = 'minmax'
```

### ONNX å¯¼å‡º

#### æµ®ç‚¹æ¨¡å‹å¯¼å‡ºï¼ˆé»˜è®¤ï¼‰

```python
from quant_gru import QuantGRU
import torch

gru = QuantGRU(input_size=64, hidden_size=128, batch_first=True).cuda()
gru.eval()

# å¯ç”¨å¯¼å‡ºæ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨æµ®ç‚¹æ ¼å¼ï¼‰
gru.export_mode = True

# å¯¼å‡º ONNX
dummy_input = torch.randn(1, 50, 64).cuda()
torch.onnx.export(
    gru, dummy_input, "gru_float.onnx",
    input_names=['input'],
    output_names=['output', 'hidden'],
    dynamic_axes={'input': {0: 'batch', 1: 'seq_len'},
                  'output': {0: 'batch', 1: 'seq_len'}},
    dynamo=False  # PyTorch 2.x éœ€è¦æ­¤å‚æ•°ä½¿ç”¨ä¼ ç»Ÿå¯¼å‡º
)

gru.export_mode = False  # æ¢å¤ CUDA æ¨¡å¼
```

#### é‡åŒ–æ¨¡å‹å¯¼å‡ºï¼ˆQDQ æ ¼å¼ï¼‰

```python
# 1. åˆ›å»ºå¹¶æ ¡å‡†æ¨¡å‹
gru = QuantGRU(input_size=64, hidden_size=128, batch_first=True).cuda()
gru.load_bitwidth_config("pytorch/config/gru_quant_bitwidth_config.json")

for batch in calibration_loader:
    gru.calibrate(batch.cuda())
gru.finalize_calibration()

# 2. å¯ç”¨å¯¼å‡ºæ¨¡å¼ï¼ŒæŒ‡å®š QDQ æ ¼å¼
gru.export_mode = True
gru.export_format = 'qdq'  # ä½¿ç”¨ QDQ ä¼ªé‡åŒ–æ ¼å¼
gru.eval()

# 3. å¯¼å‡º ONNX
torch.onnx.export(
    gru, dummy_input, "gru_quantized.onnx",
    input_names=['input'],
    output_names=['output', 'hidden'],
    dynamic_axes={'input': {0: 'batch', 1: 'seq_len'},
                  'output': {0: 'batch', 1: 'seq_len'}},
    dynamo=False
)

gru.export_mode = False  # æ¢å¤ CUDA æ¨¡å¼
```

> ğŸ’¡ **å¯¼å‡ºæ ¼å¼è¯´æ˜**ï¼š
> | `export_format` | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
> |-----------------|------|----------|
> | `'float'` | æµ®ç‚¹æ ¼å¼ï¼ˆé»˜è®¤ï¼‰ | éé‡åŒ–æ¨¡å‹éƒ¨ç½² |
> | `'qdq'` | QDQ ä¼ªé‡åŒ–æ ¼å¼ | é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼Œæ¨ç†å¼•æ“è‡ªåŠ¨ä¼˜åŒ– |

> ğŸ’¡ **æç¤º**ï¼šæ›´å¤šè¯¦ç»†ç¤ºä¾‹è¯·å‚é˜… `pytorch/example/example_usage.py`

## âš™ï¸ é‡åŒ–é…ç½®

### é‡åŒ–ä½å®½é…ç½®æ–‡ä»¶æ ¼å¼

é…ç½®æ–‡ä»¶ `pytorch/config/gru_quant_bitwidth_config.json`ï¼š

```json
{
  "GRU_config": {
    "default_config": {
      "disable_quantization": false
    },
    "operator_config": {
      "input.x": { "bitwidth": 8, "is_symmetric": false },
      "input.h": { "bitwidth": 8, "is_symmetric": false },
      "weight.W": { "bitwidth": 8, "is_symmetric": true },
      "weight.R": { "bitwidth": 8, "is_symmetric": true },
      "gate.z_out": { "bitwidth": 8, "is_symmetric": false },
      ...
    }
  }
}
```

### å¯é…ç½®çš„ç®—å­

| ç±»åˆ« | ç®—å­å | è¯´æ˜ |
|------|--------|------|
| è¾“å…¥ | `input.x`, `input.h` | è¾“å…¥åºåˆ—å’Œéšè—çŠ¶æ€ |
| æƒé‡ | `weight.W`, `weight.R`, `weight.bx`, `weight.br` | æƒé‡çŸ©é˜µå’Œåç½® |
| çŸ©é˜µä¹˜æ³• | `matmul.Wx`, `matmul.Rh` | çŸ©é˜µä¹˜æ³•ä¸­é—´ç»“æœ |
| é—¨æ§ | `gate.z_pre/out`, `gate.r_pre/out`, `gate.g_pre/out` | é—¨æ§æ¿€æ´»å‰å |
| è¿ç®— | `op.Rh_add_br`, `op.rRh`, `op.old_contrib`, `op.new_contrib` | ä¸­é—´è¿ç®— |

### å¿«é€Ÿè®¾ç½®æ‰€æœ‰ä½å®½

```python
# è®¾ç½®æ‰€æœ‰ç®—å­ä½¿ç”¨ 8bit å¯¹ç§°é‡åŒ–
gru.set_all_bitwidth(8, is_symmetric=True)

# è®¾ç½®æ‰€æœ‰ç®—å­ä½¿ç”¨ 16bit éå¯¹ç§°é‡åŒ–
gru.set_all_bitwidth(16, is_symmetric=False)
```

## ğŸ“ GRU å…¬å¼

æœ¬é¡¹ç›®å®ç°çš„ GRU éµå¾ªä»¥ä¸‹è®¡ç®—å…¬å¼ï¼š

```
z_t = Ïƒ(W_z Â· x_t + R_z Â· h_{t-1} + b_z)        # æ›´æ–°é—¨
r_t = Ïƒ(W_r Â· x_t + R_r Â· h_{t-1} + b_r)        # é‡ç½®é—¨
g_t = tanh(W_g Â· x_t + r_t âŠ™ (R_g Â· h_{t-1}) + b_g)  # å€™é€‰éšè—çŠ¶æ€
h_t = z_t âŠ™ h_{t-1} + (1 - z_t) âŠ™ g_t          # æ–°éšè—çŠ¶æ€
```

å…¶ä¸­ï¼š
- `Ïƒ` è¡¨ç¤º Sigmoid æ¿€æ´»å‡½æ•°
- `âŠ™` è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•

## ğŸ”¬ æ ¡å‡†æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **Histogram (AIMET)** â­ é»˜è®¤ | ç²¾åº¦é«˜ï¼ŒSQNR ä¼˜åŒ– | è®¡ç®—å¼€é”€ç¨å¤§ | ç”Ÿäº§éƒ¨ç½² |
| **MinMax** | é€Ÿåº¦å¿«ï¼Œå®ç°ç®€å• | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ | å¿«é€ŸåŸå‹éªŒè¯ |

## ğŸ“¦ ONNX å¯¼å‡º

### å¯¼å‡ºæ¨¡å¼

`QuantGRU` æ”¯æŒé€šè¿‡ `export_mode` å±æ€§åˆ‡æ¢åˆ°çº¯ PyTorch å®ç°ï¼Œä»¥ä¾¿ ONNX è¿½è¸ªï¼š

| å±æ€§ | è¯´æ˜ |
|------|------|
| `export_mode=False` | **é»˜è®¤**ï¼Œä½¿ç”¨ CUDA C++ å®ç°ï¼ˆé«˜æ€§èƒ½æ¨ç†ï¼‰ |
| `export_mode=True` | ä½¿ç”¨çº¯ PyTorch å®ç°ï¼ˆå¯è¢« ONNX è¿½è¸ªï¼‰ |

### å¯¼å‡ºæ ¼å¼é€‰æ‹©

é€šè¿‡ `export_format` å±æ€§è®¾ç½®å…·ä½“çš„å¯¼å‡ºæ ¼å¼ï¼š

| æ ¼å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| `'float'` | **é»˜è®¤**ï¼Œæµ®ç‚¹æ ¼å¼ | éé‡åŒ–æ¨¡å‹éƒ¨ç½²ã€ä¸ Haste GRU è¡Œä¸ºä¸€è‡´ |
| `'qdq'` | QDQï¼ˆQuantize-Dequantizeï¼‰æ ¼å¼ | é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼ˆTensorRTã€ONNX Runtimeï¼‰ |

```python
# è®¾ç½®å¯¼å‡ºæ ¼å¼
gru.export_format = 'float'      # é»˜è®¤ï¼Œæµ®ç‚¹
gru.export_format = 'qdq'        # é‡åŒ–æ¨¡å‹æ¨è
```

## ğŸ“ API å‚è€ƒ

### QuantGRU ç±»

```python
class QuantGRU(nn.Module):
    def __init__(
        self,
        input_size: int,              # è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_size: int,             # éšè—çŠ¶æ€ç»´åº¦
        num_layers: int = 1,          # å±‚æ•°ï¼ˆç›®å‰ä»…æ”¯æŒ 1ï¼‰
        bias: bool = True,            # æ˜¯å¦ä½¿ç”¨åç½®
        batch_first: bool = False,    # è¾“å…¥æ ¼å¼
        bidirectional: bool = False,  # æ˜¯å¦åŒå‘
        use_quantization: bool = False  # æ˜¯å¦å¯ç”¨é‡åŒ–
    )
    
    # é‡è¦å±æ€§ï¼ˆå¯åœ¨åˆ›å»ºåè®¾ç½®ï¼‰
    gru.export_mode = True           # ONNX å¯¼å‡ºæ¨¡å¼
    gru.calibration_method = 'histogram'  # æ ¡å‡†æ–¹æ³•ï¼ˆé»˜è®¤ï¼‰
```

### ä¸»è¦å±æ€§

| å±æ€§ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `use_quantization` | bool | False | æ˜¯å¦å¯ç”¨é‡åŒ–æ¨ç† |
| `export_mode` | bool | False | ONNX å¯¼å‡ºæ¨¡å¼ï¼ˆTrue æ—¶ä½¿ç”¨çº¯ PyTorch å®ç°ï¼‰ |
| `export_format` | str | 'float' | å¯¼å‡ºæ ¼å¼ï¼š'float'ã€'qdq' |
| `calibration_method` | str | 'histogram' | æ ¡å‡†æ–¹æ³•ï¼š'histogram'ï¼ˆé«˜ç²¾åº¦ï¼‰æˆ– 'minmax'ï¼ˆå¿«é€Ÿï¼‰ |

### ä¸»è¦æ–¹æ³•

| æ–¹æ³• | è¯´æ˜ |
|------|------|
| `forward(input, hx=None)` | å‰å‘ä¼ æ’­ï¼ˆé‡åŒ–æ¨¡å¼ä¸‹ä¼šè‡ªåŠ¨å®Œæˆæ ¡å‡†ï¼‰ |
| `calibrate(data)` | ç´¯ç§¯æ ¡å‡†æ•°æ® |
| `finalize_calibration(verbose=False)` | æ‰‹åŠ¨å®Œæˆæ ¡å‡†ï¼ˆé€šå¸¸æ— éœ€è°ƒç”¨ï¼Œforward ä¼šè‡ªåŠ¨å¤„ç†ï¼‰ |
| `reset_calibration()` | é‡ç½®æ ¡å‡†çŠ¶æ€ |
| `load_bitwidth_config(path, verbose=False)` | åŠ è½½ä½å®½é…ç½® |
| `set_all_bitwidth(bitwidth, is_symmetric=True)` | è®¾ç½®ç»Ÿä¸€ä½å®½ |
| `is_calibrated()` | æ£€æŸ¥æ˜¯å¦å·²æ ¡å‡† |

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
quant-gru-pytorch/
â”œâ”€â”€ include/                    # C++/CUDA å¤´æ–‡ä»¶
â”‚   â”œâ”€â”€ gru.h                   # æµ®ç‚¹ GRU å‰å‘/åå‘ä¼ æ’­ç±»
â”‚   â”œâ”€â”€ gru_quant.h             # é‡åŒ– GRU å‰å‘ä¼ æ’­ç±»
â”‚   â”œâ”€â”€ gru_interface.hpp       # ç»Ÿä¸€æ¥å£å±‚ï¼ˆæ ¡å‡†ã€é‡åŒ–ã€å‰å‘ä¼ æ’­ï¼‰
â”‚   â”œâ”€â”€ quantize_bitwidth_config.hpp  # é‡åŒ–ä½å®½é…ç½®
â”‚   â”œâ”€â”€ quantize_ops.cuh        # é‡åŒ–æ“ä½œ CUDA å†…æ ¸
â”‚   â”œâ”€â”€ histogram_collector.hpp # ç›´æ–¹å›¾æ”¶é›†å™¨ï¼ˆAIMET é£æ ¼æ ¡å‡†ï¼‰
â”‚   â”œâ”€â”€ pot_sqnr_calibrator.hpp # SQNR æ ¡å‡†å™¨
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/                        # C++/CUDA æºæ–‡ä»¶
â”‚   â”œâ”€â”€ gru_forward_gpu.cu      # æµ®ç‚¹å‰å‘ä¼ æ’­ GPU å®ç°
â”‚   â”œâ”€â”€ gru_forward_gpu_quant.cu # é‡åŒ–å‰å‘ä¼ æ’­ GPU å®ç°
â”‚   â”œâ”€â”€ gru_backward_gpu.cu     # åå‘ä¼ æ’­ GPU å®ç°
â”‚   â”œâ”€â”€ gru_interface.cpp       # æ¥å£å®ç°
â”‚   â””â”€â”€ quantize_ops.cu         # é‡åŒ–æ“ä½œå®ç°
â”œâ”€â”€ pytorch/                    # PyTorch ç»‘å®šå’Œ Python æ¥å£
â”‚   â”œâ”€â”€ quant_gru.py            # é‡åŒ– GRU ç±»ï¼ˆå« CUDA å’Œçº¯ PyTorch åŒå®ç°ï¼‰
â”‚   â”œâ”€â”€ setup.py                # Python æ‰©å±•ç¼–è¯‘é…ç½®
â”‚   â”œâ”€â”€ lib/                    # ç¼–è¯‘ç”Ÿæˆçš„åº“æ–‡ä»¶
â”‚   â”œâ”€â”€ config/                 # é…ç½®æ–‡ä»¶
â”‚   â”‚   â””â”€â”€ gru_quant_bitwidth_config.json  # é‡åŒ–ä½å®½é…ç½®
â”‚   â””â”€â”€ test_*.py               # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ example/                    # C++ ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ gru.cc                  # æµ®ç‚¹/é‡åŒ– GRU å¯¹æ¯”ç¤ºä¾‹
â”œâ”€â”€ CMakeLists.txt              # CMake æ„å»ºé…ç½®
```

## ğŸ“š å‚è€ƒ

- [AIMET (AI Model Efficiency Toolkit)](https://github.com/quic/aimet)
- [Haste: Fast RNN Library](https://github.com/lmnt-com/haste)
- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)

