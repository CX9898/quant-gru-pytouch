#pragma once

#include <cublas_v2.h>
#include <cuda_runtime_api.h>

#include "quantize_ops_helper.hpp"

namespace gru {

template<typename QuantT>
class ForwardPassQuant {
 public:
  // training: `true` if the caller intends to perform a backward pass to compute gradients.
  // batch_size: the number of training/inference inputs provided in each tensor.
  // input_size: the dimension of each input vector.
  // hidden_size: the expected dimension of each output vector.
  // blas_handle: an initialized cuBLAS handle (see `cublasCreate`).
  ForwardPassQuant(
      const bool training,
      const int batch_size,
      const int input_size,
      const int hidden_size,
      const cublasHandle_t &blas_handle,
      const cudaStream_t &stream = 0);

  // Releases internal resources.
  // Blocks until all iterations have completed executing on the GPU.
  ~ForwardPassQuant();

  void setRescaleParam(const GRUQuantitativeParameters &parms);

  // Performs one forward iteration of the GRU cell.
  //
  // W: [C,H*3] the input weight matrix.
  // R: [H,H*3] the recurrent weight matrix.
  // bx: [H*3] the bias for the input weight matrix.
  // br: [H*3] the bias for the recurrent weight matrix.
  // x: [N,C] the GRU input for this iteration (N vectors, each with dimension C).
  // h: [N,H] the t-1 iteration's `h_out` or the initial hidden state if this is the
  //     t=0 iteration (typically zeros).
  // h_out: [N,H] the GRU's output, and the input to the next iteration's `h`. This
  //     pointer may be the same as `h`. Each iteration may reuse the same memory region.
  // v: [N,H*4] if `training` is `false`, this can be a null pointer. If `training` is
  //     `true`, this vector will contain intermediate activations for this iteration which
  //     must be provided as-is to the corresponding backward iteration. The caller must
  //     provide a new memory region for each iteration.
  // tmp_Wx: [N,H*3] additional temporary work space required for this iteration. The caller
  //     should not use the contents of this vector, and must provide a new memory region for
  //     each iteration.
  // tmp_Rh: [N,H*3] additional temporary work space required for this iteration. The caller
  //     should not use the contents of this vector. The same memory region may be provided
  //     for each iteration.
  // zoneout_prob: 0.0 <= zoneout_prob <= 1.0; specifies the probability of a hidden
  //     activation being randomly zoned out. If zoneout was used during training, this
  //     parameter must also be specified during inference with the same value.
  // zoneout_mask: [N,H] may be null to disable zoneout. This is a random binary mask
  //     following a Bernoulli(1-zoneout_prob) distribution. A different mask is typically
  //     used for each iteration.
  void Iterate(
      const QuantT *W,
      const QuantT *R,
      const int32_t *bx,
      const int32_t *br,
      const QuantT *x,
      const QuantT *h,
      QuantT *h_out,
      QuantT *v,
      int32_t *tmp_Wx,
      int32_t *tmp_Rh,
      const float zoneout_prob,
      const QuantT *zoneout_mask);

  void Run(
      const int steps,
      const QuantT *W,
      const QuantT *R,
      const int32_t *bx,
      const int32_t *br,
      const QuantT *x,
      QuantT *h,
      QuantT *v,
      int32_t *tmp_Wx,
      int32_t *tmp_Rh,
      const float zoneout_prob,
      const QuantT *zoneout_mask);

 private:
  void IterateInternal(
      const QuantT *R,
      const int32_t *bx,
      const int32_t *br,
      const QuantT *h,
      QuantT *h_out,
      QuantT *v,
      const int32_t *tmp_Wx,
      int32_t *tmp_Rh,
      const int* W_sum_mul_x_zp, // hidden_size * 3
      const int* R_sum_mul_h_zp, // hidden_size * 3
      const float zoneout_prob,
      const QuantT *zoneout_mask);

  struct private_data;
  private_data *data_;

//  GruQuantScalesFixed gruQuantScales_;  // 用于动态更新 rescale 参数
  QuantGRUReScale rescale_param_;
};

template<typename T>
class BackwardPassQuant {
 public:
  // batch_size: the number of training inputs provided in each tensor.
  // input_size: the dimension of each input vector.
  // hidden_size: the expected dimension of each output vector.
  // blas_handle: an initialized cuBLAS handle (see `cublasCreate`).
  BackwardPassQuant(
      const int batch_size,
      const int input_size,
      const int hidden_size,
      const cublasHandle_t &blas_handle,
      const cudaStream_t &stream = 0);

  // Releases internal resources.
  // Blocks until all iterations have completed executing on the GPU.
  ~BackwardPassQuant();

  // Performs one backward iteration of the GRU cell.
  //
  // Note that BackwardPass must be iterated in the reverse order as ForwardPass.
  // If ForwardPass iterates from 0 to T-1, BackwardPass needs to iterate from
  // T-1 down to 0. When iteration numbers are described, they will be based on the
  // iteration index (i.e., the T-1'th iteration of the forward pass is the last call
  // to ForwardPass::Iterate, whereas it is the first call to BackwardPass::Iterate).
  //
  // W_t: [H*3,C] the transpose of the input weight matrix.
  // R_t: [H*3,H] the transpose of the recurrent weight matrix.
  // bx: [H*3] the bias vector for the input weight matrix.
  // br: [H*3] the bias vector for the recurrent weight matrix.
  // x_t: [C,N] the transpose of the GRU input for this iteration.
  // h: [N,H] the t-1 iteration's `h_out` or the initial hidden state if this is the t=0
  //     iteration (typically zeros).
  // v: [N,H*4] the same vector as returned by ForwardPass::Iterate on its corresponding
  //     iteration.
  // dh_new: [N,H] the gradient of `h_out` with respect to the loss at this iteration.
  // dx: [N,C] the gradient of the input at this time step with respect to the loss.
  // dW: [C,H*3] the gradient of the input weight matrix with respect to the loss.
  // dR: [H,H*3] the gradient of the recurrent weight matrix with respect to the loss.
  // dbx: [H*3] the gradient of the bias vector for the input weight matrix with respect to
  //     the loss.
  // dbr: [H*3] the gradient of the bias vector for the recurrent weight matrix with respect
  //     to the loss.
  // dh: [N,H] NOTE: this is an input and output parameter. Should be initialized to zeros
  //     for the T-1'th iteration and the same pointer should be passed in for each
  //     iteration. After a complete backward pass, this vector will contain the gradient
  //     of the initial hidden state with respect to the loss.
  // dp: [N,H*3] additional temporary work space required for this iteration. The caller
  //     should not use the contents of this vector. A new memory region must be provided
  //     for each iteration.
  // dq: [N,H*3] additional temporary work space required for this iteration. The caller
  //     should not use the contents of this vector. A new memory region must be provided
  //     for each iteration.
  // zoneout_mask: [N,H] may be null if zoneout was disabled in the forward pass. This vector
  //     must be the same as the one provided during the corresponding forward iteration.
  void Iterate(
      const T *W_t,
      const T *R_t,
      const int32_t *bx,
      const int32_t *br,
      const T *x_t,
      const T *h,
      const T *v,
      const T *dh_new,
      T *dx,
      T *dW,
      T *dR,
      int32_t *dbx,
      int32_t *dbr,
      T *dh,
      T *dp,
      T *dq,
      const T *zoneout_mask);

  void Run(
      const int steps,
      const T *W_t,
      const T *R_t,
      const int32_t *bx,
      const int32_t *br,
      const T *x_t,
      const T *h,
      const T *v,
      const T *dh_new,
      T *dx,
      T *dW,
      T *dR,
      int32_t *dbx,
      int32_t *dbr,
      T *dh,
      T *dp,
      T *dq,
      const T *zoneout_mask);

 private:
  void IterateInternal(
      const T *R_t,
      const T *h,
      const T *v,
      const T *dh_new,
      int32_t *dbx,
      int32_t *dbr,
      T *dh,
      T *dp,
      T *dq,
      const T *zoneout_mask);

  struct private_data;
  private_data *data_;
};

}  // namespace gru
