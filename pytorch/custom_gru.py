"""
è‡ªå®šä¹‰ GRU ç±»ï¼Œç»§æ‰¿è‡ª PyTorch çš„ nn.GRU
æ”¯æŒé‡åŒ–å’Œéé‡åŒ–ä¸¤ç§å‰å‘ä¼ æ’­æ¨¡å¼
"""

import json
import torch
import torch.nn as nn
from typing import Optional, Tuple, Dict, Any

try:
    import gru_interface_binding as gru_ops
except ImportError:
    raise ImportError(
        "gru_interface_binding module not found. "
        "Please compile the C++ extension first using setup.py"
    )


# ==================== ä½å®½é…ç½®å·¥å…·å‡½æ•° ====================


def _get_bitwidth_value(op_cfg: dict) -> int:
    """
    ä»æ“ä½œé…ç½®ä¸­è·å–ä½å®½å€¼
    
    Python ç«¯åªå…³æ³¨ä½å®½æ•°é‡ï¼ˆ8, 16, 32ï¼‰ï¼Œä¸å…³å¿ƒå®é™…ç±»å‹ï¼ˆINT/UINTï¼‰ã€‚
    å®é™…ç±»å‹ç”± C++ ç«¯åœ¨ to_cpp() æ—¶æ ¹æ®ä½å®½æ•°å€¼å†³å®šã€‚
    
    è¿”å›å€¼:
        æ­£æ•´æ•°è¡¨ç¤ºä½å®½: 8, 16, 32
    """
    return op_cfg.get('bitwidth', 8)


def _get_symmetric_value(op_cfg: dict) -> bool:
    """
    ä»æ“ä½œé…ç½®ä¸­è·å–æ˜¯å¦ä½¿ç”¨å¯¹ç§°é‡åŒ–
    
    Args:
        op_cfg: æ“ä½œé…ç½®å­—å…¸
        
    Returns:
        True è¡¨ç¤ºå¯¹ç§°é‡åŒ–ï¼ŒFalse è¡¨ç¤ºéå¯¹ç§°é‡åŒ–
    """
    return op_cfg.get('is_symmetric', True)


def load_bitwidth_config(config_file: str) -> gru_ops.OperatorQuantConfig:
    """
    ä» JSON é…ç½®æ–‡ä»¶åŠ è½½é‡åŒ–ä½å®½é…ç½®ï¼ˆåŒ…æ‹¬å¯¹ç§°é‡åŒ–é…ç½®ï¼‰
    
    Args:
        config_file: JSON é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        OperatorQuantConfig å¯¹è±¡
        
    JSON æ ¼å¼ç¤ºä¾‹:
    {
        "GRU_config": {
            "operator_config": {
                "input.x": { "bitwidth": 8, "is_symmetric": true },
                "gate.z_out": { "bitwidth": 8, "is_symmetric": false },
                ...
            }
        }
    }
    """
    with open(config_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    config = gru_ops.OperatorQuantConfig()
    gru_config = data.get('GRU_config', {})
    op_config = gru_config.get('operator_config', {})

    # å­—æ®µæ˜ å°„: JSON key -> (ä½å®½å±æ€§å, å¯¹ç§°é‡åŒ–å±æ€§å)
    field_map = {
        "input.x": ("x_", "x_symmetric_"),
        "input.h": ("h_", "h_symmetric_"),
        "weight.W": ("W_", "W_symmetric_"),
        "weight.R": ("R_", "R_symmetric_"),
        "weight.bx": ("bx_", "bx_symmetric_"),
        "weight.br": ("br_", "br_symmetric_"),
        "matmul.Wx": ("Wx_", "Wx_symmetric_"),
        "matmul.Rh": ("Rh_", "Rh_symmetric_"),
        "gate.z_pre": ("z_pre_", "z_pre_symmetric_"),
        "gate.z_out": ("z_out_", "z_out_symmetric_"),
        "gate.r_pre": ("r_pre_", "r_pre_symmetric_"),
        "gate.r_out": ("r_out_", "r_out_symmetric_"),
        "gate.g_pre": ("g_pre_", "g_pre_symmetric_"),
        "gate.g_out": ("g_out_", "g_out_symmetric_"),
        "op.Rh_add_br": ("Rh_add_br_", "Rh_add_br_symmetric_"),
        "op.rRh": ("rRh_", "rRh_symmetric_"),
        "op.old_contrib": ("old_contrib_", "old_contrib_symmetric_"),
        "op.new_contrib": ("new_contrib_", "new_contrib_symmetric_"),
    }

    for json_key, (bw_attr, sym_attr) in field_map.items():
        if json_key in op_config:
            op_cfg = op_config[json_key]
            # è®¾ç½®ä½å®½
            bw_val = _get_bitwidth_value(op_cfg)
            setattr(config, bw_attr, bw_val)
            # è®¾ç½®å¯¹ç§°é‡åŒ–é…ç½®
            sym_val = _get_symmetric_value(op_cfg)
            setattr(config, sym_attr, sym_val)

    return config


def _format_bitwidth(val: int) -> str:
    """æ ¼å¼åŒ–ä½å®½å€¼ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    # Python ç«¯åªæ˜¾ç¤ºä½å®½æ•°é‡
    return f"{abs(val)}bit"


def _format_symmetric(is_symmetric: bool) -> str:
    """æ ¼å¼åŒ–å¯¹ç§°é‡åŒ–å€¼ä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    return "å¯¹ç§°" if is_symmetric else "éå¯¹ç§°"


def apply_bitwidth_config(config: gru_ops.OperatorQuantConfig,
                          config_file: str,
                          verbose: bool = False) -> int:
    """
    ä» JSON é…ç½®æ–‡ä»¶åº”ç”¨é‡åŒ–ä½å®½é…ç½®ï¼ˆåŒ…æ‹¬å¯¹ç§°é‡åŒ–é…ç½®ï¼‰
    
    Args:
        config: è¦æ›´æ–°çš„ OperatorQuantConfig å¯¹è±¡
        config_file: JSON é…ç½®æ–‡ä»¶è·¯å¾„
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
    Returns:
        æˆåŠŸé…ç½®çš„å­—æ®µæ•°é‡
    """
    loaded = load_bitwidth_config(config_file)

    # å¤åˆ¶ä½å®½é…ç½®å­—æ®µ
    bitwidth_attrs = ['x_', 'h_', 'W_', 'R_', 'bx_', 'br_', 'Wx_', 'Rh_',
                      'z_pre_', 'z_out_', 'r_pre_', 'r_out_', 'g_pre_', 'g_out_',
                      'Rh_add_br_', 'rRh_', 'old_contrib_', 'new_contrib_']
    for attr in bitwidth_attrs:
        setattr(config, attr, getattr(loaded, attr))

    # å¤åˆ¶å¯¹ç§°é‡åŒ–é…ç½®å­—æ®µ
    symmetric_attrs = ['x_symmetric_', 'h_symmetric_', 'W_symmetric_', 'R_symmetric_',
                       'bx_symmetric_', 'br_symmetric_', 'Wx_symmetric_', 'Rh_symmetric_',
                       'z_pre_symmetric_', 'z_out_symmetric_', 'r_pre_symmetric_', 'r_out_symmetric_',
                       'g_pre_symmetric_', 'g_out_symmetric_', 'Rh_add_br_symmetric_', 'rRh_symmetric_',
                       'old_contrib_symmetric_', 'new_contrib_symmetric_']
    for attr in symmetric_attrs:
        setattr(config, attr, getattr(loaded, attr))

    if verbose:
        print("\n" + "=" * 70)
        print("ğŸ”§ åº”ç”¨ GRU é‡åŒ–é…ç½®ï¼ˆä½å®½ + å¯¹ç§°é‡åŒ–ï¼‰")
        print("=" * 70)
        print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config_file}")
        print("-" * 70)
        print(f"  [è¾“å…¥]  x: {_format_bitwidth(config.x_):6s} ({_format_symmetric(config.x_symmetric_)})")
        print(f"          h: {_format_bitwidth(config.h_):6s} ({_format_symmetric(config.h_symmetric_)})")
        print(f"  [æƒé‡]  W: {_format_bitwidth(config.W_):6s} ({_format_symmetric(config.W_symmetric_)})")
        print(f"          R: {_format_bitwidth(config.R_):6s} ({_format_symmetric(config.R_symmetric_)})")
        print(f"          bx: {_format_bitwidth(config.bx_):6s} ({_format_symmetric(config.bx_symmetric_)})")
        print(f"          br: {_format_bitwidth(config.br_):6s} ({_format_symmetric(config.br_symmetric_)})")
        print(f"  [çŸ©é˜µ]  Wx: {_format_bitwidth(config.Wx_):6s} ({_format_symmetric(config.Wx_symmetric_)})")
        print(f"          Rh: {_format_bitwidth(config.Rh_):6s} ({_format_symmetric(config.Rh_symmetric_)})")
        print(f"  [é—¨æ§]  z_pre: {_format_bitwidth(config.z_pre_):6s} ({_format_symmetric(config.z_pre_symmetric_)})")
        print(f"          z_out: {_format_bitwidth(config.z_out_):6s} ({_format_symmetric(config.z_out_symmetric_)})")
        print(f"          r_pre: {_format_bitwidth(config.r_pre_):6s} ({_format_symmetric(config.r_pre_symmetric_)})")
        print(f"          r_out: {_format_bitwidth(config.r_out_):6s} ({_format_symmetric(config.r_out_symmetric_)})")
        print(f"          g_pre: {_format_bitwidth(config.g_pre_):6s} ({_format_symmetric(config.g_pre_symmetric_)})")
        print(f"          g_out: {_format_bitwidth(config.g_out_):6s} ({_format_symmetric(config.g_out_symmetric_)})")
        print(
            f"  [è¿ç®—]  Rh+br: {_format_bitwidth(config.Rh_add_br_):6s} ({_format_symmetric(config.Rh_add_br_symmetric_)})")
        print(f"          rRh: {_format_bitwidth(config.rRh_):6s} ({_format_symmetric(config.rRh_symmetric_)})")
        print(
            f"  [è¾“å‡º]  old: {_format_bitwidth(config.old_contrib_):6s} ({_format_symmetric(config.old_contrib_symmetric_)})")
        print(
            f"          new: {_format_bitwidth(config.new_contrib_):6s} ({_format_symmetric(config.new_contrib_symmetric_)})")
        print("=" * 70 + "\n")

    return 38  # 19 ä½å®½å­—æ®µ + 19 å¯¹ç§°é‡åŒ–å­—æ®µ


# ==================== å·¥å…·å‡½æ•°ï¼šæƒé‡æ ¼å¼è½¬æ¢ ====================

def reorder_weights_pytorch_to_haste(w: torch.Tensor) -> torch.Tensor:
    """
    å°† PyTorch GRU æƒé‡æ ¼å¼ (r, z, n) è½¬æ¢ä¸º Haste GRU æƒé‡æ ¼å¼ (z, r, n)

    Args:
        w: æƒé‡å¼ é‡ï¼Œç¬¬ä¸€ç»´æ˜¯ 3*hidden_sizeï¼Œé¡ºåºä¸º r, z, n
           - æƒé‡çŸ©é˜µï¼šå½¢çŠ¶ä¸º [3*hidden, input] æˆ– [3*hidden, hidden]
           - åç½®å‘é‡ï¼šå½¢çŠ¶ä¸º [3*hidden]

    Returns:
        é‡æ’åºåçš„æƒé‡å¼ é‡ï¼Œé¡ºåºä¸º z, r, nï¼Œå½¢çŠ¶ä¿æŒä¸å˜
    """
    w = w.contiguous()
    hidden_size_3 = w.shape[0] // 3
    device = w.device

    # PyTorch: [r0...rH, z0...zH, n0...nH] -> Haste: [z0...zH, r0...rH, n0...nH]
    indices = torch.cat([
        torch.arange(hidden_size_3, 2 * hidden_size_3, device=device),  # z
        torch.arange(0, hidden_size_3, device=device),  # r
        torch.arange(2 * hidden_size_3, 3 * hidden_size_3, device=device)  # n
    ])

    return w.index_select(0, indices).contiguous()


def reorder_weights_haste_to_pytorch(w: torch.Tensor) -> torch.Tensor:
    """
    å°† Haste GRU æƒé‡æ ¼å¼ (z, r, n) è½¬æ¢å› PyTorch GRU æƒé‡æ ¼å¼ (r, z, n)

    Args:
        w: æƒé‡å¼ é‡ï¼Œç¬¬ä¸€ç»´æ˜¯ 3*hidden_sizeï¼Œé¡ºåºä¸º z, r, n

    Returns:
        é‡æ’åºåçš„æƒé‡å¼ é‡ï¼Œé¡ºåºä¸º r, z, nï¼Œå½¢çŠ¶ä¿æŒä¸å˜
    """
    w = w.contiguous()
    hidden_size_3 = w.shape[0] // 3
    device = w.device

    # Haste: [z0...zH, r0...rH, n0...nH] -> PyTorch: [r0...rH, z0...zH, n0...nH]
    indices = torch.cat([
        torch.arange(hidden_size_3, 2 * hidden_size_3, device=device),  # r (åœ¨ Haste ä¸­æ˜¯ç¬¬äºŒéƒ¨åˆ†)
        torch.arange(0, hidden_size_3, device=device),  # z (åœ¨ Haste ä¸­æ˜¯ç¬¬ä¸€éƒ¨åˆ†)
        torch.arange(2 * hidden_size_3, 3 * hidden_size_3, device=device)  # n (åœ¨ Haste ä¸­æ˜¯ç¬¬ä¸‰éƒ¨åˆ†)
    ])

    return w.index_select(0, indices).contiguous()


def ensure_cuda_float32(tensor: torch.Tensor, device: torch.device) -> torch.Tensor:
    """
    ç¡®ä¿å¼ é‡åœ¨æŒ‡å®šè®¾å¤‡ä¸Šä¸”ä¸º float32 ç±»å‹ï¼ˆä¿æŒæ¢¯åº¦è¿½è¸ªï¼‰

    Args:
        tensor: è¾“å…¥å¼ é‡
        device: ç›®æ ‡è®¾å¤‡

    Returns:
        è½¬æ¢åçš„å¼ é‡
    """
    if not tensor.is_cuda:
        tensor = tensor.to(device)
    if tensor.dtype != torch.float32:
        tensor = tensor.float()
    return tensor


# ==================== GRUFunctionï¼šè‡ªå®šä¹‰ autograd Function ====================

class GRUFunction(torch.autograd.Function):
    """
    GRU çš„è‡ªå®šä¹‰ autograd Functionï¼Œæ”¯æŒåå‘ä¼ æ’­

    èŒè´£ï¼š
    - å¤„ç† PyTorch å’Œ Haste æ ¼å¼ä¹‹é—´çš„è½¬æ¢
    - è°ƒç”¨ C++ æ¥å£è¿›è¡Œå‰å‘å’Œåå‘ä¼ æ’­
    - ç®¡ç†ä¸­é—´ç»“æœçš„ä¿å­˜å’Œæ¢å¤
    """

    @staticmethod
    def forward(ctx, input, weight_ih, weight_hh, bias_ih, bias_hh, h0, is_training,
                use_quantization=False, quant_params=None):
        """
        å‰å‘ä¼ æ’­

        Args:
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
            input: è¾“å…¥åºåˆ— [time_steps, batch_size, input_size]
            weight_ih: è¾“å…¥æƒé‡ [3*hidden_size, input_size] (PyTorch æ ¼å¼: r, z, n)
            weight_hh: å¾ªç¯æƒé‡ [3*hidden_size, hidden_size] (PyTorch æ ¼å¼: r, z, n)
            bias_ih: è¾“å…¥åç½® [3*hidden_size] (PyTorch æ ¼å¼: r, z, n) æˆ– None
            bias_hh: å¾ªç¯åç½® [3*hidden_size] (PyTorch æ ¼å¼: r, z, n) æˆ– None
            h0: åˆå§‹éšè—çŠ¶æ€ [batch_size, hidden_size] æˆ– None
            is_training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
            use_quantization: æ˜¯å¦ä½¿ç”¨é‡åŒ–
            quant_params: é‡åŒ–å‚æ•°ï¼ˆåŒ…å«ä½å®½é…ç½®ï¼‰

        Returns:
            output: è¾“å‡ºåºåˆ— [time_steps, batch_size, hidden_size]
            h_n: æœ€ç»ˆéšè—çŠ¶æ€ [1, batch_size, hidden_size]
        """
        time_steps, batch_size, input_size = input.shape
        hidden_size = weight_hh.shape[1]

        # ä¿å­˜ä¸Šä¸‹æ–‡ä¿¡æ¯
        ctx.time_steps = time_steps
        ctx.batch_size = batch_size
        ctx.input_size = input_size
        ctx.hidden_size = hidden_size
        ctx.bias_ih_is_none = (bias_ih is None)
        ctx.bias_hh_is_none = (bias_hh is None)
        ctx.h0_is_none = (h0 is None)

        # ç¡®ä¿è¾“å…¥åœ¨ CUDA ä¸Š
        device = input.device if input.is_cuda else torch.device('cuda')
        input = ensure_cuda_float32(input, device)

        # è½¬æ¢æƒé‡æ ¼å¼ï¼šPyTorch (r, z, n) -> Haste (z, r, n)
        # æƒé‡çŸ©é˜µéœ€è¦è½¬ç½®ï¼š[3*hidden, input] -> [input, 3*hidden]
        weight_ih = ensure_cuda_float32(weight_ih, device)
        weight_hh = ensure_cuda_float32(weight_hh, device)
        W = reorder_weights_pytorch_to_haste(weight_ih).t().contiguous()
        R = reorder_weights_pytorch_to_haste(weight_hh).t().contiguous()

        # å¤„ç†åç½®
        if bias_ih is not None and bias_hh is not None:
            bias_ih = ensure_cuda_float32(bias_ih, device)
            bias_hh = ensure_cuda_float32(bias_hh, device)
            bx = reorder_weights_pytorch_to_haste(bias_ih).contiguous()
            br = reorder_weights_pytorch_to_haste(bias_hh).contiguous()
        else:
            bx = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)
            br = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)

        # å‡†å¤‡ h0
        if h0 is not None:
            h0_tensor = ensure_cuda_float32(h0, device)
        else:
            h0_tensor = torch.empty(0, device=device, dtype=torch.float32)

        # å‡†å¤‡é‡åŒ–å‚æ•°
        if use_quantization:
            if quant_params is None:
                raise RuntimeError("quant_params is required when use_quantization=True")
        else:
            quant_params = gru_ops.GRUQuantitativeParameters()

        # è°ƒç”¨ C++ æ¥å£
        output_full, v = gru_ops.forward_interface(
            is_training=is_training,
            is_quant=use_quantization,
            time_steps=time_steps,
            batch_size=batch_size,
            input_size=input_size,
            hidden_size=hidden_size,
            W=W,
            R=R,
            bx=bx,
            br=br,
            x=input,
            h0=h0_tensor,
            quant_params=quant_params
        )

        # # æµ®ç‚¹å‰å‘
        # output_full_no_quant, v_no_quant = gru_ops.haste_gru_forward(
        #     is_training=is_training,
        #     time_steps=time_steps,
        #     batch_size=batch_size,
        #     input_size=input_size,
        #     hidden_size=hidden_size,
        #     W=W,
        #     R=R,
        #     bx=bx,
        #     br=br,
        #     x=input,
        #     h0=h0_tensor,
        # )

        # åˆ†ç¦»è¾“å‡ºï¼šoutput_full[0] æ˜¯åˆå§‹çŠ¶æ€ï¼Œoutput_full[1:] æ˜¯æ—¶é—´æ­¥è¾“å‡º
        output = output_full[1:]  # [time_steps, batch_size, hidden_size]
        h_n = output_full[-1:]  # [1, batch_size, hidden_size]

        # ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­
        ctx.save_for_backward(W, R, bx, br, input, output_full, v)

        return output, h_n

    @staticmethod
    def backward(ctx, grad_output, grad_h_n):
        """
        åå‘ä¼ æ’­

        Args:
            ctx: ä¸Šä¸‹æ–‡å¯¹è±¡
            grad_output: è¾“å‡ºåºåˆ—çš„æ¢¯åº¦ [time_steps, batch_size, hidden_size]
            grad_h_n: æœ€ç»ˆéšè—çŠ¶æ€çš„æ¢¯åº¦ [1, batch_size, hidden_size]

        Returns:
            å„è¾“å…¥å‚æ•°çš„æ¢¯åº¦
        """
        W, R, bx, br, input, h, v = ctx.saved_tensors
        time_steps = ctx.time_steps
        batch_size = ctx.batch_size
        input_size = ctx.input_size
        hidden_size = ctx.hidden_size

        # ç¡®ä¿æ‰€æœ‰æ•°æ®åœ¨ CUDA ä¸Š
        device = grad_output.device
        if not W.is_cuda:
            W = W.to(device)
        if not R.is_cuda:
            R = R.to(device)
        if not bx.is_cuda:
            bx = bx.to(device)
        if not br.is_cuda:
            br = br.to(device)
        if not input.is_cuda:
            input = input.to(device)
        if not h.is_cuda:
            h = h.to(device)
        if v is not None and not v.is_cuda:
            v = v.to(device)
        if not grad_output.is_cuda:
            grad_output = grad_output.to(device)
        if grad_h_n is not None and not grad_h_n.is_cuda:
            grad_h_n = grad_h_n.to(device)

        # æ„å»ºéšè—çŠ¶æ€æ¢¯åº¦
        # C++ æ¥å£éœ€è¦ [time_steps + 1, batch_size, hidden_size] æ ¼å¼
        # dh_new[0] æ˜¯åˆå§‹çŠ¶æ€æ¢¯åº¦ï¼ˆä¿æŒä¸º 0ï¼‰ï¼Œdh_new[1:] æ˜¯æ—¶é—´æ­¥æ¢¯åº¦
        dh_new = torch.zeros(
            (time_steps + 1, batch_size, hidden_size),
            device=device,
            dtype=grad_output.dtype
        )
        dh_new[1:] = grad_output

        # å¤„ç†æœ€ç»ˆéšè—çŠ¶æ€çš„æ¢¯åº¦ï¼ˆoutput[-1] å’Œ h_n[0] æŒ‡å‘åŒä¸€ä¸ªçŠ¶æ€ï¼‰
        if grad_h_n is not None and grad_h_n.numel() > 0:
            dh_new[-1] = dh_new[-1] + grad_h_n[0]

        # è°ƒç”¨ C++ åå‘ä¼ æ’­æ¥å£
        # Python ç»‘å®šå±‚ä¼šå†…éƒ¨å¤„ç†è½¬ç½®ï¼Œä½¿å…¶ä¸ haste çš„å®ç°ä¸€è‡´ï¼š
        # - x: [T,B,I] -> x_t: [I,T,B]
        # - W: [C,H*3] -> W_t: [H*3,C]
        # - R: [H,H*3] -> R_t: [H*3,H]
        dx, dW, dR, dbx, dbr, dh = gru_ops.haste_gru_backward(
            time_steps=time_steps,
            batch_size=batch_size,
            input_size=input_size,
            hidden_size=hidden_size,
            W=W,  # [C, H*3] - Python ç»‘å®šå±‚ä¼šè½¬ç½®ä¸º [H*3, C]
            R=R,  # [H, H*3] - Python ç»‘å®šå±‚ä¼šè½¬ç½®ä¸º [H*3, H]
            bx=bx,
            br=br,
            x=input,  # [T, B, I] - Python ç»‘å®šå±‚ä¼šè½¬ç½®ä¸º [I, T, B]
            dh_new=dh_new,
            h=h,
            v=v
        )

        # è½¬æ¢æ¢¯åº¦æ ¼å¼ï¼šHaste (z, r, n) -> PyTorch (r, z, n)
        # æ¢¯åº¦çŸ©é˜µéœ€è¦è½¬ç½®ï¼š[input, 3*hidden] -> [3*hidden, input]
        dW_pytorch = reorder_weights_haste_to_pytorch(dW.t()).contiguous()
        dR_pytorch = reorder_weights_haste_to_pytorch(dR.t()).contiguous()
        dbx_pytorch = reorder_weights_haste_to_pytorch(dbx).contiguous()
        dbr_pytorch = reorder_weights_haste_to_pytorch(dbr).contiguous()

        # å¤„ç†åç½®æ¢¯åº¦
        if ctx.bias_ih_is_none:
            dbx_pytorch = None
        if ctx.bias_hh_is_none:
            dbr_pytorch = None

        # å¤„ç† h0 æ¢¯åº¦
        grad_h0 = None if ctx.h0_is_none else dh

        # è¿”å›æ¢¯åº¦ï¼ˆå¯¹åº” forward çš„ 9 ä¸ªå‚æ•°ï¼‰
        return dx, dW_pytorch, dR_pytorch, dbx_pytorch, dbr_pytorch, grad_h0, None, None, None


# ==================== CustomGRUï¼šè‡ªå®šä¹‰ GRU ç±» ====================

class CustomGRU(nn.Module):
    """
    è‡ªå®šä¹‰ GRU å®ç°ï¼Œæ”¯æŒé‡åŒ–å‰å‘ä¼ æ’­å’ŒåŒå‘ GRU
    
    è®¾è®¡åŸåˆ™ï¼š
        - å»¶è¿Ÿåˆå§‹åŒ–ï¼šCUDA handle åœ¨é¦–æ¬¡ forward/calibrate æ—¶åˆå§‹åŒ–ï¼Œè€Œéæ„é€ æ—¶
        - é…ç½®ä¸åˆ›å»ºåˆ†ç¦»ï¼šä½å®½é…ç½®é€šè¿‡ load_bitwidth_config() å•ç‹¬åŠ è½½
        - æ ¡å‡†ä¸åˆ›å»ºåˆ†ç¦»ï¼šæ ¡å‡†é€šè¿‡ calibrate() + finalize_calibration() å•ç‹¬æ‰§è¡Œ
        - å¯åºåˆ—åŒ–ï¼šä½¿ç”¨ Python å­—å…¸å­˜å‚¨é…ç½®ï¼Œæ”¯æŒ pickle/deepcopy
        - åŒå‘æ”¯æŒï¼šå†…éƒ¨ä½¿ç”¨ä¸¤ä¸ªå•å‘ GRU æ¨¡æ‹ŸåŒå‘ GRUï¼Œå¯¹å¤–æ¥å£ä¸ nn.GRU ä¸€è‡´

    é‡åŒ–ä½¿ç”¨æµç¨‹ï¼š
        1. åˆ›å»ºæ¨¡å‹ï¼šgru = CustomGRU(..., use_quantization=True)
        2. (å¯é€‰) åŠ è½½ä½å®½é…ç½®ï¼šgru.load_bitwidth_config("config.json")
        3. ç´¯ç§¯æ ¡å‡†æ•°æ®ï¼šgru.calibrate(data1), gru.calibrate(data2), ...
        4. å®Œæˆæ ¡å‡†ï¼šgru.finalize_calibration()
        5. æ­£å¸¸æ¨ç†ï¼šoutput, h_n = gru(input)
        
    å¢é‡æ ¡å‡†ï¼ˆæ”¯æŒä¸­é€”é‡æ–°æ ¡å‡†ï¼‰ï¼š
        - å¯éšæ—¶è°ƒç”¨ calibrate() ç´¯ç§¯æ›´å¤šæ•°æ®
        - åœ¨ä¸‹æ¬¡ forward() å‰è°ƒç”¨ finalize_calibration() æ›´æ–°é‡åŒ–å‚æ•°
        - å¦‚éœ€å®Œå…¨é‡ç½®èŒƒå›´ï¼šgru.reset_calibration()

    å†…éƒ¨çŠ¶æ€ï¼š
        - _cublas_initialized: CUDA handle æ˜¯å¦å·²åˆå§‹åŒ–
        - _bitwidth_config_dict: ä½å®½é…ç½®ï¼ˆPython å­—å…¸ï¼Œå¯åºåˆ—åŒ–ï¼‰
        - quant_ranges / quant_ranges_reverse: æ ¡å‡†èŒƒå›´ï¼ˆC++ å¯¹è±¡ï¼Œcalibrate() æ—¶åˆ›å»ºï¼‰
        - quant_params / quant_params_reverse: é‡åŒ–å‚æ•°ï¼ˆC++ å¯¹è±¡ï¼Œfinalize_calibration() æ—¶åˆ›å»ºï¼‰

    Args:
        input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_size: éšè—çŠ¶æ€ç»´åº¦
        num_layers: GRU å±‚æ•°ï¼ˆç›®å‰ä»…æ”¯æŒ 1ï¼‰
        bias: æ˜¯å¦ä½¿ç”¨åç½®
        batch_first: å¦‚æœä¸º Trueï¼Œè¾“å…¥å½¢çŠ¶ä¸º [batch, seq, feature]
        dropout: å±‚é—´ dropout æ¦‚ç‡ï¼ˆç›®å‰ä¸æ”¯æŒï¼‰
        bidirectional: æ˜¯å¦åŒå‘ GRUï¼ˆTrue æ—¶è¾“å‡ºç»´åº¦ä¸º 2*hidden_sizeï¼‰

    Attributes:
        use_quantization: æ˜¯å¦å¯ç”¨é‡åŒ–ï¼ˆé»˜è®¤ Falseï¼Œå¯éšæ—¶ä¿®æ”¹ï¼‰

    Examples:
        >>> # åŸºæœ¬ä½¿ç”¨ï¼ˆéé‡åŒ–ï¼Œå•å‘ï¼‰
        >>> gru = CustomGRU(64, 128, batch_first=True)
        >>> output, h_n = gru(input_data)
        
        >>> # åŒå‘ GRUï¼ˆä¸ nn.GRU æ¥å£ä¸€è‡´ï¼‰
        >>> gru = CustomGRU(64, 128, batch_first=True, bidirectional=True)
        >>> output, h_n = gru(input_data)  # output: [B, T, 2*H], h_n: [2, B, H]
        
        >>> # é‡åŒ–ä½¿ç”¨ï¼ˆå…ˆæ ¡å‡†ï¼Œå†å¼€å¯é‡åŒ–ï¼‰
        >>> gru = CustomGRU(64, 128)
        >>> gru.load_bitwidth_config("config.json")  # å¯é€‰
        >>> for batch in calibration_loader:
        ...     gru.calibrate(batch)  # æ ¡å‡†æ—¶æ— éœ€å¼€å¯é‡åŒ–
        >>> gru.finalize_calibration()
        >>> gru.use_quantization = True  # æ¨ç†æ—¶å¼€å¯é‡åŒ–
        >>> output, h_n = gru(input_data)
    """

    def __init__(
            self,
            input_size: int,
            hidden_size: int,
            num_layers: int = 1,
            bias: bool = True,
            batch_first: bool = False,
            dropout: float = 0.0,
            bidirectional: bool = False,
            use_quantization: bool = False,
    ):
        """
        åˆå§‹åŒ– CustomGRU
        
        è®¾è®¡åŸåˆ™ï¼š
            - __init__ åªåšæœ€åŸºæœ¬çš„å±æ€§åˆå§‹åŒ–
            - å¤æ‚æ“ä½œï¼ˆCUDA åˆå§‹åŒ–ã€æ ¡å‡†ç­‰ï¼‰å»¶è¿Ÿåˆ°éœ€è¦æ—¶æ‰§è¡Œ
            - ä½å®½é…ç½®é€šè¿‡ load_bitwidth_config() å•ç‹¬åŠ è½½
            - åŒå‘ GRU å†…éƒ¨ä½¿ç”¨ä¸¤å¥—æƒé‡ï¼Œåˆ†åˆ«å¤„ç†æ­£å‘å’Œåå‘
            - é‡åŒ–å¼€å…³ use_quantization å¯éšæ—¶ä¿®æ”¹ï¼Œä»…å½±å“ forward
        
        Args:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_size: éšè—çŠ¶æ€ç»´åº¦
            num_layers: GRU å±‚æ•°ï¼ˆç›®å‰ä»…æ”¯æŒ 1ï¼‰
            bias: æ˜¯å¦ä½¿ç”¨åç½®
            batch_first: è¾“å…¥æ ¼å¼æ˜¯å¦ä¸º [batch, seq, feature]
            dropout: dropout æ¦‚ç‡ï¼ˆç›®å‰ä¸æ”¯æŒï¼‰
            bidirectional: æ˜¯å¦åŒå‘ GRU
        
        é‡åŒ–ä½¿ç”¨æµç¨‹ï¼š
            1. åˆ›å»ºæ¨¡å‹: gru = CustomGRU(...)
            2. (å¯é€‰) åŠ è½½ä½å®½é…ç½®: gru.load_bitwidth_config("config.json")
            3. ç´¯ç§¯æ ¡å‡†: gru.calibrate(data1), gru.calibrate(data2), ...
            4. å®Œæˆæ ¡å‡†: gru.finalize_calibration()
            5. å¼€å¯é‡åŒ–: gru.use_quantization = True
            6. æ­£å¸¸æ¨ç†: output, h_n = gru(input)
        """
        super(CustomGRU, self).__init__()

        # æ£€æŸ¥é™åˆ¶
        if num_layers != 1:
            raise NotImplementedError("Currently only supports num_layers=1")
        if dropout > 0:
            raise NotImplementedError("Currently does not support dropout")

        # ===== åŸºæœ¬é…ç½® =====
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.use_quantization = use_quantization  # é»˜è®¤å…³é—­é‡åŒ–ï¼Œæ ¡å‡†å®Œæˆåå¯è®¾ç½®ä¸º True
        self.num_directions = 2 if bidirectional else 1

        # ===== æƒé‡å‚æ•°ï¼ˆä¸ nn.GRU å‘½åä¸€è‡´ï¼‰ =====
        # å‰å‘æ–¹å‘æƒé‡
        self.weight_ih_l0 = nn.Parameter(torch.empty(3 * hidden_size, input_size))
        self.weight_hh_l0 = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
        if bias:
            self.bias_ih_l0 = nn.Parameter(torch.empty(3 * hidden_size))
            self.bias_hh_l0 = nn.Parameter(torch.empty(3 * hidden_size))
        else:
            self.register_parameter('bias_ih_l0', None)
            self.register_parameter('bias_hh_l0', None)

        # åå‘æ–¹å‘æƒé‡ï¼ˆä»…åŒå‘æ—¶ä½¿ç”¨ï¼‰
        if bidirectional:
            self.weight_ih_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size, input_size))
            self.weight_hh_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
            if bias:
                self.bias_ih_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size))
                self.bias_hh_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size))
            else:
                self.register_parameter('bias_ih_l0_reverse', None)
                self.register_parameter('bias_hh_l0_reverse', None)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

        # ===== é‡åŒ–çŠ¶æ€ï¼ˆåˆå§‹åŒ–ä¸º Noneï¼Œå»¶è¿Ÿåˆ›å»ºï¼‰ =====
        self.quant_ranges = None  # å‰å‘ C++ å¯¹è±¡ï¼Œcalibrate() æ—¶åˆ›å»º
        self.quant_params = None  # å‰å‘ C++ å¯¹è±¡ï¼Œfinalize_calibration() æ—¶åˆ›å»º
        if bidirectional:
            self.quant_ranges_reverse = None  # åå‘ C++ å¯¹è±¡
            self.quant_params_reverse = None  # åå‘ C++ å¯¹è±¡

        # ===== ä½å®½é…ç½®ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œä½¿ç”¨ Python å­—å…¸ä»¥æ”¯æŒåºåˆ—åŒ–ï¼‰ =====
        self._bitwidth_config_dict = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¦–æ¬¡è®¿é—®æ—¶åˆ›å»ºé»˜è®¤é…ç½®

        # ===== CUDA åˆå§‹åŒ–æ ‡å¿—ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰ =====
        self._cublas_initialized = False

    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡ï¼Œä½¿ç”¨ä¸ nn.GRU ç›¸åŒçš„åˆå§‹åŒ–ç­–ç•¥"""
        stdv = 1.0 / (self.hidden_size ** 0.5)
        for weight in self.parameters():
            nn.init.uniform_(weight, -stdv, stdv)

    # -------------------- CUDA å»¶è¿Ÿåˆå§‹åŒ– --------------------

    def _ensure_cublas_initialized(self):
        """
        ç¡®ä¿ cublas handle å·²åˆå§‹åŒ–ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–æ¨¡å¼ï¼‰
        åªåœ¨ç¬¬ä¸€æ¬¡éœ€è¦æ—¶åˆå§‹åŒ–ï¼Œé¿å…åœ¨ __init__ ä¸­è¿‡æ—©åˆå§‹åŒ–
        """
        if not self._cublas_initialized:
            gru_ops.init_gru_cublas()
            self._cublas_initialized = True

    # -------------------- ä½å®½é…ç½®å†…éƒ¨æ–¹æ³• --------------------

    def _load_bitwidth_config_to_dict(self, config_file: str):
        """ä» JSON æ–‡ä»¶åŠ è½½é…ç½®åˆ°å†…éƒ¨å­—å…¸"""
        # åˆå§‹åŒ–å­—å…¸ï¼ˆåªå­˜å‚¨ç”¨æˆ·æŒ‡å®šçš„é…ç½®ï¼‰
        if self._bitwidth_config_dict is None:
            self._bitwidth_config_dict = {}

        with open(config_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # è¯»å– GRU_config èŠ‚ç‚¹ä¸‹çš„é…ç½®
        gru_config = data.get('GRU_config', {})

        # è¯»å–å…¨å±€é…ç½®
        default_config = gru_config.get('default_config', {})
        if 'disable_quantization' in default_config:
            # disable_quantization=true è¡¨ç¤ºç¦ç”¨é‡åŒ–ï¼Œæ‰€ä»¥ use_quantization å–å
            self.use_quantization = not default_config['disable_quantization']

        op_config = gru_config.get('operator_config', {})

        # å­—æ®µæ˜ å°„: JSON key -> (ä½å®½å±æ€§å, å¯¹ç§°é‡åŒ–å±æ€§å)
        field_map = {
            "input.x": ("x_", "x_symmetric_"),
            "input.h": ("h_", "h_symmetric_"),
            "weight.W": ("W_", "W_symmetric_"),
            "weight.R": ("R_", "R_symmetric_"),
            "weight.bx": ("bx_", "bx_symmetric_"),
            "weight.br": ("br_", "br_symmetric_"),
            "matmul.Wx": ("Wx_", "Wx_symmetric_"),
            "matmul.Rh": ("Rh_", "Rh_symmetric_"),
            "gate.z_pre": ("z_pre_", "z_pre_symmetric_"),
            "gate.z_out": ("z_out_", "z_out_symmetric_"),
            "gate.r_pre": ("r_pre_", "r_pre_symmetric_"),
            "gate.r_out": ("r_out_", "r_out_symmetric_"),
            "gate.g_pre": ("g_pre_", "g_pre_symmetric_"),
            "gate.g_out": ("g_out_", "g_out_symmetric_"),
            "op.Rh_add_br": ("Rh_add_br_", "Rh_add_br_symmetric_"),
            "op.rRh": ("rRh_", "rRh_symmetric_"),
            "op.old_contrib": ("old_contrib_", "old_contrib_symmetric_"),
            "op.new_contrib": ("new_contrib_", "new_contrib_symmetric_"),
        }

        for json_key, (bw_attr, sym_attr) in field_map.items():
            if json_key in op_config:
                op_cfg = op_config[json_key]
                self._bitwidth_config_dict[bw_attr] = op_cfg.get('bitwidth', 8)
                self._bitwidth_config_dict[sym_attr] = op_cfg.get('is_symmetric', True)

    def _get_cpp_bitwidth_config(self) -> gru_ops.OperatorQuantConfig:
        """
        è·å– C++ OperatorQuantConfig å¯¹è±¡
        
        å¦‚æœç”¨æˆ·æœªåŠ è½½è‡ªå®šä¹‰é…ç½®ï¼Œè¿”å›é»˜è®¤çš„ C++ å¯¹è±¡ï¼ˆC++ ç«¯ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        å¦‚æœç”¨æˆ·å·²åŠ è½½é…ç½®ï¼Œä» Python å­—å…¸åˆ›å»º C++ å¯¹è±¡
        """
        config = gru_ops.OperatorQuantConfig()

        # åªæœ‰ç”¨æˆ·åŠ è½½äº†è‡ªå®šä¹‰é…ç½®æ—¶ï¼Œæ‰è¦†ç›– C++ é»˜è®¤å€¼
        if self._bitwidth_config_dict is not None:
            for attr, value in self._bitwidth_config_dict.items():
                setattr(config, attr, value)

        return config

    # -------------------- ä½å®½é…ç½®å…¬å¼€æ¥å£ --------------------

    def load_bitwidth_config(self, config_file: str, verbose: bool = False):
        """
        ä» JSON é…ç½®æ–‡ä»¶åŠ è½½é‡åŒ–ä½å®½é…ç½®
        
        Args:
            config_file: JSON é…ç½®æ–‡ä»¶è·¯å¾„
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        ä½¿ç”¨ç¤ºä¾‹:
            gru.load_bitwidth_config("config/gru_quant_bitwidth_config.json", verbose=True)
        """
        self._load_bitwidth_config_to_dict(config_file)
        if verbose:
            cpp_config = self._get_cpp_bitwidth_config()
            apply_bitwidth_config(cpp_config, config_file, verbose=True)
            print(f"  [å…¨å±€]  use_quantization: {self.use_quantization}")

    # -------------------- æ ¡å‡†çŠ¶æ€æŸ¥è¯¢ --------------------

    def is_calibrated(self) -> bool:
        """
        æ£€æŸ¥é‡åŒ–æ˜¯å¦å·²å®Œæˆæ ¡å‡†

        Returns:
            True å¦‚æœå·²è°ƒç”¨ finalize_calibration()ï¼Œå¦åˆ™ False
            å¯¹äºåŒå‘ GRUï¼Œéœ€è¦æ­£å‘å’Œåå‘éƒ½å·²æ ¡å‡†
        """
        if self.bidirectional:
            return self.quant_params is not None and self.quant_params_reverse is not None
        return self.quant_params is not None

    # -------------------- å…¬å…±æ ¡å‡†æ¥å£ --------------------

    def calibrate(self, calibration_data: torch.Tensor):
        """
        ç´¯ç§¯æ ¡å‡†æ•°æ®ï¼Œæ›´æ–°é‡åŒ–èŒƒå›´

        å¯éšæ—¶è°ƒç”¨ï¼Œæ¯æ¬¡è°ƒç”¨ä¼šå°†æ–°æ•°æ®çš„èŒƒå›´ä¸å·²æœ‰èŒƒå›´åˆå¹¶ï¼ˆå–å¹¶é›†ï¼‰ã€‚
        å®Œæˆæ•°æ®æ”¶é›†åï¼Œéœ€è°ƒç”¨ finalize_calibration() è®¡ç®—é‡åŒ–å‚æ•°ã€‚

        Args:
            calibration_data: æ ¡å‡†æ•°æ®ï¼Œå½¢çŠ¶ä¸º [seq_len, batch, input_size]
                             ï¼ˆå¦‚æœ batch_first=Trueï¼Œåˆ™ä¸º [batch, seq_len, input_size]ï¼‰

        Note:
            - æ ¡å‡†æ—¶æ— éœ€å¼€å¯ use_quantizationï¼Œæ ¡å‡†ä¸é‡åŒ–å¼€å…³è§£è€¦
            - æ”¯æŒå¢é‡æ ¡å‡†ï¼šå³ä½¿å·²è°ƒç”¨è¿‡ finalize_calibration()ï¼Œä»å¯ç»§ç»­è°ƒç”¨
              calibrate() ç´¯ç§¯æ›´å¤šæ•°æ®ï¼Œç„¶åå†æ¬¡è°ƒç”¨ finalize_calibration()
            - æ ¡å‡†å®Œæˆåï¼Œé€šè¿‡è®¾ç½® use_quantization = True å¼€å¯é‡åŒ–æ¨ç†
        """
        self._accumulate_calibration_ranges(calibration_data)

    def finalize_calibration(self):
        """
        å®Œæˆæ ¡å‡†ï¼Œè®¡ç®—é‡åŒ–å‚æ•°å¹¶åˆå§‹åŒ– LUT è¡¨

        æ ¹æ®ç´¯ç§¯çš„é‡åŒ–èŒƒå›´å’Œä½å®½é…ç½®è®¡ç®—å„ç®—å­çš„ scale å’Œ zero_pointã€‚
        å¯å¤šæ¬¡è°ƒç”¨ï¼Œæ¯æ¬¡ä¼šæ ¹æ®å½“å‰ç´¯ç§¯çš„èŒƒå›´é‡æ–°è®¡ç®—é‡åŒ–å‚æ•°ã€‚

        Raises:
            RuntimeError: æœªè°ƒç”¨è¿‡ calibrate()

        Note:
            æ”¯æŒå¢é‡æ ¡å‡†æµç¨‹ï¼š
                calibrate(data1) -> finalize_calibration() -> forward() ->
                calibrate(data2) -> finalize_calibration() -> forward() -> ...
            
            å¦‚æœéœ€è¦è‡ªå®šä¹‰ä½å®½é…ç½®ï¼Œè¯·åœ¨è°ƒç”¨æ­¤æ–¹æ³•å‰å…ˆè°ƒç”¨ load_bitwidth_config()ã€‚
            å¦‚éœ€å®Œå…¨é‡ç½®èŒƒå›´ï¼Œè¯·è°ƒç”¨ reset_calibration()ã€‚
            å¯¹äºåŒå‘ GRUï¼Œä¼šä¸ºæ­£å‘å’Œåå‘åˆ†åˆ«è®¡ç®—é‡åŒ–å‚æ•°ã€‚
        """
        if self.quant_ranges is None:
            raise RuntimeError(
                "No calibration data accumulated. "
                "Call calibrate(data) at least once before finalize_calibration()."
            )

        # ===== å‰å‘æ–¹å‘ï¼šè®¡ç®—é‡åŒ–å‚æ•° =====
        if self._bitwidth_config_dict is not None:
            self.quant_params = gru_ops.calculate_gru_quantitative_parameters(
                quant_ranges=self.quant_ranges,
                bitwidth_config=self._get_cpp_bitwidth_config()
            )
        else:
            self.quant_params = gru_ops.calculate_gru_quantitative_parameters(
                quant_ranges=self.quant_ranges
            )

        # åˆå§‹åŒ–æŸ¥æ‰¾è¡¨ï¼ˆå‰å‘ï¼‰
        gru_ops.initialize_quantization_lut(quant_params=self.quant_params)

        # ===== åå‘æ–¹å‘ï¼šè®¡ç®—é‡åŒ–å‚æ•°ï¼ˆä»…åŒå‘æ—¶ï¼‰ =====
        if self.bidirectional:
            if self.quant_ranges_reverse is None:
                raise RuntimeError(
                    "No reverse calibration data accumulated. "
                    "This should not happen for bidirectional GRU."
                )

            if self._bitwidth_config_dict is not None:
                self.quant_params_reverse = gru_ops.calculate_gru_quantitative_parameters(
                    quant_ranges=self.quant_ranges_reverse,
                    bitwidth_config=self._get_cpp_bitwidth_config()
                )
            else:
                self.quant_params_reverse = gru_ops.calculate_gru_quantitative_parameters(
                    quant_ranges=self.quant_ranges_reverse
                )

            # åˆå§‹åŒ–æŸ¥æ‰¾è¡¨ï¼ˆåå‘ï¼‰
            gru_ops.initialize_quantization_lut(quant_params=self.quant_params_reverse)

    def reset_calibration(self):
        """
        é‡ç½®æ ¡å‡†çŠ¶æ€

        æ¸…é™¤ç´¯ç§¯çš„é‡åŒ–èŒƒå›´å’Œé‡åŒ–å‚æ•°ï¼Œå…è®¸é‡æ–°å¼€å§‹æ ¡å‡†æµç¨‹ã€‚
        å¯¹äºåŒå‘ GRUï¼Œä¼šåŒæ—¶é‡ç½®æ­£å‘å’Œåå‘çš„çŠ¶æ€ã€‚
        """
        self.quant_ranges = None
        self.quant_params = None
        if self.bidirectional:
            self.quant_ranges_reverse = None
            self.quant_params_reverse = None

    # -------------------- è°ƒè¯•ä¸æ‰“å° --------------------

    def print_quant_params(self):
        """
        æ‰“å°é‡åŒ–å‚æ•°

        Raises:
            RuntimeError: æœªè°ƒç”¨è¿‡ finalize_calibration()
        """
        if not self.is_calibrated():
            raise RuntimeError(
                "Quantization parameters not available. "
                "Call finalize_calibration() first."
            )

        params = self.quant_params
        print("=" * 60)
        print("GRUQuantitativeParameters (é‡åŒ–å‚æ•°)")
        print("=" * 60)
        print(f"  hidden_ = {params.hidden_}")
        print(f"  [x]  exp2_inv={params.exp2_inv_x_:3d}, zp={params.zp_x_}")
        print(f"  [h]  exp2_inv={params.exp2_inv_h_:3d}, zp={params.zp_h_}")
        print(f"  [Wx] exp2_inv={params.exp2_inv_Wx_:3d}, zp={params.zp_Wx_}")
        print(f"  [Rh] exp2_inv={params.exp2_inv_Rh_:3d}, zp={params.zp_Rh_}")
        print("-" * 60)
        print(f"  [z_pre] exp2_inv={params.exp2_inv_z_pre_:3d}, zp={params.zp_z_pre_}")
        print(f"  [r_pre] exp2_inv={params.exp2_inv_r_pre_:3d}, zp={params.zp_r_pre_}")
        print(f"  [g_pre] exp2_inv={params.exp2_inv_g_pre_:3d}, zp={params.zp_g_pre_}")
        print(f"  [z_out] exp2_inv={params.exp2_inv_z_out_:3d}, zp={params.zp_z_out_}")
        print(f"  [r_out] exp2_inv={params.exp2_inv_r_out_:3d}, zp={params.zp_r_out_}")
        print(f"  [g_out] exp2_inv={params.exp2_inv_g_out_:3d}, zp={params.zp_g_out_}")
        print("-" * 60)
        print(f"  [Rh_add_br_g]        exp2_inv={params.exp2_inv_Rh_add_br_:3d}, zp={params.zp_Rh_add_br_}")
        print(f"  [rRh]              exp2_inv={params.exp2_inv_rRh_:3d}, zp={params.zp_rRh_}")
        print(f"  [new_contrib]      exp2_inv={params.exp2_inv_new_contrib_:3d}, zp={params.zp_new_contrib_}")
        print(f"  [old_contrib]      exp2_inv={params.exp2_inv_old_contrib_:3d}, zp={params.zp_old_contrib_}")
        print("-" * 60)
        if params.exp2_inv_W_:
            print(f"  [W] exp2_inv (first 5): {list(params.exp2_inv_W_[:5])} ...")
        if params.exp2_inv_R_:
            print(f"  [R] exp2_inv (first 5): {list(params.exp2_inv_R_[:5])} ...")
        if params.exp2_inv_bx_:
            print(f"  [bx] exp2_inv (first 5): {list(params.exp2_inv_bx_[:5])} ...")
        if params.exp2_inv_br_:
            print(f"  [br] exp2_inv (first 5): {list(params.exp2_inv_br_[:5])} ...")
        print("=" * 60)

    def print_quant_ranges(self):
        """
        æ‰“å°é‡åŒ–èŒƒå›´

        Raises:
            RuntimeError: æœªè°ƒç”¨è¿‡ calibrate()
        """
        if self.quant_ranges is None:
            raise RuntimeError(
                "No calibration data accumulated. "
                "Call calibrate(data) first."
            )

        r = self.quant_ranges
        print("=" * 60)
        print("GRUQuantizationRanges (é‡åŒ–èŒƒå›´)")
        print("=" * 60)
        print(f"  hidden_ = {r.hidden_}")
        print(f"  [x]  min={r.min_x_:12.6f}, max={r.max_x_:12.6f}")
        print(f"  [h]  min={r.min_h_:12.6f}, max={r.max_h_:12.6f}")
        print(f"  [Wx] min={r.min_Wx_:12.6f}, max={r.max_Wx_:12.6f}")
        print(f"  [Rh] min={r.min_Rh_:12.6f}, max={r.max_Rh_:12.6f}")
        print("-" * 60)
        print(f"  [z_pre] min={r.min_z_pre_:12.6f}, max={r.max_z_pre_:12.6f}")
        print(f"  [r_pre] min={r.min_r_pre_:12.6f}, max={r.max_r_pre_:12.6f}")
        print(f"  [g_pre] min={r.min_g_pre_:12.6f}, max={r.max_g_pre_:12.6f}")
        print(f"  [z_out] min={r.min_z_out_:12.6f}, max={r.max_z_out_:12.6f}")
        print(f"  [r_out] min={r.min_r_out_:12.6f}, max={r.max_r_out_:12.6f}")
        print(f"  [g_out] min={r.min_g_out_:12.6f}, max={r.max_g_out_:12.6f}")
        print("-" * 60)
        print(f"  [Rh_add_br_g]        min={r.min_Rh_add_br_g_:12.6f}, max={r.max_Rh_add_br_g_:12.6f}")
        print(f"  [rRh]              min={r.min_rRh_:12.6f}, max={r.max_rRh_:12.6f}")
        print(f"  [new_contrib]      min={r.min_new_contrib_:12.6f}, max={r.max_new_contrib_:12.6f}")
        print(f"  [old_contrib]      min={r.min_old_contrib_:12.6f}, max={r.max_old_contrib_:12.6f}")
        print("=" * 60)

    # -------------------- å†…éƒ¨æ–¹æ³• --------------------

    def _convert_weights_to_haste_format(self, device: torch.device, reverse: bool = False):
        """
        å°† PyTorch æ ¼å¼çš„æƒé‡è½¬æ¢ä¸º Haste æ ¼å¼ï¼ˆç”¨äºé‡åŒ–æ ¡å‡†ï¼‰

        Args:
            device: ç›®æ ‡è®¾å¤‡
            reverse: æ˜¯å¦è·å–åå‘æ–¹å‘çš„æƒé‡ï¼ˆä»…åŒå‘ GRU æ—¶æœ‰æ•ˆï¼‰

        Returns:
            W, R, bx, br: Haste æ ¼å¼çš„æƒé‡å’Œåç½®
        """
        if reverse and self.bidirectional:
            weight_ih = ensure_cuda_float32(self.weight_ih_l0_reverse, device)
            weight_hh = ensure_cuda_float32(self.weight_hh_l0_reverse, device)
        else:
            weight_ih = ensure_cuda_float32(self.weight_ih_l0, device)
            weight_hh = ensure_cuda_float32(self.weight_hh_l0, device)

        W = reorder_weights_pytorch_to_haste(weight_ih).t().contiguous()
        R = reorder_weights_pytorch_to_haste(weight_hh).t().contiguous()

        if self.bias:
            if reverse and self.bidirectional:
                bias_ih = ensure_cuda_float32(self.bias_ih_l0_reverse, device)
                bias_hh = ensure_cuda_float32(self.bias_hh_l0_reverse, device)
            else:
                bias_ih = ensure_cuda_float32(self.bias_ih_l0, device)
                bias_hh = ensure_cuda_float32(self.bias_hh_l0, device)
            bx = reorder_weights_pytorch_to_haste(bias_ih).contiguous()
            br = reorder_weights_pytorch_to_haste(bias_hh).contiguous()
        else:
            hidden_size = self.hidden_size
            bx = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)
            br = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)

        return W, R, bx, br

    def _accumulate_calibration_ranges(self, calibration_data: torch.Tensor):
        """ç´¯ç§¯æ ¡å‡†èŒƒå›´ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        # å»¶è¿Ÿåˆå§‹åŒ– cublas
        self._ensure_cublas_initialized()

        # ç¡®ä¿æ ¡å‡†æ•°æ®åœ¨ CUDA ä¸Š
        device = calibration_data.device if calibration_data.is_cuda else torch.device('cuda')
        if not calibration_data.is_cuda:
            calibration_data = calibration_data.to(device)

        # ç¡®ä¿æ¨¡å‹å‚æ•°åœ¨ GPU ä¸Š
        if not next(self.parameters()).is_cuda:
            for param in self.parameters():
                param.data = param.data.to(device)
            for buffer in self.buffers():
                buffer.data = buffer.data.to(device)

        # å¤„ç† batch_first
        if self.batch_first:
            calibration_data = calibration_data.transpose(0, 1).contiguous()

        time_steps, batch_size, input_size = calibration_data.shape
        hidden_size = self.hidden_size

        # ===== å‰å‘æ–¹å‘æ ¡å‡† =====
        W, R, bx, br = self._convert_weights_to_haste_format(device, reverse=False)

        # åˆå§‹åŒ– quant_rangesï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
        if self.quant_ranges is None:
            self.quant_ranges = gru_ops.GRUQuantizationRanges(hidden_size)

        # ç´¯ç§¯æ›´æ–°é‡åŒ–èŒƒå›´ï¼ˆå‰å‘ï¼‰
        gru_ops.calibrate_gru_ranges(
            time_steps=time_steps,
            batch_size=batch_size,
            input_size=input_size,
            hidden_size=hidden_size,
            W=W,
            R=R,
            bx=bx,
            br=br,
            x=calibration_data,
            quant_ranges=self.quant_ranges
        )

        # ===== åå‘æ–¹å‘æ ¡å‡†ï¼ˆä»…åŒå‘æ—¶ï¼‰ =====
        if self.bidirectional:
            W_rev, R_rev, bx_rev, br_rev = self._convert_weights_to_haste_format(device, reverse=True)

            # åˆå§‹åŒ–åå‘ quant_ranges
            if self.quant_ranges_reverse is None:
                self.quant_ranges_reverse = gru_ops.GRUQuantizationRanges(hidden_size)

            # åå‘è¾“å…¥ï¼šæ—¶é—´ç»´åº¦ç¿»è½¬
            calibration_data_reversed = calibration_data.flip(0).contiguous()

            # ç´¯ç§¯æ›´æ–°é‡åŒ–èŒƒå›´ï¼ˆåå‘ï¼‰
            gru_ops.calibrate_gru_ranges(
                time_steps=time_steps,
                batch_size=batch_size,
                input_size=input_size,
                hidden_size=hidden_size,
                W=W_rev,
                R=R_rev,
                bx=bx_rev,
                br=br_rev,
                x=calibration_data_reversed,
                quant_ranges=self.quant_ranges_reverse
            )

        # ç¡®ä¿æƒé‡è¿ç»­æ€§
        self.weight_ih_l0.data = self.weight_ih_l0.data.contiguous()
        self.weight_hh_l0.data = self.weight_hh_l0.data.contiguous()
        if self.bias:
            self.bias_ih_l0.data = self.bias_ih_l0.data.contiguous()
            self.bias_hh_l0.data = self.bias_hh_l0.data.contiguous()

        if self.bidirectional:
            self.weight_ih_l0_reverse.data = self.weight_ih_l0_reverse.data.contiguous()
            self.weight_hh_l0_reverse.data = self.weight_hh_l0_reverse.data.contiguous()
            if self.bias:
                self.bias_ih_l0_reverse.data = self.bias_ih_l0_reverse.data.contiguous()
                self.bias_hh_l0_reverse.data = self.bias_hh_l0_reverse.data.contiguous()

    def _initialize_quantization(self, calibration_data: torch.Tensor):
        """ä¸€æ¬¡æ€§å®Œæˆæ ¡å‡†ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œå‘åå…¼å®¹ï¼‰"""
        self._accumulate_calibration_ranges(calibration_data)
        self.finalize_calibration()

    # -------------------- é‡å†™æ–¹æ³• --------------------

    def forward(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            input: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [seq_len, batch, input_size] æˆ– [batch, seq_len, input_size]
            hx: åˆå§‹éšè—çŠ¶æ€
                - å•å‘: [num_layers, batch, hidden_size]
                - åŒå‘: [num_layers * 2, batch, hidden_size]

        Returns:
            output: è¾“å‡ºå¼ é‡
                - å•å‘: [seq_len, batch, hidden_size] æˆ– [batch, seq_len, hidden_size]
                - åŒå‘: [seq_len, batch, 2*hidden_size] æˆ– [batch, seq_len, 2*hidden_size]
            h_n: æœ€ç»ˆéšè—çŠ¶æ€
                - å•å‘: [num_layers, batch, hidden_size]
                - åŒå‘: [num_layers * 2, batch, hidden_size]

        Raises:
            RuntimeError: å¦‚æœå¯ç”¨äº†é‡åŒ–ä½†æœªæ ¡å‡†
        """
        # åˆå§‹åŒ– cublas
        self._ensure_cublas_initialized()

        # æ£€æŸ¥é‡åŒ–æ˜¯å¦å·²æ ¡å‡†å®Œæˆ
        if self.use_quantization and not self.is_calibrated():
            if self.quant_ranges is not None:
                # å·²ç´¯ç§¯èŒƒå›´ä½†æœªå®Œæˆæ ¡å‡†ï¼Œè‡ªåŠ¨è°ƒç”¨ finalize
                self.finalize_calibration()
            else:
                # æœªè¿›è¡Œä»»ä½•æ ¡å‡†
                raise RuntimeError(
                    "Quantization is enabled but not calibrated. "
                    "Please call calibrate(data) then finalize_calibration() before forward pass."
                )

        # å¤„ç† batch_first
        if self.batch_first:
            input = input.transpose(0, 1).contiguous()  # [B, T, I] -> [T, B, I]

        seq_len, batch_size, input_size = input.shape
        hidden_size = self.hidden_size

        # ç¡®ä¿è¾“å…¥åœ¨ CUDA ä¸Šä¸”ä¸º float32
        device = input.device if input.is_cuda else torch.device('cuda')
        input = ensure_cuda_float32(input, device)

        # å¤„ç†åˆå§‹éšè—çŠ¶æ€
        h0_forward = None
        h0_reverse = None
        if hx is not None:
            expected_layers = self.num_layers * self.num_directions
            expected_shape = (expected_layers, batch_size, hidden_size)
            if hx.shape != expected_shape:
                raise ValueError(
                    f"Expected hx shape {expected_shape} (num_layers*num_directions={expected_layers}, "
                    f"batch_size={batch_size}, hidden_size={hidden_size}), got {hx.shape}"
                )
            h0_forward = ensure_cuda_float32(hx[0], device)
            if self.bidirectional:
                h0_reverse = ensure_cuda_float32(hx[1], device)

        # ===== å‰å‘æ–¹å‘ =====
        weight_ih = self.weight_ih_l0
        weight_hh = self.weight_hh_l0
        bias_ih = self.bias_ih_l0 if self.bias else None
        bias_hh = self.bias_hh_l0 if self.bias else None

        output_forward, h_n_forward = GRUFunction.apply(
            input,
            weight_ih,
            weight_hh,
            bias_ih,
            bias_hh,
            h0_forward,
            self.training,
            self.use_quantization,
            self.quant_params
        )

        if self.bidirectional:
            # ===== åå‘æ–¹å‘ =====
            weight_ih_rev = self.weight_ih_l0_reverse
            weight_hh_rev = self.weight_hh_l0_reverse
            bias_ih_rev = self.bias_ih_l0_reverse if self.bias else None
            bias_hh_rev = self.bias_hh_l0_reverse if self.bias else None

            # åè½¬è¾“å…¥çš„æ—¶é—´ç»´åº¦
            input_reversed = input.flip(0)

            output_reverse, h_n_reverse = GRUFunction.apply(
                input_reversed,
                weight_ih_rev,
                weight_hh_rev,
                bias_ih_rev,
                bias_hh_rev,
                h0_reverse,
                self.training,
                self.use_quantization,
                self.quant_params_reverse
            )

            # åè½¬åå‘è¾“å‡ºä»¥å¯¹é½æ—¶é—´æ­¥
            output_reverse = output_reverse.flip(0)

            # æ‹¼æ¥å‰å‘å’Œåå‘è¾“å‡ºï¼š[T, B, H] + [T, B, H] -> [T, B, 2H]
            output = torch.cat([output_forward, output_reverse], dim=-1)

            # æ‹¼æ¥éšè—çŠ¶æ€ï¼š[1, B, H] + [1, B, H] -> [2, B, H]
            h_n = torch.cat([h_n_forward, h_n_reverse], dim=0)
        else:
            output = output_forward
            h_n = h_n_forward

        # å¤„ç† batch_first
        if self.batch_first:
            output = output.transpose(0, 1).contiguous()  # [T, B, H] -> [B, T, H]

        return output, h_n
