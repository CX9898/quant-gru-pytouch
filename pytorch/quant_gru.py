"""
QuantGRU - æ”¯æŒé‡åŒ–çš„ GRU å®ç°

åŠŸèƒ½ç‰¹æ€§:
    - å…¼å®¹ nn.GRU æ¥å£ï¼ˆæ”¯æŒ batch_firstã€bidirectional ç­‰å‚æ•°ï¼‰
    - æ”¯æŒ INT8/INT16/INT32 é‡åŒ–æ¨ç†
    - æ”¯æŒ MinMax å’Œ AIMET é£æ ¼ç›´æ–¹å›¾æ ¡å‡†
    - å»¶è¿Ÿåˆå§‹åŒ–è®¾è®¡ï¼Œæ”¯æŒ pickle/deepcopy åºåˆ—åŒ–
    - æ”¯æŒ ONNX å¯¼å‡ºï¼ˆä½¿ç”¨çº¯ PyTorch å®ç°ï¼‰
    - é‡åŒ–æ¨¡å¼ä¸‹ä½¿ç”¨çº¯å®šç‚¹è®¡ç®—ï¼Œä¸ CUDA å®ç°å®Œå…¨ä¸€è‡´

å…³é”®å±æ€§:
    - use_quantization: æ˜¯å¦å¯ç”¨é‡åŒ–ï¼ˆé»˜è®¤ Falseï¼‰
    - export_mode: æ˜¯å¦ä½¿ç”¨ ONNX å¯¼å‡ºæ¨¡å¼ï¼ˆé»˜è®¤ Falseï¼‰
    - export_format: å¯¼å‡ºæ ¼å¼ 'float'|'qdq'|'fixedpoint'ï¼ˆé«˜çº§é€‰é¡¹ï¼Œé»˜è®¤ 'float'ï¼‰

å…¸å‹ç”¨æ³•:
    >>> from quant_gru import QuantGRU
    >>>
    >>> # åˆ›å»ºå¹¶æ ¡å‡†æ¨¡å‹
    >>> gru = QuantGRU(64, 128, batch_first=True).cuda()
    >>> gru.calibrate(calibration_data)
    >>> gru.use_quantization = True
    >>>
    >>> # æ­£å¸¸æ¨ç†ï¼ˆCUDA é‡åŒ–æ¨¡å¼ï¼‰
    >>> output = gru(x)
    
ONNX å¯¼å‡º:
    >>> # å¯ç”¨å¯¼å‡ºæ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨æµ®ç‚¹æ ¼å¼ï¼‰
    >>> gru.export_mode = True
    >>> torch.onnx.export(gru, x, "model.onnx")
    >>> gru.export_mode = False  # æ¢å¤
    >>> 
    >>> # é‡åŒ–æ¨¡å‹å¯¼å‡ºéœ€æŒ‡å®šæ ¼å¼
    >>> gru.export_format = 'qdq'  # 'float' | 'qdq' | 'fixedpoint'
"""

import json
import torch
import torch.nn as nn
from typing import Optional, Tuple

try:
    import gru_interface_binding as gru_ops
except ImportError:
    raise ImportError(
        "gru_interface_binding æ¨¡å—æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ setup.py ç¼–è¯‘ C++ æ‰©å±•"
    )


# ============================================================
#                      ä½å®½é…ç½®å·¥å…·å‡½æ•°
# ============================================================


def _get_bitwidth_value(op_cfg: dict) -> int:
    """ä»é…ç½®ä¸­è·å–ä½å®½å€¼ï¼ˆ8/16/32ï¼‰ï¼Œé»˜è®¤ 8"""
    return op_cfg.get('bitwidth', 8)


def _get_symmetric_value(op_cfg: dict) -> bool:
    """ä»é…ç½®ä¸­è·å–æ˜¯å¦å¯¹ç§°é‡åŒ–ï¼Œé»˜è®¤ True"""
    return op_cfg.get('is_symmetric', True)


def load_bitwidth_config(config_file: str) -> gru_ops.OperatorQuantConfig:
    """
    ä» JSON æ–‡ä»¶åŠ è½½é‡åŒ–é…ç½®
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        OperatorQuantConfig å¯¹è±¡
    """
    with open(config_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    config = gru_ops.OperatorQuantConfig()
    gru_config = data.get('GRU_config', {})
    op_config = gru_config.get('operator_config', {})

    # å­—æ®µæ˜ å°„: JSON key -> (ä½å®½å±æ€§å, å¯¹ç§°é‡åŒ–å±æ€§å)
    field_map = {
        "input.x": ("x_", "x_symmetric_"),
        "input.h": ("h_", "h_symmetric_"),
        "weight.W": ("W_", "W_symmetric_"),
        "weight.R": ("R_", "R_symmetric_"),
        "weight.bx": ("bx_", "bx_symmetric_"),
        "weight.br": ("br_", "br_symmetric_"),
        "matmul.Wx": ("Wx_", "Wx_symmetric_"),
        "matmul.Rh": ("Rh_", "Rh_symmetric_"),
        "gate.z_pre": ("z_pre_", "z_pre_symmetric_"),
        "gate.z_out": ("z_out_", "z_out_symmetric_"),
        "gate.r_pre": ("r_pre_", "r_pre_symmetric_"),
        "gate.r_out": ("r_out_", "r_out_symmetric_"),
        "gate.g_pre": ("g_pre_", "g_pre_symmetric_"),
        "gate.g_out": ("g_out_", "g_out_symmetric_"),
        "op.Rh_add_br": ("Rh_add_br_", "Rh_add_br_symmetric_"),
        "op.rRh": ("rRh_", "rRh_symmetric_"),
        "op.old_contrib": ("old_contrib_", "old_contrib_symmetric_"),
        "op.new_contrib": ("new_contrib_", "new_contrib_symmetric_"),
    }

    for json_key, (bw_attr, sym_attr) in field_map.items():
        if json_key in op_config:
            op_cfg = op_config[json_key]
            # è®¾ç½®ä½å®½
            bw_val = _get_bitwidth_value(op_cfg)
            setattr(config, bw_attr, bw_val)
            # è®¾ç½®å¯¹ç§°é‡åŒ–é…ç½®
            sym_val = _get_symmetric_value(op_cfg)
            setattr(config, sym_attr, sym_val)

    return config


def _format_bitwidth(val: int) -> str:
    """æ ¼å¼åŒ–ä½å®½å€¼: 8 -> '8bit'"""
    return f"{abs(val)}bit"


def _format_symmetric(is_symmetric: bool) -> str:
    """æ ¼å¼åŒ–å¯¹ç§°é‡åŒ–: True -> 'å¯¹ç§°'"""
    return "å¯¹ç§°" if is_symmetric else "éå¯¹ç§°"


def apply_bitwidth_config(config: gru_ops.OperatorQuantConfig,
                          config_file: str,
                          verbose: bool = False) -> int:
    """
    ä» JSON æ–‡ä»¶åº”ç”¨é…ç½®åˆ°ç°æœ‰ OperatorQuantConfig å¯¹è±¡
    
    Args:
        config: è¦æ›´æ–°çš„é…ç½®å¯¹è±¡
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        verbose: æ˜¯å¦æ‰“å°é…ç½®è¯¦æƒ…
        
    Returns:
        é…ç½®çš„å­—æ®µæ•°é‡
    """
    loaded = load_bitwidth_config(config_file)

    # ä½å®½é…ç½®å­—æ®µï¼ˆ18 ä¸ªï¼‰
    bitwidth_attrs = ['x_', 'h_', 'W_', 'R_', 'bx_', 'br_', 'Wx_', 'Rh_',
                      'z_pre_', 'z_out_', 'r_pre_', 'r_out_', 'g_pre_', 'g_out_',
                      'Rh_add_br_', 'rRh_', 'old_contrib_', 'new_contrib_']
    for attr in bitwidth_attrs:
        setattr(config, attr, getattr(loaded, attr))

    # å¯¹ç§°é‡åŒ–é…ç½®å­—æ®µï¼ˆ18 ä¸ªï¼‰
    symmetric_attrs = ['x_symmetric_', 'h_symmetric_', 'W_symmetric_', 'R_symmetric_',
                       'bx_symmetric_', 'br_symmetric_', 'Wx_symmetric_', 'Rh_symmetric_',
                       'z_pre_symmetric_', 'z_out_symmetric_', 'r_pre_symmetric_', 'r_out_symmetric_',
                       'g_pre_symmetric_', 'g_out_symmetric_', 'Rh_add_br_symmetric_', 'rRh_symmetric_',
                       'old_contrib_symmetric_', 'new_contrib_symmetric_']
    for attr in symmetric_attrs:
        setattr(config, attr, getattr(loaded, attr))

    if verbose:
        print("\n" + "=" * 70)
        print("ğŸ”§ åº”ç”¨ GRU é‡åŒ–é…ç½®ï¼ˆä½å®½ + å¯¹ç§°é‡åŒ–ï¼‰")
        print("=" * 70)
        print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config_file}")
        print("-" * 70)
        print(f"  [è¾“å…¥]  x: {_format_bitwidth(config.x_):6s} ({_format_symmetric(config.x_symmetric_)})")
        print(f"          h: {_format_bitwidth(config.h_):6s} ({_format_symmetric(config.h_symmetric_)})")
        print(f"  [æƒé‡]  W: {_format_bitwidth(config.W_):6s} ({_format_symmetric(config.W_symmetric_)})")
        print(f"          R: {_format_bitwidth(config.R_):6s} ({_format_symmetric(config.R_symmetric_)})")
        print(f"          bx: {_format_bitwidth(config.bx_):6s} ({_format_symmetric(config.bx_symmetric_)})")
        print(f"          br: {_format_bitwidth(config.br_):6s} ({_format_symmetric(config.br_symmetric_)})")
        print(f"  [çŸ©é˜µ]  Wx: {_format_bitwidth(config.Wx_):6s} ({_format_symmetric(config.Wx_symmetric_)})")
        print(f"          Rh: {_format_bitwidth(config.Rh_):6s} ({_format_symmetric(config.Rh_symmetric_)})")
        print(f"  [é—¨æ§]  z_pre: {_format_bitwidth(config.z_pre_):6s} ({_format_symmetric(config.z_pre_symmetric_)})")
        print(f"          z_out: {_format_bitwidth(config.z_out_):6s} ({_format_symmetric(config.z_out_symmetric_)})")
        print(f"          r_pre: {_format_bitwidth(config.r_pre_):6s} ({_format_symmetric(config.r_pre_symmetric_)})")
        print(f"          r_out: {_format_bitwidth(config.r_out_):6s} ({_format_symmetric(config.r_out_symmetric_)})")
        print(f"          g_pre: {_format_bitwidth(config.g_pre_):6s} ({_format_symmetric(config.g_pre_symmetric_)})")
        print(f"          g_out: {_format_bitwidth(config.g_out_):6s} ({_format_symmetric(config.g_out_symmetric_)})")
        print(
            f"  [è¿ç®—]  Rh+br: {_format_bitwidth(config.Rh_add_br_):6s} ({_format_symmetric(config.Rh_add_br_symmetric_)})")
        print(f"          rRh: {_format_bitwidth(config.rRh_):6s} ({_format_symmetric(config.rRh_symmetric_)})")
        print(
            f"  [è¾“å‡º]  old: {_format_bitwidth(config.old_contrib_):6s} ({_format_symmetric(config.old_contrib_symmetric_)})")
        print(
            f"          new: {_format_bitwidth(config.new_contrib_):6s} ({_format_symmetric(config.new_contrib_symmetric_)})")
        print("=" * 70 + "\n")

    return len(bitwidth_attrs) + len(symmetric_attrs)  # 36 ä¸ªå­—æ®µ


# ============================================================
#                      æƒé‡æ ¼å¼è½¬æ¢
# ============================================================

def reorder_weights_pytorch_to_haste(w: torch.Tensor) -> torch.Tensor:
    """
    PyTorch æƒé‡æ ¼å¼ (r,z,n) -> Haste æ ¼å¼ (z,r,n)
    
    Args:
        w: å½¢çŠ¶ [3*H, ...] çš„æƒé‡å¼ é‡
        
    Returns:
        é‡æ’åºåçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸å˜
    """
    w = w.contiguous()
    h3 = w.shape[0] // 3
    device = w.device
    # [r, z, n] -> [z, r, n]
    indices = torch.cat([
        torch.arange(h3, 2 * h3, device=device),
        torch.arange(0, h3, device=device),
        torch.arange(2 * h3, 3 * h3, device=device)
    ])
    return w.index_select(0, indices).contiguous()


def reorder_weights_haste_to_pytorch(w: torch.Tensor) -> torch.Tensor:
    """
    Haste æƒé‡æ ¼å¼ (z,r,n) -> PyTorch æ ¼å¼ (r,z,n)
    
    Args:
        w: å½¢çŠ¶ [3*H, ...] çš„æƒé‡å¼ é‡
        
    Returns:
        é‡æ’åºåçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸å˜
    """
    w = w.contiguous()
    h3 = w.shape[0] // 3
    device = w.device
    # [z, r, n] -> [r, z, n]
    indices = torch.cat([
        torch.arange(h3, 2 * h3, device=device),
        torch.arange(0, h3, device=device),
        torch.arange(2 * h3, 3 * h3, device=device)
    ])
    return w.index_select(0, indices).contiguous()


def ensure_cuda_float32(tensor: torch.Tensor, device: torch.device) -> torch.Tensor:
    """ç¡®ä¿å¼ é‡åœ¨ CUDA ä¸Šä¸”ä¸º float32 ç±»å‹"""
    if not tensor.is_cuda:
        tensor = tensor.to(device)
    if tensor.dtype != torch.float32:
        tensor = tensor.float()
    return tensor


# ============================================================
#                      å®šç‚¹è¿ç®—è¾…åŠ©å‡½æ•°
# ============================================================

def rshift_round(x: torch.Tensor, n: int) -> torch.Tensor:
    """
    å¸¦å››èˆäº”å…¥çš„å³ç§»æ“ä½œï¼ˆä¸ CUDA å®ç°ä¸€è‡´ï¼‰
    
    Args:
        x: è¾“å…¥å¼ é‡ï¼ˆæ•´æ•°ç±»å‹ï¼‰
        n: ç§»ä½ä½æ•°ï¼ˆå¯ä»¥ä¸ºè´Ÿï¼Œè¡¨ç¤ºå·¦ç§»ï¼‰
        
    Returns:
        ç§»ä½åçš„å¼ é‡
    """
    if n <= 0:
        return x * (1 << (-n))  # å·¦ç§»
    
    # å³ç§»å¸¦å››èˆäº”å…¥
    offset = 1 << (n - 1)
    # å¤„ç†æ­£æ•°å’Œè´Ÿæ•°
    positive_mask = x >= 0
    result = torch.where(
        positive_mask,
        (x + offset) >> n,
        -(((-x) + offset) >> n)
    )
    return result


def rshift_round_i64(x: torch.Tensor, n: int) -> torch.Tensor:
    """
    å¸¦å››èˆäº”å…¥çš„å³ç§»æ“ä½œï¼ˆint64 ç‰ˆæœ¬ï¼Œç”¨äº 16 ä½é‡åŒ–çš„ä¹˜ç§¯ï¼‰
    """
    if n <= 0:
        return x * (1 << (-n))
    
    offset = 1 << (n - 1)
    positive_mask = x >= 0
    result = torch.where(
        positive_mask,
        (x + offset) >> n,
        -(((-x) + offset) >> n)
    )
    return result


def rshift_round_per_channel(x: torch.Tensor, n: torch.Tensor) -> torch.Tensor:
    """
    Per-channel å¸¦å››èˆäº”å…¥çš„å³ç§»æ“ä½œï¼ˆONNX å¯å¯¼å‡ºï¼‰
    
    Args:
        x: è¾“å…¥å¼ é‡ [B, C] æˆ– [B*T, C]ï¼Œint64 ç±»å‹
        n: æ¯ä¸ª channel çš„ç§»ä½é‡ [C]ï¼Œint8 ç±»å‹
        
    Returns:
        ç§»ä½åçš„å¼ é‡
    """
    # å°† n æ‰©å±•ä¸ºä¸ x ç›¸åŒç»´åº¦ [1, C] ä¾¿äºå¹¿æ’­
    n_expanded = n.unsqueeze(0).to(torch.int64)  # [1, C]
    
    # è®¡ç®— offset = 2^(n-1)ï¼Œå¯¹äº n <= 0 è®¾ä¸º 0
    # ä½¿ç”¨ clamp ç¡®ä¿ n-1 >= 0 æ—¶æ‰è®¡ç®— offset
    n_clamped = torch.clamp(n_expanded - 1, min=0)
    offset = (torch.ones_like(x) << n_clamped)
    # å¯¹äº n <= 0ï¼Œoffset åº”è¯¥ä¸º 0
    offset = torch.where(n_expanded > 0, offset, torch.zeros_like(offset))
    
    # è®¡ç®—å³ç§»æˆ–å·¦ç§»
    # å³ç§»ï¼š(x + offset) >> n æˆ– -(-x + offset) >> n
    # å·¦ç§»ï¼šx << (-n)
    positive_mask = x >= 0
    
    # å³ç§»ç»“æœ
    rshift_pos = (x + offset) >> n_expanded
    rshift_neg = -(((-x) + offset) >> n_expanded)
    rshift_result = torch.where(positive_mask, rshift_pos, rshift_neg)
    
    # å·¦ç§»ç»“æœ
    lshift_result = x << (-n_expanded)
    
    # æ ¹æ® n çš„æ­£è´Ÿé€‰æ‹©ç»“æœ
    result = torch.where(n_expanded > 0, rshift_result, lshift_result)
    
    return result


def clamp_to_int8(x: torch.Tensor) -> torch.Tensor:
    """æˆªæ–­åˆ° INT8 èŒƒå›´ [-128, 127]"""
    return torch.clamp(x, -128, 127).to(torch.int32)


def clamp_to_int16(x: torch.Tensor) -> torch.Tensor:
    """æˆªæ–­åˆ° INT16 èŒƒå›´ [-32768, 32767]"""
    return torch.clamp(x, -32768, 32767).to(torch.int32)


def clamp_to_uint8(x: torch.Tensor) -> torch.Tensor:
    """æˆªæ–­åˆ° UINT8 èŒƒå›´ [0, 255]"""
    return torch.clamp(x, 0, 255).to(torch.int32)


def clamp_to_uint16(x: torch.Tensor) -> torch.Tensor:
    """æˆªæ–­åˆ° UINT16 èŒƒå›´ [0, 65535]"""
    return torch.clamp(x, 0, 65535).to(torch.int32)


def quantize(x: torch.Tensor, exp2_inv: int, zp: int = 0, 
             bitwidth: int = 8, symmetric: bool = True) -> torch.Tensor:
    """
    é‡åŒ–å¼ é‡ï¼ˆä¸ CUDA quantize<QuantT> å‡½æ•°ä¸€è‡´ï¼‰
    
    é‡åŒ–å…¬å¼: q = clamp(round(x / scale) + zp, qmin, qmax)
    å…¶ä¸­ scale = 2^(-exp2_inv)
    
    Args:
        x: æµ®ç‚¹è¾“å…¥å¼ é‡
        exp2_inv: scale = 2^(-exp2_inv)
        zp: zero pointï¼ˆé»˜è®¤0ï¼Œå¯¹ç§°é‡åŒ–ï¼‰
        bitwidth: ç›®æ ‡ä½å®½ (8, 16, 32)
        symmetric: æ˜¯å¦å¯¹ç§°é‡åŒ–
        
    Returns:
        é‡åŒ–åçš„æ•´æ•°å¼ é‡ (int32)
    """
    # æ ¹æ®ä½å®½ç¡®å®šé‡åŒ–èŒƒå›´
    if bitwidth == 8:
        qmin, qmax = (-128, 127) if symmetric else (0, 255)
    elif bitwidth == 16:
        qmin, qmax = (-32768, 32767) if symmetric else (0, 65535)
    else:  # INT32
        qmin, qmax = (-2147483648, 2147483647)
    
    if exp2_inv >= 0:
        scale = 1.0 / (1 << exp2_inv)
    else:
        scale = float(1 << (-exp2_inv))
    
    # q = round(x / scale) + zp
    q = torch.round(x / scale).to(torch.int32) + zp
    q = torch.clamp(q, qmin, qmax)
    
    return q


def dequantize(q: torch.Tensor, exp2_inv: int, zp: int = 0) -> torch.Tensor:
    """
    åé‡åŒ–å¼ é‡ï¼ˆä¸ CUDA dequantize å‡½æ•°ä¸€è‡´ï¼‰
    
    åé‡åŒ–å…¬å¼: x = (q - zp) * scale
    å…¶ä¸­ scale = 2^(-exp2_inv)
    
    Args:
        q: é‡åŒ–æ•´æ•°å¼ é‡
        exp2_inv: scale = 2^(-exp2_inv)
        zp: zero point
        
    Returns:
        åé‡åŒ–åçš„æµ®ç‚¹å¼ é‡
    """
    v = q.to(torch.int32) - zp
    
    if exp2_inv >= 0:
        return v.float() / float(1 << exp2_inv)
    else:
        return v.float() * float(1 << (-exp2_inv))


def quantize_per_channel(x: torch.Tensor, exp2_invs: list, zp: int = 0,
                         bitwidth: int = 8, symmetric: bool = True) -> torch.Tensor:
    """
    Per-channel é‡åŒ–ï¼ˆä¸ CUDA quantificationPerChannel ä¸€è‡´ï¼‰
    
    Args:
        x: æµ®ç‚¹è¾“å…¥å¼ é‡ï¼Œshape [..., channel_size]
        exp2_invs: æ¯ä¸ª channel çš„ exp2_inv åˆ—è¡¨
        zp: zero pointï¼ˆé»˜è®¤0ï¼Œå¯¹ç§°é‡åŒ–ï¼‰
        bitwidth: ç›®æ ‡ä½å®½ (8, 16, 32)
        symmetric: æ˜¯å¦å¯¹ç§°é‡åŒ–
        
    Returns:
        é‡åŒ–åçš„æ•´æ•°å¼ é‡ (int32)
    """
    # æ ¹æ®ä½å®½ç¡®å®šé‡åŒ–èŒƒå›´
    if bitwidth == 8:
        qmin, qmax = (-128, 127) if symmetric else (0, 255)
    elif bitwidth == 16:
        qmin, qmax = (-32768, 32767) if symmetric else (0, 65535)
    else:  # INT32
        qmin, qmax = (-2147483648, 2147483647)
    
    device = x.device
    channel_size = len(exp2_invs)
    q = torch.zeros_like(x, dtype=torch.int32, device=device)
    
    for c in range(channel_size):
        exp2_inv = exp2_invs[c]
        if exp2_inv >= 0:
            scale = 1.0 / (1 << exp2_inv)
        else:
            scale = float(1 << (-exp2_inv))
        
        q[..., c] = torch.clamp(
            torch.round(x[..., c] / scale).to(torch.int32) + zp,
            qmin, qmax
        )
    
    return q


# ============================================================
#                      QDQ (Quantize-Dequantize) è¾…åŠ©å‡½æ•°
#                      ç”¨äº ONNX å¯¼å‡ºçš„ä¼ªé‡åŒ–æ“ä½œ
# ============================================================

def fake_quantize(x: torch.Tensor, exp2_inv: int, zp: int = 0,
                  bitwidth: int = 8, symmetric: bool = True) -> torch.Tensor:
    """
    ä¼ªé‡åŒ–ï¼ˆFake Quantizeï¼‰: é‡åŒ–åç«‹å³åé‡åŒ–ï¼Œä¿æŒæµ®ç‚¹æ ¼å¼
    
    ç”¨äº ONNX å¯¼å‡ºï¼Œæ¨ç†å¼•æ“ä¼šè¯†åˆ« QDQ æ¨¡å¼å¹¶ä¼˜åŒ–
    
    [ä¸ CUDA ä¸€è‡´] é‡åŒ–å‚æ•° (exp2_inv, zp) ä¸ CUDA ç«¯å®Œå…¨ä¸€è‡´
    [ONNX å…¼å®¹] ä½¿ç”¨æµ®ç‚¹è¿ç®—æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœ
    """
    # è®¡ç®— scale
    if exp2_inv >= 0:
        scale = 1.0 / (1 << exp2_inv)
    else:
        scale = float(1 << (-exp2_inv))
    
    # ç¡®å®šé‡åŒ–èŒƒå›´
    if bitwidth == 8:
        qmin, qmax = (-128, 127) if symmetric else (0, 255)
    elif bitwidth == 16:
        qmin, qmax = (-32768, 32767) if symmetric else (0, 65535)
    else:
        qmin, qmax = (-2147483648, 2147483647)
    
    # é‡åŒ–: q = clamp(round(x / scale) + zp, qmin, qmax)
    q = torch.clamp(torch.round(x / scale) + zp, qmin, qmax)
    
    # åé‡åŒ–: x' = (q - zp) * scale
    x_dequant = (q - zp) * scale
    
    return x_dequant


def fake_quantize_per_channel(x: torch.Tensor, exp2_invs: list, zp: int = 0,
                               bitwidth: int = 8, symmetric: bool = True) -> torch.Tensor:
    """
    Per-channel ä¼ªé‡åŒ–
    
    [ä¸ CUDA ä¸€è‡´] per-channel é‡åŒ–å‚æ•°ä¸ CUDA quantificationPerChannel ä¸€è‡´
    [ONNX å…¼å®¹] ä½¿ç”¨æµ®ç‚¹è¿ç®—æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœ
    """
    if bitwidth == 8:
        qmin, qmax = (-128, 127) if symmetric else (0, 255)
    elif bitwidth == 16:
        qmin, qmax = (-32768, 32767) if symmetric else (0, 65535)
    else:
        qmin, qmax = (-2147483648, 2147483647)
    
    device = x.device
    result = torch.zeros_like(x)
    channel_size = len(exp2_invs)
    
    for c in range(channel_size):
        exp2_inv = exp2_invs[c]
        if exp2_inv >= 0:
            scale = 1.0 / (1 << exp2_inv)
        else:
            scale = float(1 << (-exp2_inv))
        
        q = torch.clamp(torch.round(x[..., c] / scale) + zp, qmin, qmax)
        result[..., c] = (q - zp) * scale
    
    return result


# ============================================================
#                      åˆ†æ®µçº¿æ€§ LUT å®ç°
# ============================================================

class SegmentParams:
    """åˆ†æ®µçº¿æ€§å‚æ•°ï¼ˆä¸ CUDA SegmentParams_INT16/INT8 å¯¹åº”ï¼‰"""
    def __init__(self, q_b: int, n_BX_total: int, term_c_precomputed: int, threshold: int):
        self.q_b = q_b                           # é‡åŒ–æ–œç‡
        self.n_BX_total = n_BX_total             # èåˆç§»ä½ä½æ•°
        self.term_c_precomputed = term_c_precomputed  # é¢„è®¡ç®—å¸¸æ•°é¡¹
        self.threshold = threshold               # æ®µé˜ˆå€¼


class PiecewiseLUT:
    """åˆ†æ®µçº¿æ€§æŸ¥æ‰¾è¡¨ï¼ˆä¸ CUDA SigmoidLUT_INT16/INT8 å¯¹åº”ï¼‰"""
    NUM_SEGMENTS = 16
    
    def __init__(self, zp_x: int, shift_bits_x: int, shift_bits_y: int, zp_y: int):
        self.segments = []  # List[SegmentParams]
        self.zp_x = zp_x
        self.shift_bits_x = shift_bits_x
        self.shift_bits_y = shift_bits_y
        self.zp_y = zp_y


def find_segment(q_x: torch.Tensor, thresholds: torch.Tensor) -> torch.Tensor:
    """
    æ®µæŸ¥æ‰¾å‡½æ•°ï¼ˆONNX å¯å¯¼å‡ºçš„å‘é‡åŒ–å®ç°ï¼‰
    
    Args:
        q_x: é‡åŒ–è¾“å…¥å¼ é‡ [N]
        thresholds: å„æ®µé˜ˆå€¼å¼ é‡ [num_segments]
        
    Returns:
        æ¯ä¸ªå…ƒç´ å¯¹åº”çš„æ®µç´¢å¼• [N]
    """
    # æ‰©å±•ç»´åº¦è¿›è¡Œæ¯”è¾ƒ: q_x [N, 1] >= thresholds [1, S] -> [N, S]
    q_x_expanded = q_x.unsqueeze(-1)  # [N, 1]
    thresholds_expanded = thresholds.unsqueeze(0)  # [1, S]
    
    # æ¯”è¾ƒå¹¶ç´¯åŠ å¾—åˆ°æ®µç´¢å¼•
    # æ¯ä¸ªå…ƒç´  >= threshold[i] åˆ™ç´¯åŠ  1
    comparisons = (q_x_expanded >= thresholds_expanded).to(torch.long)  # [N, S]
    seg_ids = comparisons.sum(dim=-1)  # [N]
    
    # ç¡®ä¿ä¸è¶…è¿‡æœ€åä¸€æ®µ
    seg_ids = torch.clamp(seg_ids, 0, thresholds.shape[0] - 1)
    return seg_ids


def rshift_round_by_index(bx: torch.Tensor, n_BXs: torch.Tensor, seg_ids: torch.Tensor) -> torch.Tensor:
    """
    æ ¹æ®æ®µç´¢å¼•è¿›è¡Œ rshift_roundï¼ˆONNX å¯å¯¼å‡ºï¼‰
    
    Args:
        bx: è¾“å…¥å¼ é‡ [N] (int64)
        n_BXs: æ¯æ®µçš„ç§»ä½é‡ [num_segments] (int8)
        seg_ids: æ®µç´¢å¼• [N]
        
    Returns:
        ç§»ä½åçš„å¼ é‡ [N]
    """
    # é€‰æ‹©å¯¹åº”çš„ n å€¼
    n_selected = n_BXs[seg_ids].to(torch.int64)  # [N]
    
    # è®¡ç®— offset = 2^(n-1)ï¼Œå¯¹äº n <= 0 è®¾ä¸º 0
    n_clamped = torch.clamp(n_selected - 1, min=0)
    offset = torch.ones_like(bx) << n_clamped
    offset = torch.where(n_selected > 0, offset, torch.zeros_like(offset))
    
    # å³ç§»ç»“æœ
    positive_mask = bx >= 0
    rshift_pos = (bx + offset) >> n_selected
    rshift_neg = -(((-bx) + offset) >> n_selected)
    rshift_result = torch.where(positive_mask, rshift_pos, rshift_neg)
    
    # å·¦ç§»ç»“æœ
    lshift_result = bx << (-n_selected)
    
    # æ ¹æ® n çš„æ­£è´Ÿé€‰æ‹©ç»“æœ
    result = torch.where(n_selected > 0, rshift_result, lshift_result)
    
    return result


def piecewise_linear_forward(q_x: torch.Tensor, lut: PiecewiseLUT, 
                             output_signed: bool = True, bitwidth: int = 16) -> torch.Tensor:
    """
    åˆ†æ®µçº¿æ€§è¿‘ä¼¼å‰å‘è®¡ç®—ï¼ˆONNX å¯å¯¼å‡ºï¼Œä¸ CUDA sigmoid/tanh_piecewise_linear ä¸€è‡´ï¼‰
    
    Args:
        q_x: é‡åŒ–è¾“å…¥å¼ é‡ï¼ˆint32ï¼‰
        lut: åˆ†æ®µçº¿æ€§æŸ¥æ‰¾è¡¨
        output_signed: è¾“å‡ºæ˜¯å¦æœ‰ç¬¦å·ï¼ˆsigmoid=False, tanh=Trueï¼‰
        bitwidth: è¾“å‡ºä½å®½ (8 æˆ– 16)
        
    Returns:
        é‡åŒ–è¾“å‡ºå¼ é‡
    """
    device = q_x.device
    original_shape = q_x.shape
    q_x_flat = q_x.flatten()
    
    # æ”¶é›† LUT å‚æ•°ä¸ºå¼ é‡
    thresholds = torch.tensor([seg.threshold for seg in lut.segments], device=device, dtype=torch.int32)
    q_bs = torch.tensor([seg.q_b for seg in lut.segments], device=device, dtype=torch.int64)
    n_BXs = torch.tensor([seg.n_BX_total for seg in lut.segments], device=device, dtype=torch.int8)
    term_cs = torch.tensor([seg.term_c_precomputed for seg in lut.segments], device=device, dtype=torch.int64)
    
    # Step 1: æ®µæŸ¥æ‰¾ï¼ˆå‘é‡åŒ–ï¼‰
    seg_ids = find_segment(q_x_flat, thresholds)
    
    # Step 2: x_offset = q_x - zp_x
    x_offset = q_x_flat.to(torch.int64) - lut.zp_x
    
    # Step 3: bx = q_b * x_offset
    q_b_selected = q_bs[seg_ids]
    bx = q_b_selected * x_offset
    
    # Step 4: term_bx = bx >> n_BX_totalï¼ˆå‘é‡åŒ–ï¼‰
    term_bx = rshift_round_by_index(bx, n_BXs, seg_ids)
    
    # Step 5: q_y = term_bx + term_c
    term_c_selected = term_cs[seg_ids]
    y = term_bx + term_c_selected
    
    # Step 6: clamp
    if bitwidth == 16:
        if output_signed:
            y = torch.clamp(y, -32768, 32767)
        else:
            y = torch.clamp(y, 0, 65535)
    else:  # INT8
        if output_signed:
            y = torch.clamp(y, -128, 127)
        else:
            y = torch.clamp(y, 0, 255)
    
    return y.to(torch.int32).view(original_shape)


def generate_sigmoid_lut(exp2_inv_x: int, zp_x: int, exp2_inv_y: int, zp_y: int,
                         x_min: float = -8.0, x_max: float = 8.0, 
                         bitwidth: int = 16) -> PiecewiseLUT:
    """
    ç”Ÿæˆ Sigmoid åˆ†æ®µçº¿æ€§ LUTï¼ˆä¸ CUDA generate_sigmoid_lut_int16/int8 ä¸€è‡´ï¼‰
    
    Args:
        exp2_inv_x: è¾“å…¥é‡åŒ–å‚æ•°
        zp_x: è¾“å…¥é›¶ç‚¹
        exp2_inv_y: è¾“å‡ºé‡åŒ–å‚æ•°
        zp_y: è¾“å‡ºé›¶ç‚¹
        x_min, x_max: sigmoid æœ‰æ•ˆèŒƒå›´
        bitwidth: ä½å®½ (8 æˆ– 16)
        
    Returns:
        PiecewiseLUT å¯¹è±¡
    """
    import math
    
    lut = PiecewiseLUT(zp_x, exp2_inv_x, exp2_inv_y, zp_y)
    
    # è®¡ç®— scale
    scale_x = 1.0 / (1 << exp2_inv_x) if exp2_inv_x >= 0 else float(1 << (-exp2_inv_x))
    scale_y = 1.0 / (1 << exp2_inv_y) if exp2_inv_y >= 0 else float(1 << (-exp2_inv_y))
    
    # é™åˆ¶èŒƒå›´
    x_min = max(x_min, -8.0)
    x_max = min(x_max, 8.0)
    
    # ç¡®å®šé‡åŒ–èŒƒå›´
    if bitwidth == 16:
        q_min, q_max = -32768, 32767
        y_min, y_max = (0, 65535)  # sigmoid è¾“å‡ºæ— ç¬¦å·
    else:
        q_min, q_max = -128, 127
        y_min, y_max = (0, 255)
    
    # åˆ†æ®µè¾¹ç•Œ
    num_segments = PiecewiseLUT.NUM_SEGMENTS
    segment_width = (x_max - x_min) / num_segments
    
    for i in range(num_segments):
        # æ®µè¾¹ç•Œ
        seg_start = x_min + i * segment_width
        seg_end = seg_start + segment_width
        seg_mid = (seg_start + seg_end) / 2
        
        # è®¡ç®—è¯¥æ®µçš„çº¿æ€§è¿‘ä¼¼: y = b * x + c
        # sigmoid(x) åœ¨ seg_mid å¤„çš„æ–œç‡
        sigmoid_mid = 1.0 / (1.0 + math.exp(-seg_mid))
        b_fp = sigmoid_mid * (1.0 - sigmoid_mid)  # sigmoid å¯¼æ•°
        c_fp = sigmoid_mid - b_fp * seg_mid
        
        # é‡åŒ–å‚æ•°
        # q_y = q_b * (q_x - zp_x) >> n_BX + term_c
        # éœ€è¦æ»¡è¶³: (q_y - zp_y) * scale_y = b_fp * (q_x - zp_x) * scale_x + c_fp
        
        # è®¡ç®— q_b å’Œç§»ä½
        shift_bits_b = exp2_inv_y  # è¿‘ä¼¼
        q_b = int(round(b_fp * (1 << shift_bits_b) / scale_x * scale_y))
        n_BX_total = shift_bits_b + exp2_inv_x - exp2_inv_y
        
        # è®¡ç®— term_c (åŒ…å« zp_y)
        c_adjusted = c_fp + zp_y * scale_y
        term_c = int(round(c_adjusted / scale_y))
        
        # é˜ˆå€¼ï¼ˆé‡åŒ–åçš„æ®µè¾¹ç•Œï¼‰
        threshold = int(round(seg_end / scale_x)) + zp_x
        if bitwidth == 16:
            threshold = max(-32768, min(32767, threshold))
        else:
            threshold = max(-128, min(127, threshold))
        
        seg_params = SegmentParams(q_b, n_BX_total, term_c, threshold)
        lut.segments.append(seg_params)
    
    return lut


def generate_tanh_lut(exp2_inv_x: int, zp_x: int, exp2_inv_y: int, zp_y: int,
                      x_min: float = -4.0, x_max: float = 4.0,
                      bitwidth: int = 16) -> PiecewiseLUT:
    """
    ç”Ÿæˆ Tanh åˆ†æ®µçº¿æ€§ LUTï¼ˆä¸ CUDA generate_tanh_lut_int16/int8 ä¸€è‡´ï¼‰
    """
    import math
    
    lut = PiecewiseLUT(zp_x, exp2_inv_x, exp2_inv_y, zp_y)
    
    scale_x = 1.0 / (1 << exp2_inv_x) if exp2_inv_x >= 0 else float(1 << (-exp2_inv_x))
    scale_y = 1.0 / (1 << exp2_inv_y) if exp2_inv_y >= 0 else float(1 << (-exp2_inv_y))
    
    x_min = max(x_min, -4.0)
    x_max = min(x_max, 4.0)
    
    if bitwidth == 16:
        q_min, q_max = -32768, 32767
    else:
        q_min, q_max = -128, 127
    
    num_segments = PiecewiseLUT.NUM_SEGMENTS
    segment_width = (x_max - x_min) / num_segments
    
    for i in range(num_segments):
        seg_start = x_min + i * segment_width
        seg_end = seg_start + segment_width
        seg_mid = (seg_start + seg_end) / 2
        
        # tanh åŠå…¶å¯¼æ•°
        tanh_mid = math.tanh(seg_mid)
        b_fp = 1.0 - tanh_mid ** 2  # tanh å¯¼æ•°
        c_fp = tanh_mid - b_fp * seg_mid
        
        shift_bits_b = exp2_inv_y
        q_b = int(round(b_fp * (1 << shift_bits_b) / scale_x * scale_y))
        n_BX_total = shift_bits_b + exp2_inv_x - exp2_inv_y
        
        c_adjusted = c_fp + zp_y * scale_y
        term_c = int(round(c_adjusted / scale_y))
        
        threshold = int(round(seg_end / scale_x)) + zp_x
        if bitwidth == 16:
            threshold = max(-32768, min(32767, threshold))
        else:
            threshold = max(-128, min(127, threshold))
        
        seg_params = SegmentParams(q_b, n_BX_total, term_c, threshold)
        lut.segments.append(seg_params)
    
    return lut


# ============================================================
#                      GRUFunction (autograd)
# ============================================================

class GRUFunction(torch.autograd.Function):
    """
    GRU è‡ªå®šä¹‰ autograd Function
    
    è´Ÿè´£ PyTorch/Haste æ ¼å¼è½¬æ¢ã€è°ƒç”¨ C++ æ¥å£ã€ç®¡ç†åå‘ä¼ æ’­
    """

    @staticmethod
    def forward(ctx, input, weight_ih, weight_hh, bias_ih, bias_hh, h0, is_training,
                use_quantization=False, quant_params=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—
            weight_ih: [3*H, I] è¾“å…¥æƒé‡ (PyTorch r,z,n æ ¼å¼)
            weight_hh: [3*H, H] å¾ªç¯æƒé‡
            bias_ih, bias_hh: [3*H] åç½®æˆ– None
            h0: [B, H] åˆå§‹çŠ¶æ€æˆ– None
            is_training: è®­ç»ƒæ¨¡å¼æ ‡å¿—
            use_quantization: é‡åŒ–å¼€å…³
            quant_params: é‡åŒ–å‚æ•°
            
        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—
            h_n: [1, B, H] æœ€ç»ˆçŠ¶æ€
        """
        time_steps, batch_size, input_size = input.shape
        hidden_size = weight_hh.shape[1]

        # ä¿å­˜ç»´åº¦ä¿¡æ¯å’Œ None æ ‡å¿—
        ctx.time_steps, ctx.batch_size = time_steps, batch_size
        ctx.input_size, ctx.hidden_size = input_size, hidden_size
        ctx.bias_ih_is_none = (bias_ih is None)
        ctx.bias_hh_is_none = (bias_hh is None)
        ctx.h0_is_none = (h0 is None)

        device = input.device if input.is_cuda else torch.device('cuda')
        input = ensure_cuda_float32(input, device)

        # æƒé‡æ ¼å¼è½¬æ¢: PyTorch (r,z,n) -> Haste (z,r,n)ï¼Œå¹¶è½¬ç½®
        weight_ih = ensure_cuda_float32(weight_ih, device)
        weight_hh = ensure_cuda_float32(weight_hh, device)
        W = reorder_weights_pytorch_to_haste(weight_ih).t().contiguous()
        R = reorder_weights_pytorch_to_haste(weight_hh).t().contiguous()

        # åç½®å¤„ç†
        if bias_ih is not None and bias_hh is not None:
            bias_ih = ensure_cuda_float32(bias_ih, device)
            bias_hh = ensure_cuda_float32(bias_hh, device)
            bx = reorder_weights_pytorch_to_haste(bias_ih).contiguous()
            br = reorder_weights_pytorch_to_haste(bias_hh).contiguous()
        else:
            bx = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)
            br = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)

        # åˆå§‹çŠ¶æ€
        h0_tensor = ensure_cuda_float32(h0, device) if h0 is not None else torch.empty(0, device=device,
                                                                                       dtype=torch.float32)

        # é‡åŒ–å‚æ•°
        if use_quantization:
            if quant_params is None:
                raise RuntimeError("use_quantization=True æ—¶å¿…é¡»æä¾› quant_params")
        else:
            quant_params = gru_ops.GRUQuantitativeParameters()

        # è°ƒç”¨ C++ å‰å‘æ¥å£
        output_full, v = gru_ops.forward_interface(
            is_training=is_training,
            is_quant=use_quantization,
            time_steps=time_steps,
            batch_size=batch_size,
            input_size=input_size,
            hidden_size=hidden_size,
            W=W,
            R=R,
            bx=bx,
            br=br,
            x=input,
            h0=h0_tensor,
            quant_params=quant_params
        )

        # åˆ†ç¦»è¾“å‡º: output_full[0] æ˜¯åˆå§‹çŠ¶æ€ï¼Œ[1:] æ˜¯æ—¶é—´æ­¥è¾“å‡º
        output = output_full[1:]
        h_n = output_full[-1:]

        # ä¿å­˜åå‘ä¼ æ’­æ‰€éœ€çš„ä¸­é—´ç»“æœ
        ctx.save_for_backward(W, R, bx, br, input, output_full, v)

        return output, h_n

    @staticmethod
    def backward(ctx, grad_output, grad_h_n):
        """
        åå‘ä¼ æ’­
        
        Args:
            grad_output: [T, B, H] è¾“å‡ºæ¢¯åº¦
            grad_h_n: [1, B, H] æœ€ç»ˆçŠ¶æ€æ¢¯åº¦
            
        Returns:
            å¯¹åº” forward å„å‚æ•°çš„æ¢¯åº¦
        """
        W, R, bx, br, input, h, v = ctx.saved_tensors
        time_steps, batch_size = ctx.time_steps, ctx.batch_size
        input_size, hidden_size = ctx.input_size, ctx.hidden_size

        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨ CUDA ä¸Š
        device = grad_output.device
        tensors = [W, R, bx, br, input, h]
        W, R, bx, br, input, h = [t.to(device) if not t.is_cuda else t for t in tensors]
        if v is not None and not v.is_cuda:
            v = v.to(device)
        if not grad_output.is_cuda:
            grad_output = grad_output.to(device)
        if grad_h_n is not None and not grad_h_n.is_cuda:
            grad_h_n = grad_h_n.to(device)

        # æ„å»ºéšè—çŠ¶æ€æ¢¯åº¦
        # C++ æ¥å£éœ€è¦ [T+1, B, H] æ ¼å¼
        # dh_new[0] æ˜¯åˆå§‹çŠ¶æ€æ¢¯åº¦ï¼ˆä¿æŒä¸º 0ï¼‰ï¼Œdh_new[1:] æ˜¯æ—¶é—´æ­¥æ¢¯åº¦
        dh_new = torch.zeros(
            (time_steps + 1, batch_size, hidden_size),
            device=device, dtype=grad_output.dtype
        )
        dh_new[1:] = grad_output

        # ç´¯åŠ æœ€ç»ˆçŠ¶æ€æ¢¯åº¦ï¼ˆoutput[-1] å’Œ h_n[0] æŒ‡å‘åŒä¸€æ—¶é—´æ­¥ï¼‰
        if grad_h_n is not None and grad_h_n.numel() > 0:
            dh_new[-1] = dh_new[-1] + grad_h_n[0]

        # è°ƒç”¨ C++ åå‘æ¥å£ï¼ˆç»‘å®šå±‚ä¼šå¤„ç†æ ¼å¼è½¬æ¢ï¼‰
        dx, dW, dR, dbx, dbr, dh = gru_ops.haste_gru_backward(
            time_steps=time_steps, batch_size=batch_size,
            input_size=input_size, hidden_size=hidden_size,
            W=W, R=R, bx=bx, br=br, x=input,
            dh_new=dh_new, h=h, v=v
        )

        # æ¢¯åº¦æ ¼å¼è½¬æ¢: Haste (z,r,n) -> PyTorch (r,z,n)
        dW_pytorch = reorder_weights_haste_to_pytorch(dW.t()).contiguous()
        dR_pytorch = reorder_weights_haste_to_pytorch(dR.t()).contiguous()
        dbx_pytorch = reorder_weights_haste_to_pytorch(dbx).contiguous() if not ctx.bias_ih_is_none else None
        dbr_pytorch = reorder_weights_haste_to_pytorch(dbr).contiguous() if not ctx.bias_hh_is_none else None
        grad_h0 = None if ctx.h0_is_none else dh

        # è¿”å›æ¢¯åº¦ï¼ˆå¯¹åº” forward çš„ 9 ä¸ªå‚æ•°ï¼‰
        return dx, dW_pytorch, dR_pytorch, dbx_pytorch, dbr_pytorch, grad_h0, None, None, None


# ============================================================
#                      QuantGRU æ¨¡å—
# ============================================================

class QuantGRU(nn.Module):
    """
    æ”¯æŒé‡åŒ–çš„è‡ªå®šä¹‰ GRU å®ç°ï¼Œå…¼å®¹ nn.GRU æ¥å£
    
    ç‰¹æ€§:
        - å»¶è¿Ÿåˆå§‹åŒ–: CUDA handle åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–
        - å¯åºåˆ—åŒ–: æ”¯æŒ pickle/deepcopy
        - åŒå‘æ”¯æŒ: bidirectional=True æ—¶è¾“å‡ºç»´åº¦ä¸º 2*hidden_size
        - ONNX å¯¼å‡º: export_mode=True æ—¶ä½¿ç”¨çº¯ PyTorch å®ç°

    é‡åŒ–æµç¨‹:
        1. gru.load_bitwidth_config("config.json")  # å¯é€‰
        2. gru.calibrate(data1), gru.calibrate(data2), ...
        3. gru.finalize_calibration()
        4. gru.use_quantization = True
        5. output, h_n = gru(input)
    
    ONNX å¯¼å‡ºæµç¨‹:
        1. gru.export_mode = True
        2. torch.onnx.export(model, ...)
        3. gru.export_mode = False  # æ¢å¤ CUDA æ¨¡å¼
    
    é«˜çº§ï¼šæŒ‡å®šå¯¼å‡ºæ ¼å¼:
        gru.export_format = 'float'      # æµ®ç‚¹ï¼ˆé»˜è®¤ï¼Œä¸ Haste ä¸€è‡´ï¼‰
        gru.export_format = 'qdq'        # QDQ ä¼ªé‡åŒ–ï¼ˆé‡åŒ–æ¨¡å‹æ¨èï¼‰
        gru.export_format = 'fixedpoint' # çº¯å®šç‚¹ï¼ˆä¸ CUDA é‡åŒ–ä¸€è‡´ï¼‰

    Args:
        input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_size: éšè—çŠ¶æ€ç»´åº¦
        num_layers: å±‚æ•°ï¼ˆä»…æ”¯æŒ 1ï¼‰
        bias: æ˜¯å¦ä½¿ç”¨åç½®
        batch_first: True æ—¶è¾“å…¥ä¸º [B, T, I]
        dropout: æš‚ä¸æ”¯æŒ
        bidirectional: æ˜¯å¦åŒå‘
    
    Attributes:
        use_quantization: é‡åŒ–å¼€å…³ï¼ˆé»˜è®¤ Falseï¼‰
        calibration_method: æ ¡å‡†æ–¹æ³• ('minmax' æˆ– 'histogram')
        export_mode: ONNX å¯¼å‡ºæ¨¡å¼ï¼ˆé»˜è®¤ Falseï¼Œä½¿ç”¨ CUDAï¼›True æ—¶ä½¿ç”¨çº¯ PyTorchï¼‰
    """

    def __init__(
            self,
            input_size: int,
            hidden_size: int,
            num_layers: int = 1,
            bias: bool = True,
            batch_first: bool = False,
            dropout: float = 0.0,
            bidirectional: bool = False,
            use_quantization: bool = False,
    ):
        super(QuantGRU, self).__init__()

        if num_layers != 1:
            raise NotImplementedError("ä»…æ”¯æŒ num_layers=1")
        if dropout > 0:
            raise NotImplementedError("æš‚ä¸æ”¯æŒ dropout")

        # åŸºæœ¬é…ç½®
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.use_quantization = use_quantization
        self.num_directions = 2 if bidirectional else 1

        # ONNX å¯¼å‡ºå¼€å…³ï¼šTrue æ—¶ä½¿ç”¨çº¯ PyTorch å®ç°ï¼Œå¯è¢« ONNX è¿½è¸ª
        self.export_mode = False
        # å¯¼å‡ºæ ¼å¼ï¼ˆé«˜çº§é€‰é¡¹ï¼Œä»…åœ¨ export_mode=True æ—¶æœ‰æ•ˆï¼‰
        # 'float': æµ®ç‚¹ï¼ˆé»˜è®¤ï¼Œä¸ Haste GRU è¡Œä¸ºä¸€è‡´ï¼‰
        # 'qdq': QDQ ä¼ªé‡åŒ–ï¼ˆæ¨èç”¨äºé‡åŒ–æ¨¡å‹ï¼‰
        # 'fixedpoint': çº¯å®šç‚¹ï¼ˆä¸ CUDA é‡åŒ–ä¸€è‡´ï¼Œç”¨äºéªŒè¯ï¼‰
        self._export_format = 'float'

        # æƒé‡å‚æ•°ï¼ˆå‘½åä¸ nn.GRU ä¸€è‡´ï¼‰
        self.weight_ih_l0 = nn.Parameter(torch.empty(3 * hidden_size, input_size))
        self.weight_hh_l0 = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
        if bias:
            self.bias_ih_l0 = nn.Parameter(torch.empty(3 * hidden_size))
            self.bias_hh_l0 = nn.Parameter(torch.empty(3 * hidden_size))
        else:
            self.register_parameter('bias_ih_l0', None)
            self.register_parameter('bias_hh_l0', None)

        # åå‘æƒé‡ï¼ˆåŒå‘æ—¶ï¼‰
        if bidirectional:
            self.weight_ih_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size, input_size))
            self.weight_hh_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
            if bias:
                self.bias_ih_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size))
                self.bias_hh_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size))
            else:
                self.register_parameter('bias_ih_l0_reverse', None)
                self.register_parameter('bias_hh_l0_reverse', None)

        self.reset_parameters()

        # é‡åŒ–çŠ¶æ€ï¼ˆå»¶è¿Ÿåˆ›å»ºï¼‰
        self.quant_ranges = None  # calibrate() æ—¶åˆ›å»º
        self.quant_params = None  # finalize_calibration() æ—¶åˆ›å»º
        if bidirectional:
            self.quant_ranges_reverse = None
            self.quant_params_reverse = None

        self._calibration_dirty = False  # æ ¡å‡†æ•°æ®æ›´æ–°æ ‡å¿—
        self._bitwidth_config_dict = None  # ä½å®½é…ç½®ï¼ˆPython å­—å…¸ï¼Œå¯åºåˆ—åŒ–ï¼‰
        self._cublas_initialized = False  # CUDA å»¶è¿Ÿåˆå§‹åŒ–æ ‡å¿—

        # æ ¡å‡†æ–¹æ³•: 'minmax'ï¼ˆå¿«é€Ÿï¼‰æˆ– 'histogram'ï¼ˆAIMET é£æ ¼ï¼Œé«˜ç²¾åº¦ï¼‰
        self.calibration_method = 'histogram'

        # ç›´æ–¹å›¾æ”¶é›†å™¨ï¼ˆhistogram æ–¹æ³•ä½¿ç”¨ï¼‰
        self.hist_collectors = None
        if bidirectional:
            self.hist_collectors_reverse = None

    def reset_parameters(self):
        """æƒé‡åˆå§‹åŒ–ï¼ˆä¸ nn.GRU ç›¸åŒçš„å‡åŒ€åˆ†å¸ƒï¼‰"""
        stdv = 1.0 / (self.hidden_size ** 0.5)
        for param in self.parameters():
            nn.init.uniform_(param, -stdv, stdv)

    # -------------------- å†…éƒ¨æ–¹æ³• --------------------

    def _ensure_cublas_initialized(self):
        """å»¶è¿Ÿåˆå§‹åŒ– cublas handle"""
        if not self._cublas_initialized:
            gru_ops.init_gru_cublas()
            self._cublas_initialized = True

    def _load_bitwidth_config_to_dict(self, config_file: str):
        """ä» JSON æ–‡ä»¶åŠ è½½é…ç½®åˆ°å†…éƒ¨å­—å…¸"""
        if self._bitwidth_config_dict is None:
            self._bitwidth_config_dict = {}

        with open(config_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # è¯»å– GRU_config èŠ‚ç‚¹ä¸‹çš„é…ç½®
        gru_config = data.get('GRU_config', {})

        # è¯»å–å…¨å±€é…ç½®
        default_config = gru_config.get('default_config', {})
        if 'disable_quantization' in default_config:
            # disable_quantization=true è¡¨ç¤ºç¦ç”¨é‡åŒ–ï¼Œæ‰€ä»¥ use_quantization å–å
            self.use_quantization = not default_config['disable_quantization']

        op_config = gru_config.get('operator_config', {})

        # å­—æ®µæ˜ å°„: JSON key -> (ä½å®½å±æ€§å, å¯¹ç§°é‡åŒ–å±æ€§å)
        field_map = {
            "input.x": ("x_", "x_symmetric_"),
            "input.h": ("h_", "h_symmetric_"),
            "weight.W": ("W_", "W_symmetric_"),
            "weight.R": ("R_", "R_symmetric_"),
            "weight.bx": ("bx_", "bx_symmetric_"),
            "weight.br": ("br_", "br_symmetric_"),
            "matmul.Wx": ("Wx_", "Wx_symmetric_"),
            "matmul.Rh": ("Rh_", "Rh_symmetric_"),
            "gate.z_pre": ("z_pre_", "z_pre_symmetric_"),
            "gate.z_out": ("z_out_", "z_out_symmetric_"),
            "gate.r_pre": ("r_pre_", "r_pre_symmetric_"),
            "gate.r_out": ("r_out_", "r_out_symmetric_"),
            "gate.g_pre": ("g_pre_", "g_pre_symmetric_"),
            "gate.g_out": ("g_out_", "g_out_symmetric_"),
            "op.Rh_add_br": ("Rh_add_br_", "Rh_add_br_symmetric_"),
            "op.rRh": ("rRh_", "rRh_symmetric_"),
            "op.old_contrib": ("old_contrib_", "old_contrib_symmetric_"),
            "op.new_contrib": ("new_contrib_", "new_contrib_symmetric_"),
        }

        for json_key, (bw_attr, sym_attr) in field_map.items():
            if json_key in op_config:
                op_cfg = op_config[json_key]
                self._bitwidth_config_dict[bw_attr] = op_cfg.get('bitwidth', 8)
                self._bitwidth_config_dict[sym_attr] = op_cfg.get('is_symmetric', True)

    def _get_cpp_bitwidth_config(self) -> gru_ops.OperatorQuantConfig:
        """ä» Python å­—å…¸åˆ›å»º C++ OperatorQuantConfig å¯¹è±¡"""
        config = gru_ops.OperatorQuantConfig()
        if self._bitwidth_config_dict is not None:
            for attr, value in self._bitwidth_config_dict.items():
                setattr(config, attr, value)
        return config

    def _convert_weights_to_haste_format(self, device: torch.device, reverse: bool = False):
        """
        å°†æƒé‡è½¬æ¢ä¸º Haste æ ¼å¼ (z,r,n)
        
        Returns:
            W, R, bx, br: Haste æ ¼å¼çš„æƒé‡å’Œåç½®
        """
        if reverse and self.bidirectional:
            weight_ih = ensure_cuda_float32(self.weight_ih_l0_reverse, device)
            weight_hh = ensure_cuda_float32(self.weight_hh_l0_reverse, device)
        else:
            weight_ih = ensure_cuda_float32(self.weight_ih_l0, device)
            weight_hh = ensure_cuda_float32(self.weight_hh_l0, device)

        W = reorder_weights_pytorch_to_haste(weight_ih).t().contiguous()
        R = reorder_weights_pytorch_to_haste(weight_hh).t().contiguous()

        if self.bias:
            if reverse and self.bidirectional:
                bias_ih = ensure_cuda_float32(self.bias_ih_l0_reverse, device)
                bias_hh = ensure_cuda_float32(self.bias_hh_l0_reverse, device)
            else:
                bias_ih = ensure_cuda_float32(self.bias_ih_l0, device)
                bias_hh = ensure_cuda_float32(self.bias_hh_l0, device)
            bx = reorder_weights_pytorch_to_haste(bias_ih).contiguous()
            br = reorder_weights_pytorch_to_haste(bias_hh).contiguous()
        else:
            hidden_size = self.hidden_size
            bx = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)
            br = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)

        return W, R, bx, br

    def _accumulate_calibration_ranges(self, calibration_data: torch.Tensor):
        """ç´¯ç§¯æ ¡å‡†èŒƒå›´"""
        self._ensure_cublas_initialized()

        device = calibration_data.device if calibration_data.is_cuda else torch.device('cuda')
        if not calibration_data.is_cuda:
            calibration_data = calibration_data.to(device)

        # ç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Š
        if not next(self.parameters()).is_cuda:
            for param in self.parameters():
                param.data = param.data.to(device)
            for buffer in self.buffers():
                buffer.data = buffer.data.to(device)

        if self.batch_first:
            calibration_data = calibration_data.transpose(0, 1).contiguous()

        time_steps, batch_size, input_size = calibration_data.shape
        hidden_size = self.hidden_size

        # å‰å‘æ ¡å‡†
        W, R, bx, br = self._convert_weights_to_haste_format(device, reverse=False)
        if self.calibration_method == 'histogram':
            if self.hist_collectors is None:
                self.hist_collectors = gru_ops.GRUHistogramCollectors(hidden_size, num_bins=2048)
            gru_ops.calibrate_gru_histograms(
                time_steps=time_steps, batch_size=batch_size, input_size=input_size, hidden_size=hidden_size,
                W=W, R=R, bx=bx, br=br, x=calibration_data, hist_collectors=self.hist_collectors)
        else:
            if self.quant_ranges is None:
                self.quant_ranges = gru_ops.GRUQuantizationRanges(hidden_size)
            gru_ops.calibrate_gru_ranges(
                time_steps=time_steps, batch_size=batch_size, input_size=input_size, hidden_size=hidden_size,
                W=W, R=R, bx=bx, br=br, x=calibration_data, quant_ranges=self.quant_ranges)

        # åå‘æ ¡å‡†ï¼ˆåŒå‘æ—¶ï¼‰
        if self.bidirectional:
            W_rev, R_rev, bx_rev, br_rev = self._convert_weights_to_haste_format(device, reverse=True)
            calibration_data_reversed = calibration_data.flip(0).contiguous()

            if self.calibration_method == 'histogram':
                if self.hist_collectors_reverse is None:
                    self.hist_collectors_reverse = gru_ops.GRUHistogramCollectors(hidden_size, num_bins=2048)
                gru_ops.calibrate_gru_histograms(
                    time_steps=time_steps, batch_size=batch_size, input_size=input_size, hidden_size=hidden_size,
                    W=W_rev, R=R_rev, bx=bx_rev, br=br_rev, x=calibration_data_reversed,
                    hist_collectors=self.hist_collectors_reverse)
            else:
                if self.quant_ranges_reverse is None:
                    self.quant_ranges_reverse = gru_ops.GRUQuantizationRanges(hidden_size)
                gru_ops.calibrate_gru_ranges(
                    time_steps=time_steps, batch_size=batch_size, input_size=input_size, hidden_size=hidden_size,
                    W=W_rev, R=R_rev, bx=bx_rev, br=br_rev, x=calibration_data_reversed,
                    quant_ranges=self.quant_ranges_reverse)

        # ç¡®ä¿æƒé‡è¿ç»­
        self.weight_ih_l0.data = self.weight_ih_l0.data.contiguous()
        self.weight_hh_l0.data = self.weight_hh_l0.data.contiguous()
        if self.bias:
            self.bias_ih_l0.data = self.bias_ih_l0.data.contiguous()
            self.bias_hh_l0.data = self.bias_hh_l0.data.contiguous()
        if self.bidirectional:
            self.weight_ih_l0_reverse.data = self.weight_ih_l0_reverse.data.contiguous()
            self.weight_hh_l0_reverse.data = self.weight_hh_l0_reverse.data.contiguous()
            if self.bias:
                self.bias_ih_l0_reverse.data = self.bias_ih_l0_reverse.data.contiguous()
                self.bias_hh_l0_reverse.data = self.bias_hh_l0_reverse.data.contiguous()

    # -------------------- å…¬å¼€æ¥å£ --------------------

    def load_bitwidth_config(self, config_file: str, verbose: bool = False):
        """ä» JSON æ–‡ä»¶åŠ è½½ä½å®½é…ç½®"""
        self._load_bitwidth_config_to_dict(config_file)
        if verbose:
            cpp_config = self._get_cpp_bitwidth_config()
            apply_bitwidth_config(cpp_config, config_file, verbose=True)
            print(f"  [å…¨å±€]  use_quantization: {self.use_quantization}")

    def set_all_bitwidth(self, bitwidth: int = 8, is_symmetric: bool = True, verbose: bool = False):
        """
        è®¾ç½®æ‰€æœ‰ç®—å­ç»Ÿä¸€çš„ä½å®½å’Œå¯¹ç§°é‡åŒ–é…ç½®
        
        Args:
            bitwidth: ä½å®½ (8/16/32)
            is_symmetric: æ˜¯å¦å¯¹ç§°é‡åŒ–
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
        """
        if bitwidth not in (8, 16, 32):
            raise ValueError(f"bitwidth must be 8, 16 or 32, got {bitwidth}")

        # åˆå§‹åŒ–é…ç½®å­—å…¸
        if self._bitwidth_config_dict is None:
            self._bitwidth_config_dict = {}

        # ä½å®½å±æ€§åˆ—è¡¨
        bitwidth_attrs = [
            'x_', 'h_', 'W_', 'R_', 'bx_', 'br_', 'Wx_', 'Rh_',
            'z_pre_', 'z_out_', 'r_pre_', 'r_out_', 'g_pre_', 'g_out_',
            'Rh_add_br_', 'rRh_', 'old_contrib_', 'new_contrib_'
        ]

        # å¯¹ç§°é‡åŒ–å±æ€§åˆ—è¡¨
        symmetric_attrs = [
            'x_symmetric_', 'h_symmetric_', 'W_symmetric_', 'R_symmetric_',
            'bx_symmetric_', 'br_symmetric_', 'Wx_symmetric_', 'Rh_symmetric_',
            'z_pre_symmetric_', 'z_out_symmetric_', 'r_pre_symmetric_', 'r_out_symmetric_',
            'g_pre_symmetric_', 'g_out_symmetric_', 'Rh_add_br_symmetric_', 'rRh_symmetric_',
            'old_contrib_symmetric_', 'new_contrib_symmetric_'
        ]

        # è®¾ç½®æ‰€æœ‰ä½å®½
        for attr in bitwidth_attrs:
            self._bitwidth_config_dict[attr] = bitwidth

        # è®¾ç½®æ‰€æœ‰å¯¹ç§°é‡åŒ–é…ç½®
        for attr in symmetric_attrs:
            self._bitwidth_config_dict[attr] = is_symmetric

        if verbose:
            sym_str = "å¯¹ç§°" if is_symmetric else "éå¯¹ç§°"
            print(f"\n[QuantGRU] è®¾ç½®æ‰€æœ‰ç®—å­: {bitwidth}bit {sym_str}é‡åŒ–")

    def is_calibrated(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ ¡å‡†"""
        if self.bidirectional:
            return self.quant_params is not None and self.quant_params_reverse is not None
        return self.quant_params is not None

    def calibrate(self, calibration_data: torch.Tensor):
        """
        ç´¯ç§¯æ ¡å‡†æ•°æ®
        
        Args:
            calibration_data: [T, B, I] æˆ– [B, T, I] (batch_first) çš„æ•°æ®
        
        Note:
            æ”¯æŒå¢é‡æ ¡å‡†ï¼Œå®Œæˆåéœ€è°ƒç”¨ finalize_calibration()
        """
        self._accumulate_calibration_ranges(calibration_data)
        self._calibration_dirty = True

    def finalize_calibration(self, verbose: bool = False):
        """
        å®Œæˆæ ¡å‡†ï¼Œè®¡ç®—é‡åŒ–å‚æ•°å¹¶åˆå§‹åŒ– LUT
        
        Args:
            verbose: æ˜¯å¦æ‰“å°æ ¡å‡†ä¿¡æ¯
            
        Raises:
            RuntimeError: æœªè°ƒç”¨è¿‡ calibrate()
        """
        use_histogram = (self.calibration_method == 'histogram')

        # æ£€æŸ¥æ ¡å‡†æ•°æ®
        if use_histogram:
            if self.hist_collectors is None or not self.hist_collectors.is_valid():
                raise RuntimeError("æœªæ”¶é›†ç›´æ–¹å›¾æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ calibrate()")
        else:
            if self.quant_ranges is None:
                raise RuntimeError("æœªæ”¶é›†æ ¡å‡†æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ calibrate()")

        cpp_config = self._get_cpp_bitwidth_config()

        if verbose:
            method_name = {'minmax': 'MINMAX', 'histogram': 'HISTOGRAM'}.get(
                self.calibration_method, self.calibration_method.upper())
            print(f"\n[QuantGRU] æ ¡å‡†æ–¹æ³•: {method_name}")

        # å‰å‘æ–¹å‘
        if use_histogram:
            self.quant_params = gru_ops.calculate_gru_quantitative_parameters_from_histograms(
                hist_collectors=self.hist_collectors, bitwidth_config=cpp_config, verbose=verbose)
        else:
            self.quant_params = gru_ops.calculate_gru_quantitative_parameters(
                quant_ranges=self.quant_ranges, bitwidth_config=cpp_config)
        gru_ops.initialize_quantization_lut(quant_params=self.quant_params)

        # åå‘æ–¹å‘ï¼ˆåŒå‘æ—¶ï¼‰
        if self.bidirectional:
            if use_histogram:
                if self.hist_collectors_reverse is None or not self.hist_collectors_reverse.is_valid():
                    raise RuntimeError("åŒå‘ GRU åå‘ç›´æ–¹å›¾æ•°æ®å¼‚å¸¸")
                self.quant_params_reverse = gru_ops.calculate_gru_quantitative_parameters_from_histograms(
                    hist_collectors=self.hist_collectors_reverse, bitwidth_config=cpp_config, verbose=verbose)
            else:
                if self.quant_ranges_reverse is None:
                    raise RuntimeError("åŒå‘ GRU åå‘æ ¡å‡†æ•°æ®å¼‚å¸¸")
                self.quant_params_reverse = gru_ops.calculate_gru_quantitative_parameters(
                    quant_ranges=self.quant_ranges_reverse, bitwidth_config=cpp_config)
            gru_ops.initialize_quantization_lut(quant_params=self.quant_params_reverse)

        self._calibration_dirty = False

    def reset_calibration(self):
        """é‡ç½®æ ¡å‡†çŠ¶æ€ï¼Œæ¸…é™¤æ‰€æœ‰ç´¯ç§¯çš„èŒƒå›´å’Œå‚æ•°"""
        self.quant_ranges = None
        self.quant_params = None
        self.hist_collectors = None
        self._calibration_dirty = False
        if self.bidirectional:
            self.quant_ranges_reverse = None
            self.quant_params_reverse = None
            self.hist_collectors_reverse = None

    # -------------------- ONNX å¯¼å‡ºæ¨¡å¼ï¼šçº¯ PyTorch å®ç° --------------------

    def _get_quant_param(self, param_name: str, quant_params) -> Tuple[int, int]:
        """è·å–é‡åŒ–å‚æ•° (exp2_inv, zero_point)"""
        exp2_inv = getattr(quant_params, f'exp2_inv_{param_name}_', 0)
        zp = getattr(quant_params, f'zp_{param_name}_', 0)
        return exp2_inv, zp

    def _get_bitwidth(self, op_name: str) -> int:
        """è·å–æŒ‡å®šæ“ä½œçš„ä½å®½"""
        if self._bitwidth_config_dict is not None:
            return self._bitwidth_config_dict.get(f'{op_name}_', 8)
        return 8

    def _get_symmetric(self, op_name: str) -> bool:
        """è·å–æŒ‡å®šæ“ä½œæ˜¯å¦å¯¹ç§°é‡åŒ–"""
        if self._bitwidth_config_dict is not None:
            return self._bitwidth_config_dict.get(f'{op_name}_symmetric_', True)
        return True

    @property
    def export_format(self) -> str:
        """
        è·å–å¯¼å‡ºæ ¼å¼ï¼ˆé«˜çº§é€‰é¡¹ï¼Œä»…åœ¨ export_mode=True æ—¶æœ‰æ•ˆï¼‰
        
        Returns:
            'float': æµ®ç‚¹æ ¼å¼ï¼ˆé»˜è®¤ï¼Œä¸ Haste GRU è¡Œä¸ºä¸€è‡´ï¼‰
            'qdq': QDQ ä¼ªé‡åŒ–æ ¼å¼ï¼ˆæ¨èç”¨äºé‡åŒ–æ¨¡å‹ ONNX å¯¼å‡ºï¼‰
            'fixedpoint': çº¯å®šç‚¹æ ¼å¼ï¼ˆä¸ CUDA é‡åŒ–å®Œå…¨ä¸€è‡´ï¼Œç”¨äºç²¾åº¦éªŒè¯ï¼‰
        """
        return self._export_format
    
    @export_format.setter
    def export_format(self, mode: str):
        """
        è®¾ç½®å¯¼å‡ºæ ¼å¼ï¼ˆé«˜çº§ç”¨æ³•ï¼Œå¤§å¤šæ•°ç”¨æˆ·ä¸éœ€è¦ä¿®æ”¹ï¼‰
        
        Args:
            mode: 'qdq' | 'fixedpoint' | 'float'
        """
        valid_modes = ('qdq', 'fixedpoint', 'float')
        if mode not in valid_modes:
            raise ValueError(f"Invalid export_format: '{mode}'. Use one of {valid_modes}")
        self._export_format = mode

    def _forward_python_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
            quant_params
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        çº¯ PyTorch å®ç°çš„å•å‘ GRU å‰å‘ä¼ æ’­ï¼ˆå¯è¢« ONNX è¿½è¸ªï¼‰

        GRU å…¬å¼ï¼ˆHaste æ ¼å¼ï¼Œé—¨é¡ºåºä¸º z, r, gï¼‰ï¼š
            z = sigmoid(W_z @ x + R_z @ h + bx_z + br_z)  # update gate
            r = sigmoid(W_r @ x + R_r @ h + bx_r + br_r)  # reset gate
            g = tanh(W_g @ x + r * (R_g @ h + br_g) + bx_g)  # candidate gate
            h' = z * h + (1 - z) * g

        é‡åŒ–æ¨¡å¼ä¸‹æ ¹æ® ONNX å¯¼å‡ºæ¨¡å¼é€‰æ‹©å®ç°ï¼š
            - 'qdq': QDQ æ ¼å¼ï¼Œä½¿ç”¨æ ‡å‡†ç®—å­ + ä¼ªé‡åŒ–
            - 'fixedpoint': çº¯å®šç‚¹ï¼Œä¸ CUDA å®Œå…¨ä¸€è‡´
            - 'float': æ ‡å‡†æµ®ç‚¹è®¡ç®—ï¼ˆHaste æ ¼å¼ï¼‰

        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—
            h0: [B, H] åˆå§‹éšè—çŠ¶æ€ æˆ– None
            weight_ih: [3*H, I] è¾“å…¥æƒé‡ (PyTorch r,z,n æ ¼å¼ï¼Œå†…éƒ¨è‡ªåŠ¨è½¬æ¢)
            weight_hh: [3*H, H] å¾ªç¯æƒé‡ (PyTorch r,z,n æ ¼å¼ï¼Œå†…éƒ¨è‡ªåŠ¨è½¬æ¢)
            bias_ih: [3*H] è¾“å…¥åç½® æˆ– None (PyTorch æ ¼å¼ï¼Œå†…éƒ¨è‡ªåŠ¨è½¬æ¢)
            bias_hh: [3*H] å¾ªç¯åç½® æˆ– None (PyTorch æ ¼å¼ï¼Œå†…éƒ¨è‡ªåŠ¨è½¬æ¢)
            quant_params: é‡åŒ–å‚æ•°ï¼ˆæ¥è‡ª finalize_calibrationï¼‰

        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—
            h_n: [1, B, H] æœ€ç»ˆéšè—çŠ¶æ€
        """
        # æ ¹æ® export_format é€‰æ‹©å®ç°
        if self._export_format == 'float':
            # æµ®ç‚¹æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æµ®ç‚¹å®ç°
            return self._forward_python_float_single_direction(
                input, h0, weight_ih, weight_hh, bias_ih, bias_hh
            )
        
        # qdq/fixedpoint éœ€è¦é‡åŒ–å‚æ•°
        if quant_params is None:
            raise RuntimeError(
                f"export_format='{self._export_format}' éœ€è¦é‡åŒ–å‚æ•°ï¼Œ"
                f"è¯·å…ˆè°ƒç”¨ calibrate() å’Œ finalize_calibration()"
            )
        
        if self._export_format == 'qdq':
            return self._forward_onnx_qdq_single_direction(
                input, h0, weight_ih, weight_hh, bias_ih, bias_hh, quant_params
            )
        else:  # 'fixedpoint'
            return self._forward_python_fixedpoint_single_direction(
                input, h0, weight_ih, weight_hh, bias_ih, bias_hh, quant_params
            )

    def _forward_python_float_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        æµ®ç‚¹å®ç°çš„å•å‘ GRU å‰å‘ä¼ æ’­ï¼ˆHaste æ ¼å¼ï¼‰
        
        ä¸ HasteGRU CUDA æµ®ç‚¹æ¨ç†è¡Œä¸ºä¸€è‡´
        é—¨æ§é¡ºåºï¼šHaste æ ¼å¼ (z, r, g)
        
        å…¬å¼ï¼ˆä¸ gru_forward_gpu.cu ä¸€è‡´ï¼‰ï¼š
            z = sigmoid(Wx_z + Rh_z + bx_z + br_z)
            r = sigmoid(Wx_r + Rh_r + bx_r + br_r)
            g = tanh(Wx_g + r * (Rh_g + br_g) + bx_g)
            h_new = z * h_old + (1 - z) * g
        
        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—
            h0: [B, H] åˆå§‹éšè—çŠ¶æ€ æˆ– None
            weight_ih: [3*H, I] è¾“å…¥æƒé‡ (PyTorch r,z,n æ ¼å¼ï¼Œå†…éƒ¨è½¬æ¢)
            weight_hh: [3*H, H] å¾ªç¯æƒé‡ (PyTorch r,z,n æ ¼å¼ï¼Œå†…éƒ¨è½¬æ¢)
            bias_ih: [3*H] è¾“å…¥åç½® æˆ– None (PyTorch æ ¼å¼ï¼Œå†…éƒ¨è½¬æ¢)
            bias_hh: [3*H] å¾ªç¯åç½® æˆ– None (PyTorch æ ¼å¼ï¼Œå†…éƒ¨è½¬æ¢)
            
        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—
            h_n: [1, B, H] æœ€ç»ˆéšè—çŠ¶æ€
        """
        T, B, I = input.shape
        H = self.hidden_size
        device = input.device
        dtype = input.dtype

        # åˆå§‹åŒ–éšè—çŠ¶æ€
        if h0 is None:
            h = torch.zeros(B, H, device=device, dtype=dtype)
        else:
            h = h0

        # æƒé‡æ ¼å¼è½¬æ¢ï¼šPyTorch (r,z,n) -> Haste (z,r,g)
        W = reorder_weights_pytorch_to_haste(weight_ih)  # [3*H, I]
        R = reorder_weights_pytorch_to_haste(weight_hh)  # [3*H, H]

        # å¤„ç†åç½®å¹¶è½¬æ¢æ ¼å¼
        if bias_ih is None:
            bx = torch.zeros(3 * H, device=device, dtype=dtype)
        else:
            bx = reorder_weights_pytorch_to_haste(bias_ih)
        if bias_hh is None:
            br = torch.zeros(3 * H, device=device, dtype=dtype)
        else:
            br = reorder_weights_pytorch_to_haste(bias_hh)

        # ========== å¾ªç¯å¤–ä¸€æ¬¡æ€§è®¡ç®— Wx GEMMï¼ˆä¸ CUDA ä¸€è‡´ï¼‰==========
        # input: [T, B, I] -> x_flat: [T*B, I]
        # W: [3*H, I] -> W.t(): [I, 3*H]
        # Wx_all: [T*B, 3*H] -> reshape: [T, B, 3*H]
        x_flat = input.reshape(T * B, I)
        Wx_all = torch.mm(x_flat, W.t())  # [T*B, 3*H]
        Wx_all = Wx_all.reshape(T, B, 3 * H)  # [T, B, 3*H]

        # é¢„åˆ†å‰²åç½®ï¼ˆå¾ªç¯å¤–å®Œæˆï¼‰
        bx_z, bx_r, bx_g = bx.chunk(3)
        br_z, br_r, br_g = br.chunk(3)

        outputs = []

        for t in range(T):
            # è·å–å½“å‰æ—¶é—´æ­¥çš„ Wxï¼ˆå·²åœ¨å¾ªç¯å¤–è®¡ç®—å¥½ï¼‰
            Wx = Wx_all[t]  # [B, 3*H]
            
            # Rh = h @ R.T, shape [B, 3H]ï¼ˆä¾èµ–ä¸Šä¸€æ­¥çš„ hï¼Œå¿…é¡»åœ¨å¾ªç¯å†…ï¼‰
            Rh = torch.mm(h, R.t())

            # åˆ†å‰²é—¨æ§ï¼ˆHaste æ ¼å¼ï¼šz, r, gï¼‰
            Wx_z, Wx_r, Wx_g = Wx.chunk(3, dim=1)
            Rh_z, Rh_r, Rh_g = Rh.chunk(3, dim=1)

            # Update gate (z)
            z = torch.sigmoid(Wx_z + Rh_z + bx_z + br_z)

            # Reset gate (r)
            r = torch.sigmoid(Wx_r + Rh_r + bx_r + br_r)

            # Candidate gate (g): r åªä¹˜ä»¥ (Rh_g + br_g)
            Rh_add_br_g = Rh_g + br_g
            g = torch.tanh(Wx_g + r * Rh_add_br_g + bx_g)

            # æ–°éšè—çŠ¶æ€: h_new = z * h_old + (1 - z) * g
            h = z * h + (1 - z) * g

            outputs.append(h)

        # å †å è¾“å‡º: [T, B, H]
        output = torch.stack(outputs, dim=0)
        h_n = h.unsqueeze(0)  # [1, B, H]

        return output, h_n

    # -------------------- çº¯å®šç‚¹å®ç°ï¼ˆä¸ CUDA å®Œå…¨ä¸€è‡´ï¼‰--------------------

    def _build_rescale_params(self, quant_params, H: int, device: torch.device):
        """
        ä» quant_params æ„å»º rescale å‚æ•°ï¼ˆä¸ CUDA ForwardPassQuant::set_parms ä¸€è‡´ï¼‰
        
        Returns:
            dict: åŒ…å«æ‰€æœ‰ rescale å‚æ•°çš„å­—å…¸
        """
        params = {}
        
        # åŸºç¡€å‚æ•°
        params['zp_x'] = quant_params.zp_x_
        params['zp_h'] = quant_params.zp_h_
        params['zp_Wx'] = quant_params.zp_Wx_
        params['zp_Rh'] = quant_params.zp_Rh_
        params['exp2_inv_x'] = quant_params.exp2_inv_x_
        params['exp2_inv_h'] = quant_params.exp2_inv_h_
        params['exp2_inv_Wx'] = quant_params.exp2_inv_Wx_
        params['exp2_inv_Rh'] = quant_params.exp2_inv_Rh_
        
        # GEMM rescale å‚æ•° (per-channel)
        # n_W_mul_x_div_Wx[c] = (exp2_inv_W[c] + exp2_inv_x) - exp2_inv_Wx
        # n_R_mul_h_div_Rh[c] = (exp2_inv_R[c] + exp2_inv_h) - exp2_inv_Rh
        exp2_inv_W = list(quant_params.exp2_inv_W_)
        exp2_inv_R = list(quant_params.exp2_inv_R_)
        params['n_W_mul_x_div_Wx'] = [(exp2_inv_W[c] + quant_params.exp2_inv_x_) - quant_params.exp2_inv_Wx_ 
                                      for c in range(3 * H)]
        params['n_R_mul_h_div_Rh'] = [(exp2_inv_R[c] + quant_params.exp2_inv_h_) - quant_params.exp2_inv_Rh_ 
                                      for c in range(3 * H)]
        
        # z é—¨
        params['zp_z_pre'] = quant_params.zp_z_pre_
        params['zp_z_out'] = quant_params.zp_z_out_
        params['exp2_inv_Wx_div_z_pre'] = quant_params.exp2_inv_Wx_ - quant_params.exp2_inv_z_pre_
        params['exp2_inv_Rh_div_z_pre'] = quant_params.exp2_inv_Rh_ - quant_params.exp2_inv_z_pre_
        
        # per-channel bias rescale for z gate
        exp2_inv_bx = list(quant_params.exp2_inv_bx_)
        exp2_inv_br = list(quant_params.exp2_inv_br_)
        params['n_bx_div_z'] = [exp2_inv_bx[i] - quant_params.exp2_inv_z_pre_ for i in range(H)]
        params['n_br_div_z'] = [exp2_inv_br[i] - quant_params.exp2_inv_z_pre_ for i in range(H)]
        
        # r é—¨
        params['zp_r_pre'] = quant_params.zp_r_pre_
        params['zp_r_out'] = quant_params.zp_r_out_
        params['exp2_inv_Wx_div_r_pre'] = quant_params.exp2_inv_Wx_ - quant_params.exp2_inv_r_pre_
        params['exp2_inv_Rh_div_r_pre'] = quant_params.exp2_inv_Rh_ - quant_params.exp2_inv_r_pre_
        params['n_bx_div_r'] = [exp2_inv_bx[H + i] - quant_params.exp2_inv_r_pre_ for i in range(H)]
        params['n_br_div_r'] = [exp2_inv_br[H + i] - quant_params.exp2_inv_r_pre_ for i in range(H)]
        
        # g é—¨ (new gate)
        params['zp_g_pre'] = quant_params.zp_g_pre_
        params['zp_g_out'] = quant_params.zp_g_out_
        params['n_Rh_div_Rh_add_br'] = quant_params.exp2_inv_Rh_ - quant_params.exp2_inv_Rh_add_br_
        params['n_br_div_Rh_add_br'] = [exp2_inv_br[2*H + i] - quant_params.exp2_inv_Rh_add_br_ for i in range(H)]
        params['zp_Rh_add_br'] = quant_params.zp_Rh_add_br_
        params['n_r_mul_Rh_add_br_div_rRh'] = (quant_params.exp2_inv_r_out_ + quant_params.exp2_inv_Rh_add_br_) - quant_params.exp2_inv_rRh_
        params['zp_rRh'] = quant_params.zp_rRh_
        params['n_Wx_div_g_pre'] = quant_params.exp2_inv_Wx_ - quant_params.exp2_inv_g_pre_
        params['n_rRh_div_g_pre'] = quant_params.exp2_inv_rRh_ - quant_params.exp2_inv_g_pre_
        params['n_bx_div_g_pre'] = [exp2_inv_bx[2*H + i] - quant_params.exp2_inv_g_pre_ for i in range(H)]
        
        # h_new
        # one_in_z_scale = round(1.0 * 2^exp2_inv_z_out) + zp_z_out
        exp2_z_out = quant_params.exp2_inv_z_out_
        if exp2_z_out >= 0:
            one_scaled = 1 << exp2_z_out
        else:
            one_scaled = 1  # è¿‘ä¼¼å¤„ç†
        params['one_in_z_scale'] = one_scaled + quant_params.zp_z_out_
        
        params['zp_new_contrib'] = quant_params.zp_new_contrib_
        params['n_z_out_mul_g_div_new_contrib'] = (quant_params.exp2_inv_z_out_ + quant_params.exp2_inv_g_out_) - quant_params.exp2_inv_new_contrib_
        params['zp_old_contrib'] = quant_params.zp_old_contrib_
        params['n_z_mul_h_div_old_contrib'] = (quant_params.exp2_inv_z_out_ + quant_params.exp2_inv_h_) - quant_params.exp2_inv_old_contrib_
        params['n_new_contrib_div_h'] = quant_params.exp2_inv_new_contrib_ - quant_params.exp2_inv_h_
        params['n_old_contrib_div_h'] = quant_params.exp2_inv_old_contrib_ - quant_params.exp2_inv_h_
        
        # LUT å‚æ•°
        params['exp2_inv_z_pre'] = quant_params.exp2_inv_z_pre_
        params['exp2_inv_z_out'] = quant_params.exp2_inv_z_out_
        params['exp2_inv_r_pre'] = quant_params.exp2_inv_r_pre_
        params['exp2_inv_r_out'] = quant_params.exp2_inv_r_out_
        params['exp2_inv_g_pre'] = quant_params.exp2_inv_g_pre_
        params['exp2_inv_g_out'] = quant_params.exp2_inv_g_out_
        params['exp2_inv_h'] = quant_params.exp2_inv_h_
        
        return params

    def _compute_z_fixedpoint(self, Wx_z: torch.Tensor, Rh_z: torch.Tensor, 
                               bx_z: torch.Tensor, br_z: torch.Tensor,
                               channel_idx: int, rescale: dict, 
                               sigmoid_lut: PiecewiseLUT, bitwidth: int) -> torch.Tensor:
        """
        è®¡ç®— z é—¨ï¼ˆçº¯å®šç‚¹ï¼Œä¸ CUDA computeZ ä¸€è‡´ï¼‰
        
        z = sigmoid(Wx + Rh + bx + br)
        """
        # Rescale å„é¡¹åˆ° z_pre çš„é‡åŒ–ç©ºé—´
        Wx_shifted = rshift_round(Wx_z - rescale['zp_Wx'], rescale['exp2_inv_Wx_div_z_pre'])
        Rh_shifted = rshift_round(Rh_z - rescale['zp_Rh'], rescale['exp2_inv_Rh_div_z_pre'])
        bx_shifted = rshift_round(bx_z, rescale['n_bx_div_z'][channel_idx])
        br_shifted = rshift_round(br_z, rescale['n_br_div_z'][channel_idx])
        
        z_pre = Wx_shifted + Rh_shifted + bx_shifted + br_shifted + rescale['zp_z_pre']
        
        # é€šè¿‡ LUT è®¡ç®— sigmoid
        if bitwidth == 16:
            z_pre = clamp_to_int16(z_pre)
        else:
            z_pre = clamp_to_int8(z_pre)
        
        z = piecewise_linear_forward(z_pre, sigmoid_lut, output_signed=False, bitwidth=bitwidth)
        return z

    def _compute_r_fixedpoint(self, Wx_r: torch.Tensor, Rh_r: torch.Tensor,
                               bx_r: torch.Tensor, br_r: torch.Tensor,
                               channel_idx: int, rescale: dict,
                               sigmoid_lut: PiecewiseLUT, bitwidth: int) -> torch.Tensor:
        """
        è®¡ç®— r é—¨ï¼ˆçº¯å®šç‚¹ï¼Œä¸ CUDA computeR ä¸€è‡´ï¼‰
        
        r = sigmoid(Wx + Rh + bx + br)
        """
        Wx_shifted = rshift_round(Wx_r - rescale['zp_Wx'], rescale['exp2_inv_Wx_div_r_pre'])
        Rh_shifted = rshift_round(Rh_r - rescale['zp_Rh'], rescale['exp2_inv_Rh_div_r_pre'])
        bx_shifted = rshift_round(bx_r, rescale['n_bx_div_r'][channel_idx])
        br_shifted = rshift_round(br_r, rescale['n_br_div_r'][channel_idx])
        
        r_pre = Wx_shifted + Rh_shifted + bx_shifted + br_shifted + rescale['zp_r_pre']
        
        if bitwidth == 16:
            r_pre = clamp_to_int16(r_pre)
        else:
            r_pre = clamp_to_int8(r_pre)
        
        r = piecewise_linear_forward(r_pre, sigmoid_lut, output_signed=False, bitwidth=bitwidth)
        return r

    def _compute_g_fixedpoint(self, Wx_g: torch.Tensor, Rh_g: torch.Tensor,
                               bx_g: torch.Tensor, br_g: torch.Tensor, r: torch.Tensor,
                               channel_idx: int, rescale: dict,
                               tanh_lut: PiecewiseLUT, bitwidth: int) -> torch.Tensor:
        """
        è®¡ç®— g é—¨ï¼ˆçº¯å®šç‚¹ï¼Œä¸ CUDA computeG ä¸€è‡´ï¼‰
        
        g = tanh(Wx + r * (Rh + br) + bx)
        """
        # Rh_add_br = Rh + br (rescale åˆ° Rh_add_br ç©ºé—´)
        Rh_shifted = rshift_round(Rh_g - rescale['zp_Rh'], rescale['n_Rh_div_Rh_add_br'])
        br_shifted = rshift_round(br_g, rescale['n_br_div_Rh_add_br'][channel_idx])
        Rh_add_br = Rh_shifted + br_shifted + rescale['zp_Rh_add_br']
        
        # rRh = r * Rh_add_br (æ•´æ•°ä¹˜æ³•ï¼Œç„¶å rescale)
        r_diff = (r - rescale['zp_r_out']).to(torch.int64)
        Rh_add_br_diff = (Rh_add_br - rescale['zp_Rh_add_br']).to(torch.int64)
        rRh_mul = r_diff * Rh_add_br_diff
        rRh = rshift_round_i64(rRh_mul, rescale['n_r_mul_Rh_add_br_div_rRh']).to(torch.int32) + rescale['zp_rRh']
        
        # g_pre = Wx + rRh + bx
        Wx_shifted = rshift_round(Wx_g - rescale['zp_Wx'], rescale['n_Wx_div_g_pre'])
        rRh_shifted = rshift_round(rRh - rescale['zp_rRh'], rescale['n_rRh_div_g_pre'])
        bx_shifted = rshift_round(bx_g, rescale['n_bx_div_g_pre'][channel_idx])
        
        g_pre = Wx_shifted + rRh_shifted + bx_shifted + rescale['zp_g_pre']
        
        if bitwidth == 16:
            g_pre = clamp_to_int16(g_pre)
        else:
            g_pre = clamp_to_int8(g_pre)
        
        g = piecewise_linear_forward(g_pre, tanh_lut, output_signed=True, bitwidth=bitwidth)
        return g

    def _compute_h_fixedpoint(self, z: torch.Tensor, g: torch.Tensor, h_old: torch.Tensor,
                               rescale: dict, bitwidth: int) -> torch.Tensor:
        """
        è®¡ç®—æ–°éšè—çŠ¶æ€ï¼ˆçº¯å®šç‚¹ï¼Œä¸ CUDA computeH ä¸€è‡´ï¼‰
        
        h_new = z * h_old + (1 - z) * g
        """
        # old_contrib = z * h_old
        z_diff = (z - rescale['zp_z_out']).to(torch.int64)
        h_diff = (h_old - rescale['zp_h']).to(torch.int64)
        old_contrib_mul = z_diff * h_diff
        old_contrib = rshift_round_i64(old_contrib_mul, rescale['n_z_mul_h_div_old_contrib']).to(torch.int32) + rescale['zp_old_contrib']
        
        # new_contrib = (1 - z) * g
        # (1 - z) åœ¨é‡åŒ–ç©ºé—´: one_in_z_scale - z
        one_minus_z_diff = rescale['one_in_z_scale'] - z.to(torch.int64)
        g_diff = (g - rescale['zp_g_out']).to(torch.int64)
        new_contrib_mul = one_minus_z_diff * g_diff
        new_contrib = rshift_round_i64(new_contrib_mul, rescale['n_z_out_mul_g_div_new_contrib']).to(torch.int32) + rescale['zp_new_contrib']
        
        # h_new = old_contrib + new_contrib (rescale åˆ° h ç©ºé—´)
        old_shifted = rshift_round(old_contrib - rescale['zp_old_contrib'], rescale['n_old_contrib_div_h'])
        new_shifted = rshift_round(new_contrib - rescale['zp_new_contrib'], rescale['n_new_contrib_div_h'])
        h_new = old_shifted + new_shifted + rescale['zp_h']
        
        # clamp åˆ°ç›®æ ‡ä½å®½
        if bitwidth == 16:
            h_new = clamp_to_int16(h_new)
        else:
            h_new = clamp_to_int8(h_new)
        
        return h_new

    # -------------------- å‘é‡åŒ–é—¨æ§è®¡ç®—ï¼ˆONNX å¯å¯¼å‡ºï¼‰--------------------
    
    def _compute_z_vectorized(self, Wx_z: torch.Tensor, Rh_z: torch.Tensor,
                               bx_z: torch.Tensor, br_z: torch.Tensor,
                               n_bx_div_z: torch.Tensor, n_br_div_z: torch.Tensor,
                               rescale: dict, sigmoid_lut: PiecewiseLUT, bitwidth: int) -> torch.Tensor:
        """
        å‘é‡åŒ–è®¡ç®— z é—¨ï¼ˆONNX å¯å¯¼å‡ºï¼‰
        
        Args:
            Wx_z: [B, H], Rh_z: [B, H]
            bx_z: [H], br_z: [H] (per-channel åç½®)
            n_bx_div_z: [H], n_br_div_z: [H] (per-channel shift)
        """
        # Wx, Rh: å…¨å±€ shiftï¼ˆæ ‡é‡ï¼‰
        Wx_shifted = rshift_round(Wx_z - rescale['zp_Wx'], rescale['exp2_inv_Wx_div_z_pre'])
        Rh_shifted = rshift_round(Rh_z - rescale['zp_Rh'], rescale['exp2_inv_Rh_div_z_pre'])
        
        # åç½®: per-channel shiftï¼ˆå‘é‡åŒ–ï¼‰
        # bx_z: [H], n_bx_div_z: [H] -> å¹¿æ’­åˆ° [1, H] ç„¶åå¯¹ [B, H] æ“ä½œ
        bx_shifted = rshift_round_per_channel(bx_z.unsqueeze(0).to(torch.int64), n_bx_div_z).squeeze(0).to(torch.int32)
        br_shifted = rshift_round_per_channel(br_z.unsqueeze(0).to(torch.int64), n_br_div_z).squeeze(0).to(torch.int32)
        
        # å¹¿æ’­åŠ æ³•: [B, H] + [H] -> [B, H]
        z_pre = Wx_shifted + Rh_shifted + bx_shifted.unsqueeze(0) + br_shifted.unsqueeze(0) + rescale['zp_z_pre']
        
        if bitwidth == 16:
            z_pre = clamp_to_int16(z_pre)
        else:
            z_pre = clamp_to_int8(z_pre)
        
        z = piecewise_linear_forward(z_pre, sigmoid_lut, output_signed=False, bitwidth=bitwidth)
        return z

    def _compute_r_vectorized(self, Wx_r: torch.Tensor, Rh_r: torch.Tensor,
                               bx_r: torch.Tensor, br_r: torch.Tensor,
                               n_bx_div_r: torch.Tensor, n_br_div_r: torch.Tensor,
                               rescale: dict, sigmoid_lut: PiecewiseLUT, bitwidth: int) -> torch.Tensor:
        """å‘é‡åŒ–è®¡ç®— r é—¨ï¼ˆONNX å¯å¯¼å‡ºï¼‰"""
        Wx_shifted = rshift_round(Wx_r - rescale['zp_Wx'], rescale['exp2_inv_Wx_div_r_pre'])
        Rh_shifted = rshift_round(Rh_r - rescale['zp_Rh'], rescale['exp2_inv_Rh_div_r_pre'])
        
        bx_shifted = rshift_round_per_channel(bx_r.unsqueeze(0).to(torch.int64), n_bx_div_r).squeeze(0).to(torch.int32)
        br_shifted = rshift_round_per_channel(br_r.unsqueeze(0).to(torch.int64), n_br_div_r).squeeze(0).to(torch.int32)
        
        r_pre = Wx_shifted + Rh_shifted + bx_shifted.unsqueeze(0) + br_shifted.unsqueeze(0) + rescale['zp_r_pre']
        
        if bitwidth == 16:
            r_pre = clamp_to_int16(r_pre)
        else:
            r_pre = clamp_to_int8(r_pre)
        
        r = piecewise_linear_forward(r_pre, sigmoid_lut, output_signed=False, bitwidth=bitwidth)
        return r

    def _compute_g_vectorized(self, Wx_g: torch.Tensor, Rh_g: torch.Tensor,
                               bx_g: torch.Tensor, br_g: torch.Tensor, r: torch.Tensor,
                               n_br_div_Rh_add_br: torch.Tensor, n_bx_div_g_pre: torch.Tensor,
                               rescale: dict, tanh_lut: PiecewiseLUT, bitwidth: int) -> torch.Tensor:
        """å‘é‡åŒ–è®¡ç®— g é—¨ï¼ˆONNX å¯å¯¼å‡ºï¼‰"""
        # Rh_add_br = Rh + br
        Rh_shifted = rshift_round(Rh_g - rescale['zp_Rh'], rescale['n_Rh_div_Rh_add_br'])
        br_shifted = rshift_round_per_channel(br_g.unsqueeze(0).to(torch.int64), n_br_div_Rh_add_br).squeeze(0).to(torch.int32)
        Rh_add_br = Rh_shifted + br_shifted.unsqueeze(0) + rescale['zp_Rh_add_br']
        
        # rRh = r * Rh_add_br
        r_diff = (r - rescale['zp_r_out']).to(torch.int64)
        Rh_add_br_diff = (Rh_add_br - rescale['zp_Rh_add_br']).to(torch.int64)
        rRh_mul = r_diff * Rh_add_br_diff
        rRh = rshift_round_i64(rRh_mul, rescale['n_r_mul_Rh_add_br_div_rRh']).to(torch.int32) + rescale['zp_rRh']
        
        # g_pre = Wx + rRh + bx
        Wx_shifted = rshift_round(Wx_g - rescale['zp_Wx'], rescale['n_Wx_div_g_pre'])
        rRh_shifted = rshift_round(rRh - rescale['zp_rRh'], rescale['n_rRh_div_g_pre'])
        bx_shifted = rshift_round_per_channel(bx_g.unsqueeze(0).to(torch.int64), n_bx_div_g_pre).squeeze(0).to(torch.int32)
        
        g_pre = Wx_shifted + rRh_shifted + bx_shifted.unsqueeze(0) + rescale['zp_g_pre']
        
        if bitwidth == 16:
            g_pre = clamp_to_int16(g_pre)
        else:
            g_pre = clamp_to_int8(g_pre)
        
        g = piecewise_linear_forward(g_pre, tanh_lut, output_signed=True, bitwidth=bitwidth)
        return g

    def _compute_h_vectorized(self, z: torch.Tensor, g: torch.Tensor, h_old: torch.Tensor,
                               rescale: dict, bitwidth: int) -> torch.Tensor:
        """å‘é‡åŒ–è®¡ç®—æ–°éšè—çŠ¶æ€ï¼ˆONNX å¯å¯¼å‡ºï¼‰"""
        # old_contrib = z * h_old
        z_diff = (z - rescale['zp_z_out']).to(torch.int64)
        h_diff = (h_old - rescale['zp_h']).to(torch.int64)
        old_contrib_mul = z_diff * h_diff
        old_contrib = rshift_round_i64(old_contrib_mul, rescale['n_z_mul_h_div_old_contrib']).to(torch.int32) + rescale['zp_old_contrib']
        
        # new_contrib = (1 - z) * g
        one_minus_z_diff = rescale['one_in_z_scale'] - z.to(torch.int64)
        g_diff = (g - rescale['zp_g_out']).to(torch.int64)
        new_contrib_mul = one_minus_z_diff * g_diff
        new_contrib = rshift_round_i64(new_contrib_mul, rescale['n_z_out_mul_g_div_new_contrib']).to(torch.int32) + rescale['zp_new_contrib']
        
        # h_new = old_contrib + new_contrib
        old_shifted = rshift_round(old_contrib - rescale['zp_old_contrib'], rescale['n_old_contrib_div_h'])
        new_shifted = rshift_round(new_contrib - rescale['zp_new_contrib'], rescale['n_new_contrib_div_h'])
        h_new = old_shifted + new_shifted + rescale['zp_h']
        
        if bitwidth == 16:
            h_new = clamp_to_int16(h_new)
        else:
            h_new = clamp_to_int8(h_new)
        
        return h_new

    def _forward_python_fixedpoint_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
            quant_params
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        çº¯å®šç‚¹å®ç°çš„å•å‘ GRU å‰å‘ä¼ æ’­ï¼ˆç”¨äº ONNX å¯¼å‡ºï¼‰
        
        æ‰€æœ‰ä¸­é—´è®¡ç®—éƒ½æ˜¯æ•´æ•°è¿ç®—ï¼Œsigmoid/tanh ä½¿ç”¨ LUT æŸ¥è¡¨ã€‚
        é‡åŒ–å‚æ•°å’Œ rescale é€»è¾‘ä¸ CUDA å®ç°å®Œå…¨ä¸€è‡´ã€‚
        
        [ONNXå…¼å®¹è¯´æ˜]
        éƒ¨åˆ†æ“ä½œçš„å®ç°æ–¹å¼ä¸ CUDA ç«¯ä¸åŒï¼Œä½†æ•°å­¦ä¸Šç­‰ä»·ï¼ŒONNX å¯¼å‡ºåç”±æ¨ç†å¼•æ“
        é€‰æ‹©æœ€ä¼˜å®ç°ã€‚è¿™äº›å·®å¼‚åœ¨ä»£ç ä¸­ç”¨ [ONNXå…¼å®¹] æ ‡è®°è¯´æ˜ï¼š
        - GEMM: Python ç”¨ float æ¨¡æ‹Ÿï¼ŒCUDA ç”¨ cuBLAS INT8/èåˆ INT16 kernel
        - Rescale: Python ç»Ÿä¸€ç”¨åå¤„ç†ï¼ŒCUDA INT16 ç”¨èåˆæ–¹å¼
        
        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—ï¼ˆæµ®ç‚¹ï¼Œä¼šè¢«é‡åŒ–ï¼‰
            h0: [B, H] åˆå§‹éšè—çŠ¶æ€ æˆ– None
            weight_ih: [3*H, I] è¾“å…¥æƒé‡
            weight_hh: [3*H, H] å¾ªç¯æƒé‡
            bias_ih: [3*H] è¾“å…¥åç½® æˆ– None
            bias_hh: [3*H] å¾ªç¯åç½® æˆ– None
            quant_params: é‡åŒ–å‚æ•°ï¼ˆæ¥è‡ª finalize_calibrationï¼‰
            
        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—ï¼ˆæµ®ç‚¹ï¼Œä»æ•´æ•°åé‡åŒ–ï¼‰
            h_n: [1, B, H] æœ€ç»ˆéšè—çŠ¶æ€
        """
        T, B, I = input.shape
        H = self.hidden_size
        device = input.device
        
        # è·å–ä½å®½é…ç½®
        h_bitwidth = self._get_bitwidth('h')
        z_out_bitwidth = self._get_bitwidth('z_out')
        r_out_bitwidth = self._get_bitwidth('r_out')
        g_out_bitwidth = self._get_bitwidth('g_out')
        
        # æ„å»º rescale å‚æ•°
        rescale = self._build_rescale_params(quant_params, H, device)
        
        # ç”Ÿæˆ LUT
        sigmoid_z_lut = generate_sigmoid_lut(
            rescale['exp2_inv_z_pre'], rescale['zp_z_pre'],
            rescale['exp2_inv_z_out'], rescale['zp_z_out'],
            bitwidth=z_out_bitwidth
        )
        sigmoid_r_lut = generate_sigmoid_lut(
            rescale['exp2_inv_r_pre'], rescale['zp_r_pre'],
            rescale['exp2_inv_r_out'], rescale['zp_r_out'],
            bitwidth=r_out_bitwidth
        )
        tanh_lut = generate_tanh_lut(
            rescale['exp2_inv_g_pre'], rescale['zp_g_pre'],
            rescale['exp2_inv_g_out'], rescale['zp_g_out'],
            bitwidth=g_out_bitwidth
        )
        
        # é‡åŒ–å‚æ•°
        exp2_x = quant_params.exp2_inv_x_
        zp_x = quant_params.zp_x_
        exp2_h = quant_params.exp2_inv_h_
        zp_h = quant_params.zp_h_
        
        # é‡æ’åºæƒé‡/åç½®ï¼šPyTorch æ ¼å¼ (r, z, n) -> Haste æ ¼å¼ (z, r, n)
        # ä¸ GRUFunction.forward ä¸€è‡´
        W_reordered = reorder_weights_pytorch_to_haste(weight_ih)  # [3*H, I], (z, r, n) é¡ºåº
        R_reordered = reorder_weights_pytorch_to_haste(weight_hh)  # [3*H, H], (z, r, n) é¡ºåº
        
        # è½¬ç½®æƒé‡åˆ° CUDA æ ¼å¼ï¼š[3*H, I] -> [I, 3*H]
        W = W_reordered.t().contiguous()  # [I, 3*H]
        R = R_reordered.t().contiguous()  # [H, 3*H]
        
        # é‡åŒ–æƒé‡ï¼ˆper-channelï¼Œzp=0ï¼‰
        # quantificationPerChannel(W, W_quant, input_size, 3*hidden_size, exp2_inv_W)
        exp2_W = list(quant_params.exp2_inv_W_)
        exp2_R = list(quant_params.exp2_inv_R_)
        W_q = quantize_per_channel(W, exp2_W, zp=0, 
                                   bitwidth=self._get_bitwidth('W'), symmetric=self._get_symmetric('W'))
        R_q = quantize_per_channel(R, exp2_R, zp=0,
                                   bitwidth=self._get_bitwidth('R'), symmetric=self._get_symmetric('R'))
        
        # é‡æ’åºåç½®ï¼šPyTorch æ ¼å¼ (r, z, n) -> Haste æ ¼å¼ (z, r, n)
        if bias_ih is not None:
            bx_reordered = reorder_weights_pytorch_to_haste(bias_ih)
        else:
            bx_reordered = None
            
        if bias_hh is not None:
            br_reordered = reorder_weights_pytorch_to_haste(bias_hh)
        else:
            br_reordered = None
        
        # é‡åŒ–åç½®ï¼ˆper-channelï¼Œzp=0ï¼Œä½¿ç”¨ INT32ï¼‰
        exp2_bx = list(quant_params.exp2_inv_bx_)
        exp2_br = list(quant_params.exp2_inv_br_)
        
        if bx_reordered is not None:
            bx_q = quantize_per_channel(bx_reordered.unsqueeze(0), exp2_bx, zp=0, bitwidth=32).squeeze(0)
        else:
            bx_q = torch.zeros(3 * H, device=device, dtype=torch.int32)
            
        if br_reordered is not None:
            br_q = quantize_per_channel(br_reordered.unsqueeze(0), exp2_br, zp=0, bitwidth=32).squeeze(0)
        else:
            br_q = torch.zeros(3 * H, device=device, dtype=torch.int32)
        
        # ========== å¾ªç¯å¤–ä¸€æ¬¡æ€§é‡åŒ–è¾“å…¥ x ==========
        # input: [T, B, I] -> x_q_all: [T*B, I]
        x_flat = input.reshape(T * B, I)
        x_q_all = quantize(x_flat, exp2_x, zp_x, 
                           bitwidth=self._get_bitwidth('x'), symmetric=self._get_symmetric('x'))
        
        # ========== å¾ªç¯å¤–ä¸€æ¬¡æ€§è®¡ç®— Wx GEMM ==========
        # x_q_all: [T*B, I], W_q: [I, 3*H]
        # Wx_all_raw: [T*B, 3*H] (int64)
        # 
        # [ONNXå…¼å®¹] ä¸ CUDA å®ç°å·®å¼‚ï¼š
        #   - CUDA INT8: ä½¿ç”¨ cuBLAS INT8 GEMM (cublasGemmEx)
        #   - CUDA INT16: ä½¿ç”¨èåˆ kernel (quantizedGemmInt16Fused)
        #   - Python: ä½¿ç”¨ float GEMM æ¨¡æ‹Ÿï¼ˆæ•°å€¼ç­‰ä»·ï¼Œint16*int16 åœ¨ float32 å¯ç²¾ç¡®è¡¨ç¤ºï¼‰
        #   - ONNX å¯¼å‡ºåç”±æ¨ç†å¼•æ“é€‰æ‹©æœ€ä¼˜å®ç° (MatMulInteger/QLinearMatMul)
        Wx_all_raw = torch.mm(x_q_all.to(torch.int64).float(), W_q.to(torch.int64).float()).to(torch.int64)
        
        # ========== Rescale Wx: (Wx_raw - W_sum_mul_x_zp) >> n + zp_Wx ==========
        # [ONNXå…¼å®¹] ä¸ CUDA å®ç°å·®å¼‚ï¼š
        #   - CUDA INT8: GEMM åå¤„ç† (rescaleGemmI32 kernel)
        #   - CUDA INT16: èåˆåœ¨ GEMM kernel ä¸­
        #   - Python: ç»Ÿä¸€ä½¿ç”¨åå¤„ç†æ–¹å¼ï¼ˆæ•°å­¦ç­‰ä»·ï¼šÎ£ W[k]*(x[k]-zp) = W@x - W_sum*zpï¼‰
        # è®¡ç®— W_sum_mul_x_zp[c] = sum_k(W_q[k, c]) * zp_x
        W_sum = W_q.sum(dim=0).to(torch.int64)  # [3*H]
        W_sum_mul_x_zp = W_sum * zp_x  # [3*H]
        
        # n_W_mul_x_div_Wx[c] = (exp2_inv_W[c] + exp2_inv_x) - exp2_inv_Wx (per-channel)
        n_W_mul_x_div_Wx = torch.tensor(rescale['n_W_mul_x_div_Wx'], device=device, dtype=torch.int8)  # [3*H]
        
        # å‘é‡åŒ– per-channel rescale: Wx[b, c] = rshift_round(Wx_raw[b, c] - W_sum_mul_x_zp[c], n[c]) + zp_Wx
        # Wx_all_raw: [T*B, 3*H], W_sum_mul_x_zp: [3*H] -> å¹¿æ’­å‡æ³•
        Wx_compensated = Wx_all_raw - W_sum_mul_x_zp.unsqueeze(0)  # [T*B, 3*H]
        Wx_all = rshift_round_per_channel(Wx_compensated, n_W_mul_x_div_Wx).to(torch.int32) + quant_params.zp_Wx_
        
        # é‡å¡‘ä¸º [T, B, 3*H]
        Wx_all = Wx_all.reshape(T, B, 3 * H)
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        if h0 is None:
            h_q = torch.full((B, H), zp_h, device=device, dtype=torch.int32)
        else:
            h_q = quantize(h0, exp2_h, zp_h, bitwidth=h_bitwidth, symmetric=self._get_symmetric('h'))
        
        # R_sum_mul_h_zp å’Œ n_R_mul_h_div_Rh é¢„è®¡ç®—
        R_sum = R_q.sum(dim=0).to(torch.int64)  # [3*H]
        R_sum_mul_h_zp = R_sum * zp_h  # [3*H], å¸¸é‡ï¼Œå¾ªç¯å¤–è®¡ç®—
        n_R_mul_h_div_Rh = torch.tensor(rescale['n_R_mul_h_div_Rh'], device=device, dtype=torch.int8)  # [3*H]
        
        # é¢„è®¡ç®— per-channel bias shift å¼ é‡ï¼ˆONNX å¯¼å‡ºéœ€è¦ï¼‰
        # z é—¨: bx[0:H], br[0:H]
        n_bx_div_z = torch.tensor(rescale['n_bx_div_z'], device=device, dtype=torch.int8)  # [H]
        n_br_div_z = torch.tensor(rescale['n_br_div_z'], device=device, dtype=torch.int8)  # [H]
        bx_z = bx_q[:H]  # [H]
        br_z = br_q[:H]  # [H]
        
        # r é—¨: bx[H:2H], br[H:2H]
        n_bx_div_r = torch.tensor(rescale['n_bx_div_r'], device=device, dtype=torch.int8)  # [H]
        n_br_div_r = torch.tensor(rescale['n_br_div_r'], device=device, dtype=torch.int8)  # [H]
        bx_r = bx_q[H:2*H]  # [H]
        br_r = br_q[H:2*H]  # [H]
        
        # g é—¨: bx[2H:3H], br[2H:3H]
        n_br_div_Rh_add_br = torch.tensor(rescale['n_br_div_Rh_add_br'], device=device, dtype=torch.int8)  # [H]
        n_bx_div_g_pre = torch.tensor(rescale['n_bx_div_g_pre'], device=device, dtype=torch.int8)  # [H]
        bx_g = bx_q[2*H:3*H]  # [H]
        br_g = br_q[2*H:3*H]  # [H]
        
        # é¢„åˆ†é…è¾“å‡ºå¼ é‡ï¼ˆONNX å¯¼å‡ºéœ€è¦ï¼Œé¿å… list appendï¼‰
        outputs_q = torch.zeros(T, B, H, device=device, dtype=torch.int32)
        
        for t in range(T):
            # è·å–å½“å‰æ—¶é—´æ­¥çš„ Wxï¼ˆå·²åœ¨å¾ªç¯å¤–è®¡ç®—å¥½ï¼‰
            Wx = Wx_all[t]  # [B, 3*H]
            
            # ========== è®¡ç®— Rh GEMMï¼ˆæ¯ä¸ªæ—¶é—´æ­¥ä¾èµ–ä¸Šä¸€æ­¥çš„ hï¼‰==========
            # h_q: [B, H], R_q: [H, 3*H], Rh_raw: [B, 3*H]
            # [ONNXå…¼å®¹] åŒ Wx GEMMï¼Œä½¿ç”¨ float æ¨¡æ‹Ÿæ•´æ•° GEMM
            Rh_raw = torch.mm(h_q.to(torch.int64).float(), R_q.to(torch.int64).float()).to(torch.int64)
            
            # Rescale Rh: (Rh_raw - R_sum_mul_h_zp) >> n + zp_Rh (per-channel å‘é‡åŒ–)
            # [ONNXå…¼å®¹] åŒ Wx rescaleï¼Œä½¿ç”¨åå¤„ç†æ–¹å¼
            Rh_compensated = Rh_raw - R_sum_mul_h_zp.unsqueeze(0)  # [B, 3*H]
            Rh = rshift_round_per_channel(Rh_compensated, n_R_mul_h_div_Rh).to(torch.int32) + quant_params.zp_Rh_
            
            # åˆ†å‰²é—¨æ§ï¼ˆHaste æ ¼å¼ï¼šz, r, nï¼‰
            Wx_z, Wx_r, Wx_n = Wx.chunk(3, dim=1)
            Rh_z, Rh_r, Rh_n = Rh.chunk(3, dim=1)
            
            # å‘é‡åŒ–é—¨æ§è®¡ç®—ï¼ˆONNX å¯å¯¼å‡ºï¼‰
            z_out = self._compute_z_vectorized(
                Wx_z, Rh_z, bx_z, br_z,
                n_bx_div_z, n_br_div_z,
                rescale, sigmoid_z_lut, z_out_bitwidth
            )
            
            r_out = self._compute_r_vectorized(
                Wx_r, Rh_r, bx_r, br_r,
                n_bx_div_r, n_br_div_r,
                rescale, sigmoid_r_lut, r_out_bitwidth
            )
            
            g_out = self._compute_g_vectorized(
                Wx_n, Rh_n, bx_g, br_g, r_out,
                n_br_div_Rh_add_br, n_bx_div_g_pre,
                rescale, tanh_lut, g_out_bitwidth
            )
            
            h_new = self._compute_h_vectorized(
                z_out, g_out, h_q,
                rescale, h_bitwidth
            )
            
            h_q = h_new
            
            # å­˜å‚¨é‡åŒ–å€¼ï¼ˆä½¿ç”¨ç´¢å¼•èµ‹å€¼ï¼Œé¿å… list appendï¼‰
            outputs_q[t] = h_q
        
        # å¾ªç¯ç»“æŸåä¸€æ¬¡æ€§åé‡åŒ–æ‰€æœ‰æ—¶é—´æ­¥ï¼ˆä¸ CUDA dev::dequantification ä¸€è‡´ï¼‰
        # outputs_q: [T, B, H] (é‡åŒ–ï¼Œå·²é¢„åˆ†é…)
        output = dequantize(outputs_q, exp2_h, zp_h)  # [T, B, H] (æµ®ç‚¹)
        h_n = dequantize(h_q, exp2_h, zp_h).unsqueeze(0)  # [1, B, H]
        
        return output, h_n

    # -------------------- ONNX å¯¼å‡ºç‰ˆæœ¬ï¼ˆQDQ æ ¼å¼ï¼‰--------------------
    
    def _forward_onnx_qdq_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
            quant_params
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ç”¨äº ONNX å¯¼å‡ºçš„ QDQ æ ¼å¼å‰å‘ä¼ æ’­
        
        ä½¿ç”¨ä¼ªé‡åŒ–ï¼ˆFake Quantizeï¼‰åœ¨å…³é”®ç‚¹æ’å…¥ Q/DQ æ“ä½œï¼Œ
        æ¨ç†å¼•æ“ä¼šè¯†åˆ« QDQ æ¨¡å¼å¹¶è‡ªåŠ¨ä¼˜åŒ–ä¸ºé‡åŒ–ç®—å­ã€‚
        
        è®¾è®¡åŸåˆ™ï¼š
        ==========
        [ä¸ CUDA ä¸€è‡´]
          - é‡åŒ–å‚æ•°ï¼ˆscale/zpï¼‰å®Œå…¨ä¸€è‡´
          - è®¡ç®—å›¾ç»“æ„ä¸€è‡´ï¼ˆé—¨é¡ºåºã€è®¡ç®—é¡ºåºï¼‰
          - æƒé‡/åç½®çš„ per-channel é‡åŒ–å‚æ•°ä¸€è‡´
          
        [ONNX å…¼å®¹ - ä¸ CUDA å®ç°ä¸åŒ]
          - GEMM: ä½¿ç”¨æ ‡å‡† torch.mmï¼ˆæ¨ç†å¼•æ“ä¼šç”¨ MatMulIntegerï¼‰
          - sigmoid/tanh: ä½¿ç”¨æ ‡å‡† torch.sigmoid/tanhï¼ˆæ¨ç†å¼•æ“ä¼šä¼˜åŒ–ï¼‰
          - rescale: é€šè¿‡ QDQ å®ç°ï¼ˆä¸ç”¨æ˜¾å¼ rshift_roundï¼‰
        
        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—
            h0: [B, H] åˆå§‹éšè—çŠ¶æ€ æˆ– None
            weight_ih: [3*H, I] è¾“å…¥æƒé‡
            weight_hh: [3*H, H] å¾ªç¯æƒé‡
            bias_ih: [3*H] è¾“å…¥åç½® æˆ– None
            bias_hh: [3*H] å¾ªç¯åç½® æˆ– None
            quant_params: é‡åŒ–å‚æ•°
            
        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—
            h_n: [1, B, H] æœ€ç»ˆéšè—çŠ¶æ€
        """
        T, B, I = input.shape
        H = self.hidden_size
        device = input.device
        dtype = input.dtype
        
        # ========== é‡åŒ–å‚æ•°æå– ==========
        # [ä¸ CUDA ä¸€è‡´] ä½¿ç”¨ç›¸åŒçš„é‡åŒ–å‚æ•°
        exp2_x = quant_params.exp2_inv_x_
        zp_x = quant_params.zp_x_
        exp2_h = quant_params.exp2_inv_h_
        zp_h = quant_params.zp_h_
        exp2_Wx = quant_params.exp2_inv_Wx_
        zp_Wx = quant_params.zp_Wx_
        exp2_Rh = quant_params.exp2_inv_Rh_
        zp_Rh = quant_params.zp_Rh_
        
        # æ¿€æ´»å‡½æ•°é‡åŒ–å‚æ•°
        exp2_z_pre = quant_params.exp2_inv_z_pre_
        zp_z_pre = quant_params.zp_z_pre_
        exp2_z_out = quant_params.exp2_inv_z_out_
        zp_z_out = quant_params.zp_z_out_
        
        exp2_r_pre = quant_params.exp2_inv_r_pre_
        zp_r_pre = quant_params.zp_r_pre_
        exp2_r_out = quant_params.exp2_inv_r_out_
        zp_r_out = quant_params.zp_r_out_
        
        exp2_g_pre = quant_params.exp2_inv_g_pre_
        zp_g_pre = quant_params.zp_g_pre_
        exp2_g_out = quant_params.exp2_inv_g_out_
        zp_g_out = quant_params.zp_g_out_
        
        # per-channel é‡åŒ–å‚æ•°
        exp2_W = list(quant_params.exp2_inv_W_)
        exp2_R = list(quant_params.exp2_inv_R_)
        exp2_bx = list(quant_params.exp2_inv_bx_)
        exp2_br = list(quant_params.exp2_inv_br_)
        
        # ========== æƒé‡é‡æ’åº ==========
        # [ä¸ CUDA ä¸€è‡´] PyTorch æ ¼å¼ (r, z, n) -> Haste æ ¼å¼ (z, r, n)
        W_reordered = reorder_weights_pytorch_to_haste(weight_ih)  # [3*H, I]
        R_reordered = reorder_weights_pytorch_to_haste(weight_hh)  # [3*H, H]
        
        if bias_ih is not None:
            bx_reordered = reorder_weights_pytorch_to_haste(bias_ih)  # [3*H]
        else:
            bx_reordered = torch.zeros(3 * H, device=device, dtype=dtype)
            
        if bias_hh is not None:
            br_reordered = reorder_weights_pytorch_to_haste(bias_hh)  # [3*H]
        else:
            br_reordered = torch.zeros(3 * H, device=device, dtype=dtype)
        
        # ========== æƒé‡ä¼ªé‡åŒ– ==========
        # [ä¸ CUDA ä¸€è‡´] per-channel é‡åŒ–
        # [ONNX å…¼å®¹] ä½¿ç”¨ fake_quantize ä¿æŒæµ®ç‚¹æ ¼å¼
        W_q = fake_quantize_per_channel(W_reordered.t(), exp2_W, zp=0,
                                        bitwidth=self._get_bitwidth('W'),
                                        symmetric=self._get_symmetric('W')).t()
        R_q = fake_quantize_per_channel(R_reordered.t(), exp2_R, zp=0,
                                        bitwidth=self._get_bitwidth('R'),
                                        symmetric=self._get_symmetric('R')).t()
        bx_q = fake_quantize_per_channel(bx_reordered.unsqueeze(0), exp2_bx, zp=0,
                                         bitwidth=32, symmetric=True).squeeze(0)
        br_q = fake_quantize_per_channel(br_reordered.unsqueeze(0), exp2_br, zp=0,
                                         bitwidth=32, symmetric=True).squeeze(0)
        
        # åˆ†å‰²åç½®ï¼ˆHaste æ ¼å¼ï¼šz, r, nï¼‰
        bx_z, bx_r, bx_n = bx_q.chunk(3)  # å„ [H]
        br_z, br_r, br_n = br_q.chunk(3)  # å„ [H]
        
        # ========== åˆå§‹åŒ–éšè—çŠ¶æ€ ==========
        if h0 is None:
            h = torch.zeros(B, H, device=device, dtype=dtype)
        else:
            h = h0
        
        # [ä¸ CUDA ä¸€è‡´] é‡åŒ–åˆå§‹çŠ¶æ€
        h = fake_quantize(h, exp2_h, zp_h, bitwidth=self._get_bitwidth('h'),
                          symmetric=self._get_symmetric('h'))
        
        # ========== è¾“å…¥ä¼ªé‡åŒ– ==========
        # [ä¸ CUDA ä¸€è‡´] æ‰€æœ‰æ—¶é—´æ­¥ä¸€èµ·é‡åŒ–
        x_q = fake_quantize(input, exp2_x, zp_x, bitwidth=self._get_bitwidth('x'),
                            symmetric=self._get_symmetric('x'))
        
        # ========== Wx GEMMï¼ˆå¾ªç¯å¤–ä¸€æ¬¡æ€§è®¡ç®—ï¼‰==========
        # [ä¸ CUDA ä¸€è‡´] è®¡ç®—é¡ºåºä¸€è‡´
        # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† matmulï¼Œæ¨ç†å¼•æ“ä¼šæ›¿æ¢ä¸º MatMulInteger
        # x_q: [T, B, I], W_q: [3*H, I] -> Wx: [T, B, 3*H]
        Wx_all = torch.matmul(x_q, W_q.t())  # [T, B, 3*H]
        
        # [ä¸ CUDA ä¸€è‡´] GEMM è¾“å‡ºé‡åŒ–
        Wx_all = fake_quantize(Wx_all, exp2_Wx, zp_Wx, bitwidth=self._get_bitwidth('Wx'),
                               symmetric=self._get_symmetric('Wx'))
        
        # é¢„åˆ†é…è¾“å‡ºå¼ é‡ï¼ˆONNX å‹å¥½ï¼Œé¿å…åŠ¨æ€åˆ—è¡¨ï¼‰
        outputs = torch.zeros(T, B, H, device=device, dtype=dtype)
        
        for t in range(T):
            Wx = Wx_all[t]  # [B, 3*H]
            
            # ========== Rh GEMM ==========
            # [ä¸ CUDA ä¸€è‡´] æ¯ä¸ªæ—¶é—´æ­¥è®¡ç®— Rh
            # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† matmul
            Rh = torch.mm(h, R_q.t())  # [B, 3*H]
            
            # [ä¸ CUDA ä¸€è‡´] GEMM è¾“å‡ºé‡åŒ–
            Rh = fake_quantize(Rh, exp2_Rh, zp_Rh, bitwidth=self._get_bitwidth('Rh'),
                               symmetric=self._get_symmetric('Rh'))
            
            # ========== åˆ†å‰²é—¨æ§ ==========
            # [ä¸ CUDA ä¸€è‡´] Haste æ ¼å¼ (z, r, n)
            Wx_z, Wx_r, Wx_n = Wx.chunk(3, dim=1)  # å„ [B, H]
            Rh_z, Rh_r, Rh_n = Rh.chunk(3, dim=1)  # å„ [B, H]
            
            # ========== z é—¨ï¼ˆUpdate Gateï¼‰==========
            # [ä¸ CUDA ä¸€è‡´] z = sigmoid(Wx_z + Rh_z + bx_z + br_z)
            z_pre = Wx_z + Rh_z + bx_z.unsqueeze(0) + br_z.unsqueeze(0)
            
            # [ä¸ CUDA ä¸€è‡´] æ¿€æ´»å‰é‡åŒ–
            z_pre = fake_quantize(z_pre, exp2_z_pre, zp_z_pre,
                                  bitwidth=self._get_bitwidth('z_pre'),
                                  symmetric=self._get_symmetric('z_pre'))
            
            # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† sigmoidï¼ˆæ¨ç†å¼•æ“ä¼šç”¨é‡åŒ–ç‰ˆæœ¬æˆ– LUTï¼‰
            z = torch.sigmoid(z_pre)
            
            # [ä¸ CUDA ä¸€è‡´] æ¿€æ´»åé‡åŒ–
            z = fake_quantize(z, exp2_z_out, zp_z_out,
                              bitwidth=self._get_bitwidth('z_out'),
                              symmetric=False)  # sigmoid è¾“å‡ºæ˜¯ [0,1]ï¼Œéå¯¹ç§°
            
            # ========== r é—¨ï¼ˆReset Gateï¼‰==========
            # [ä¸ CUDA ä¸€è‡´] r = sigmoid(Wx_r + Rh_r + bx_r + br_r)
            r_pre = Wx_r + Rh_r + bx_r.unsqueeze(0) + br_r.unsqueeze(0)
            
            r_pre = fake_quantize(r_pre, exp2_r_pre, zp_r_pre,
                                  bitwidth=self._get_bitwidth('r_pre'),
                                  symmetric=self._get_symmetric('r_pre'))
            
            # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† sigmoid
            r = torch.sigmoid(r_pre)
            
            r = fake_quantize(r, exp2_r_out, zp_r_out,
                              bitwidth=self._get_bitwidth('r_out'),
                              symmetric=False)
            
            # ========== g é—¨ï¼ˆNew Gate / Candidateï¼‰==========
            # [ä¸ CUDA ä¸€è‡´] g = tanh(Wx_n + r * (Rh_n + br_n) + bx_n)
            Rh_add_br = Rh_n + br_n.unsqueeze(0)
            
            # [ä¸ CUDA ä¸€è‡´] ä¸­é—´ç»“æœé‡åŒ–
            Rh_add_br = fake_quantize(Rh_add_br, quant_params.exp2_inv_Rh_add_br_,
                                      quant_params.zp_Rh_add_br_,
                                      bitwidth=16, symmetric=True)
            
            rRh = r * Rh_add_br
            
            # [ä¸ CUDA ä¸€è‡´] ä¹˜ç§¯é‡åŒ–
            rRh = fake_quantize(rRh, quant_params.exp2_inv_rRh_,
                                quant_params.zp_rRh_,
                                bitwidth=16, symmetric=True)
            
            g_pre = Wx_n + rRh + bx_n.unsqueeze(0)
            
            g_pre = fake_quantize(g_pre, exp2_g_pre, zp_g_pre,
                                  bitwidth=self._get_bitwidth('g_pre'),
                                  symmetric=self._get_symmetric('g_pre'))
            
            # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† tanh
            g = torch.tanh(g_pre)
            
            g = fake_quantize(g, exp2_g_out, zp_g_out,
                              bitwidth=self._get_bitwidth('g_out'),
                              symmetric=True)  # tanh è¾“å‡ºæ˜¯ [-1,1]ï¼Œå¯¹ç§°
            
            # ========== æ–°éšè—çŠ¶æ€ ==========
            # [ä¸ CUDA ä¸€è‡´] h_new = z * h + (1 - z) * g
            # CUDA computeH åˆ†åˆ«è®¡ç®—å¹¶é‡åŒ– old_contrib å’Œ new_contrib
            
            # old_contrib = z * h
            old_contrib = z * h
            old_contrib = fake_quantize(old_contrib, quant_params.exp2_inv_old_contrib_,
                                        quant_params.zp_old_contrib_,
                                        bitwidth=16, symmetric=True)
            
            # new_contrib = (1 - z) * g
            new_contrib = (1 - z) * g
            new_contrib = fake_quantize(new_contrib, quant_params.exp2_inv_new_contrib_,
                                        quant_params.zp_new_contrib_,
                                        bitwidth=16, symmetric=True)
            
            # h_new = old_contrib + new_contrib
            h_new = old_contrib + new_contrib
            
            # [ä¸ CUDA ä¸€è‡´] è¾“å‡ºé‡åŒ–
            h_new = fake_quantize(h_new, exp2_h, zp_h,
                                  bitwidth=self._get_bitwidth('h'),
                                  symmetric=self._get_symmetric('h'))
            
            h = h_new
            
            # ä½¿ç”¨ç´¢å¼•èµ‹å€¼å­˜å‚¨ï¼ˆONNX å‹å¥½ï¼‰
            outputs[t] = h
        
        # ========== è¾“å‡º ==========
        output = outputs  # [T, B, H]ï¼Œå·²é¢„åˆ†é…
        h_n = h.unsqueeze(0)  # [1, B, H]
        
        return output, h_n

    def _forward_python(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        çº¯ PyTorch å®ç°çš„ GRU å‰å‘ä¼ æ’­ï¼ˆç”¨äº ONNX å¯¼å‡ºï¼‰

        æ”¯æŒå•å‘å’ŒåŒå‘æ¨¡å¼
        
        Note:
            é‡åŒ–æ¨¡å¼ä¸‹ä½¿ç”¨çº¯å®šç‚¹è®¡ç®—ï¼Œä¸ CUDA é‡åŒ–å®ç°å®Œå…¨ä¸€è‡´
        """
        if self.batch_first:
            input = input.transpose(0, 1).contiguous()

        T, B, I = input.shape
        H = self.hidden_size
        device = input.device

        # åˆå§‹çŠ¶æ€å¤„ç†
        h0_forward, h0_reverse = None, None
        if hx is not None:
            expected_layers = self.num_layers * self.num_directions
            expected_shape = (expected_layers, B, H)
            if hx.shape != expected_shape:
                raise ValueError(f"hx å½¢çŠ¶åº”ä¸º {expected_shape}ï¼Œå®é™… {hx.shape}")
            h0_forward = hx[0]
            if self.bidirectional:
                h0_reverse = hx[1]

        # å‰å‘æ–¹å‘
        output_forward, h_n_forward = self._forward_python_single_direction(
            input, h0_forward,
            self.weight_ih_l0, self.weight_hh_l0,
            self.bias_ih_l0 if self.bias else None,
            self.bias_hh_l0 if self.bias else None,
            self.quant_params
        )

        if self.bidirectional:
            # åå‘æ–¹å‘ï¼ˆè¾“å…¥éœ€è¦ç¿»è½¬ï¼‰
            output_reverse, h_n_reverse = self._forward_python_single_direction(
                input.flip(0), h0_reverse,
                self.weight_ih_l0_reverse, self.weight_hh_l0_reverse,
                self.bias_ih_l0_reverse if self.bias else None,
                self.bias_hh_l0_reverse if self.bias else None,
                self.quant_params_reverse
            )

            # åè½¬åå‘è¾“å‡ºä»¥å¯¹é½æ—¶é—´æ­¥
            output_reverse = output_reverse.flip(0)
            # æ‹¼æ¥è¾“å‡º: [T, B, H] + [T, B, H] -> [T, B, 2H]
            output = torch.cat([output_forward, output_reverse], dim=-1)
            # æ‹¼æ¥éšè—çŠ¶æ€: [1, B, H] + [1, B, H] -> [2, B, H]
            h_n = torch.cat([h_n_forward, h_n_reverse], dim=0)
        else:
            output = output_forward
            h_n = h_n_forward

        if self.batch_first:
            output = output.transpose(0, 1).contiguous()

        return output, h_n

    # -------------------- ä¸» forward æ–¹æ³• --------------------

    def forward(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input: [T, B, I] æˆ– [B, T, I] (batch_first) çš„è¾“å…¥
            hx: åˆå§‹éšè—çŠ¶æ€ï¼Œå•å‘ [1, B, H]ï¼ŒåŒå‘ [2, B, H]
            
        Returns:
            output: [T, B, H] æˆ– [T, B, 2H] (åŒå‘)
            h_n: [1, B, H] æˆ– [2, B, H] (åŒå‘)

        Note:
            - export_mode=False (é»˜è®¤): ä½¿ç”¨ CUDA C++ å®ç°ï¼ˆé«˜æ€§èƒ½ï¼‰
            - export_mode=True: ä½¿ç”¨çº¯ PyTorch å®ç°ï¼ˆå¯è¢« ONNX è¿½è¸ªï¼‰
        """
        # ===== ONNX å¯¼å‡ºæ¨¡å¼ï¼šä½¿ç”¨çº¯ PyTorch å®ç° =====
        if self.export_mode:
            return self._forward_python(input, hx)

        # ===== æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨ CUDA C++ å®ç° =====
        self._ensure_cublas_initialized()

        # é‡åŒ–æ¨¡å¼ä¸‹æ£€æŸ¥æ ¡å‡†çŠ¶æ€
        if self.use_quantization:
            if self._calibration_dirty:
                # æ ¡å‡†æ•°æ®å·²æ›´æ–°ï¼Œéœ€è¦é‡æ–°è®¡ç®—é‡åŒ–å‚æ•°
                self.finalize_calibration()
            elif not self.is_calibrated():
                # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„æ ¡å‡†æ•°æ®ï¼ˆæ”¯æŒ minmax å’Œ histogram ä¸¤ç§æ–¹æ³•ï¼‰
                if self.quant_ranges is not None or self.hist_collectors is not None:
                    # å·²ç´¯ç§¯æ•°æ®ä½†æœªå®Œæˆæ ¡å‡†ï¼Œè‡ªåŠ¨è°ƒç”¨ finalize
                    self.finalize_calibration()
                else:
                    raise RuntimeError("é‡åŒ–å·²å¯ç”¨ä½†æœªæ ¡å‡†ï¼Œè¯·å…ˆè°ƒç”¨ calibrate() å’Œ finalize_calibration()")

        if self.batch_first:
            input = input.transpose(0, 1).contiguous()

        seq_len, batch_size, input_size = input.shape
        hidden_size = self.hidden_size

        device = input.device if input.is_cuda else torch.device('cuda')
        input = ensure_cuda_float32(input, device)

        # åˆå§‹çŠ¶æ€å¤„ç†
        h0_forward, h0_reverse = None, None
        if hx is not None:
            expected_layers = self.num_layers * self.num_directions
            expected_shape = (expected_layers, batch_size, hidden_size)
            if hx.shape != expected_shape:
                raise ValueError(f"hx å½¢çŠ¶åº”ä¸º {expected_shape}ï¼Œå®é™… {hx.shape}")
            h0_forward = ensure_cuda_float32(hx[0], device)
            if self.bidirectional:
                h0_reverse = ensure_cuda_float32(hx[1], device)

        # å‰å‘æ–¹å‘
        output_forward, h_n_forward = GRUFunction.apply(
            input, self.weight_ih_l0, self.weight_hh_l0,
            self.bias_ih_l0 if self.bias else None,
            self.bias_hh_l0 if self.bias else None,
            h0_forward, self.training, self.use_quantization, self.quant_params)

        if self.bidirectional:
            # åå‘æ–¹å‘
            output_reverse, h_n_reverse = GRUFunction.apply(
                input.flip(0), self.weight_ih_l0_reverse, self.weight_hh_l0_reverse,
                self.bias_ih_l0_reverse if self.bias else None,
                self.bias_hh_l0_reverse if self.bias else None,
                h0_reverse, self.training, self.use_quantization, self.quant_params_reverse)

            # åè½¬åå‘è¾“å‡ºä»¥å¯¹é½æ—¶é—´æ­¥
            output_reverse = output_reverse.flip(0)
            # æ‹¼æ¥è¾“å‡º: [T, B, H] + [T, B, H] -> [T, B, 2H]
            output = torch.cat([output_forward, output_reverse], dim=-1)
            # æ‹¼æ¥éšè—çŠ¶æ€: [1, B, H] + [1, B, H] -> [2, B, H]
            h_n = torch.cat([h_n_forward, h_n_reverse], dim=0)
        else:
            output = output_forward
            h_n = h_n_forward

        if self.batch_first:
            output = output.transpose(0, 1).contiguous()

        return output, h_n


# ============================================================
#                      è°ƒè¯•å·¥å…·å‡½æ•°
# ============================================================

def print_quant_params(gru: QuantGRU):
    """
    æ‰“å° QuantGRU çš„é‡åŒ–å‚æ•°

    Args:
        gru: å·²å®Œæˆæ ¡å‡†çš„ QuantGRU å®ä¾‹
    """
    if not gru.is_calibrated():
        raise RuntimeError("è¯·å…ˆè°ƒç”¨ finalize_calibration()")

    params = gru.quant_params
    print("=" * 60)
    print("GRUQuantitativeParameters (é‡åŒ–å‚æ•°)")
    print("=" * 60)
    print(f"  hidden_ = {params.hidden_}")
    print(f"  [x]  exp2_inv={params.exp2_inv_x_:3d}, zp={params.zp_x_}")
    print(f"  [h]  exp2_inv={params.exp2_inv_h_:3d}, zp={params.zp_h_}")
    print(f"  [Wx] exp2_inv={params.exp2_inv_Wx_:3d}, zp={params.zp_Wx_}")
    print(f"  [Rh] exp2_inv={params.exp2_inv_Rh_:3d}, zp={params.zp_Rh_}")
    print("-" * 60)
    print(f"  [z_pre] exp2_inv={params.exp2_inv_z_pre_:3d}, zp={params.zp_z_pre_}")
    print(f"  [r_pre] exp2_inv={params.exp2_inv_r_pre_:3d}, zp={params.zp_r_pre_}")
    print(f"  [g_pre] exp2_inv={params.exp2_inv_g_pre_:3d}, zp={params.zp_g_pre_}")
    print(f"  [z_out] exp2_inv={params.exp2_inv_z_out_:3d}, zp={params.zp_z_out_}")
    print(f"  [r_out] exp2_inv={params.exp2_inv_r_out_:3d}, zp={params.zp_r_out_}")
    print(f"  [g_out] exp2_inv={params.exp2_inv_g_out_:3d}, zp={params.zp_g_out_}")
    print("-" * 60)
    print(f"  [Rh_add_br_g]        exp2_inv={params.exp2_inv_Rh_add_br_:3d}, zp={params.zp_Rh_add_br_}")
    print(f"  [rRh]              exp2_inv={params.exp2_inv_rRh_:3d}, zp={params.zp_rRh_}")
    print(f"  [new_contrib]      exp2_inv={params.exp2_inv_new_contrib_:3d}, zp={params.zp_new_contrib_}")
    print(f"  [old_contrib]      exp2_inv={params.exp2_inv_old_contrib_:3d}, zp={params.zp_old_contrib_}")
    print("-" * 60)
    if params.exp2_inv_W_:
        print(f"  [W] exp2_inv (first 5): {list(params.exp2_inv_W_[:5])} ...")
    if params.exp2_inv_R_:
        print(f"  [R] exp2_inv (first 5): {list(params.exp2_inv_R_[:5])} ...")
    if params.exp2_inv_bx_:
        print(f"  [bx] exp2_inv (first 5): {list(params.exp2_inv_bx_[:5])} ...")
    if params.exp2_inv_br_:
        print(f"  [br] exp2_inv (first 5): {list(params.exp2_inv_br_[:5])} ...")
    print("=" * 60)


def print_quant_ranges(gru: QuantGRU):
    """
    æ‰“å° QuantGRU çš„é‡åŒ–èŒƒå›´

    Args:
        gru: å·²è°ƒç”¨ calibrate() çš„ QuantGRU å®ä¾‹
    """
    if gru.quant_ranges is None:
        raise RuntimeError("è¯·å…ˆè°ƒç”¨ calibrate()")

    r = gru.quant_ranges
    print("=" * 60)
    print("GRUQuantizationRanges (é‡åŒ–èŒƒå›´)")
    print("=" * 60)
    print(f"  hidden_ = {r.hidden_}")
    print(f"  [x]  min={r.min_x_:12.6f}, max={r.max_x_:12.6f}")
    print(f"  [h]  min={r.min_h_:12.6f}, max={r.max_h_:12.6f}")
    print(f"  [Wx] min={r.min_Wx_:12.6f}, max={r.max_Wx_:12.6f}")
    print(f"  [Rh] min={r.min_Rh_:12.6f}, max={r.max_Rh_:12.6f}")
    print("-" * 60)
    print(f"  [z_pre] min={r.min_z_pre_:12.6f}, max={r.max_z_pre_:12.6f}")
    print(f"  [r_pre] min={r.min_r_pre_:12.6f}, max={r.max_r_pre_:12.6f}")
    print(f"  [g_pre] min={r.min_g_pre_:12.6f}, max={r.max_g_pre_:12.6f}")
    print(f"  [z_out] min={r.min_z_out_:12.6f}, max={r.max_z_out_:12.6f}")
    print(f"  [r_out] min={r.min_r_out_:12.6f}, max={r.max_r_out_:12.6f}")
    print(f"  [g_out] min={r.min_g_out_:12.6f}, max={r.max_g_out_:12.6f}")
    print("-" * 60)
    print(f"  [Rh_add_br_g]        min={r.min_Rh_add_br_g_:12.6f}, max={r.max_Rh_add_br_g_:12.6f}")
    print(f"  [rRh]              min={r.min_rRh_:12.6f}, max={r.max_rRh_:12.6f}")
    print(f"  [new_contrib]      min={r.min_new_contrib_:12.6f}, max={r.max_new_contrib_:12.6f}")
    print(f"  [old_contrib]      min={r.min_old_contrib_:12.6f}, max={r.max_old_contrib_:12.6f}")
    print("=" * 60)
