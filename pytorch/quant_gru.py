"""
QuantGRU - ÊîØÊåÅÈáèÂåñÁöÑ GRU ÂÆûÁé∞

ÂäüËÉΩÁâπÊÄß:
    - ÂÖºÂÆπ nn.GRU Êé•Âè£ÔºàÊîØÊåÅ batch_first„ÄÅbidirectional Á≠âÂèÇÊï∞Ôºâ
    - ÊîØÊåÅ INT8/INT16/INT32 ÈáèÂåñÊé®ÁêÜ
    - ÊîØÊåÅ MinMax Âíå AIMET È£éÊ†ºÁõ¥ÊñπÂõæÊ†°ÂáÜ
    - Âª∂ËøüÂàùÂßãÂåñËÆæËÆ°ÔºåÊîØÊåÅ pickle/deepcopy Â∫èÂàóÂåñ
    - ÊîØÊåÅ ONNX ÂØºÂá∫Ôºà‰ΩøÁî®Á∫Ø PyTorch ÂÆûÁé∞Ôºâ

ÂÖ≥ÈîÆÂ±ûÊÄß:
    - use_quantization: ÊòØÂê¶ÂêØÁî®ÈáèÂåñÔºàÈªòËÆ§ FalseÔºâ
    - export_mode: ÊòØÂê¶‰ΩøÁî® ONNX ÂØºÂá∫Ê®°ÂºèÔºàÈªòËÆ§ FalseÔºâ
    - export_format: ÂØºÂá∫Ê†ºÂºè 'float'|'qdq'ÔºàÈ´òÁ∫ßÈÄâÈ°πÔºåÈªòËÆ§ 'float'Ôºâ

ÂÖ∏ÂûãÁî®Ê≥ï:
    >>> from quant_gru import QuantGRU
    >>>
    >>> # ÂàõÂª∫Âπ∂Ê†°ÂáÜÊ®°Âûã
    >>> gru = QuantGRU(64, 128, batch_first=True).cuda()
    >>> gru.calibrate(calibration_data)
    >>> gru.use_quantization = True
    >>>
    >>> # Ê≠£Â∏∏Êé®ÁêÜÔºàCUDA ÈáèÂåñÊ®°ÂºèÔºâ
    >>> output = gru(x)
    
ONNX ÂØºÂá∫:
    >>> # ÂêØÁî®ÂØºÂá∫Ê®°ÂºèÔºàÈªòËÆ§‰ΩøÁî®ÊµÆÁÇπÊ†ºÂºèÔºâ
    >>> gru.export_mode = True
    >>> torch.onnx.export(gru, x, "model.onnx")
    >>> gru.export_mode = False  # ÊÅ¢Â§ç
    >>> 
    >>> # ÈáèÂåñÊ®°ÂûãÂØºÂá∫ÈúÄÊåáÂÆöÊ†ºÂºè
    >>> gru.export_format = 'qdq'  # 'float' | 'qdq'
"""

import json
import torch
import torch.nn as nn
from typing import Optional, Tuple

try:
    import gru_interface_binding as gru_ops
except ImportError:
    raise ImportError(
        "gru_interface_binding Ê®°ÂùóÊú™ÊâæÂà∞ÔºåËØ∑ÂÖàËøêË°å setup.py ÁºñËØë C++ Êâ©Â±ï"
    )


# ============================================================
#                      ‰ΩçÂÆΩÈÖçÁΩÆÂ∑•ÂÖ∑ÂáΩÊï∞
# ============================================================


def _get_bitwidth_value(op_cfg: dict) -> int:
    """‰ªéÈÖçÁΩÆ‰∏≠Ëé∑Âèñ‰ΩçÂÆΩÂÄºÔºà8/16/32ÔºâÔºåÈªòËÆ§ 8"""
    return op_cfg.get('bitwidth', 8)


def _get_symmetric_value(op_cfg: dict) -> bool:
    """‰ªéÈÖçÁΩÆ‰∏≠Ëé∑ÂèñÊòØÂê¶ÂØπÁß∞ÈáèÂåñÔºåÈªòËÆ§ True"""
    return op_cfg.get('is_symmetric', True)


def load_bitwidth_config(config_file: str) -> gru_ops.OperatorQuantConfig:
    """
    ‰ªé JSON Êñá‰ª∂Âä†ËΩΩÈáèÂåñÈÖçÁΩÆ
    
    Args:
        config_file: ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
        
    Returns:
        OperatorQuantConfig ÂØπË±°
    """
    with open(config_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    config = gru_ops.OperatorQuantConfig()
    gru_config = data.get('GRU_config', {})
    op_config = gru_config.get('operator_config', {})

    # Â≠óÊÆµÊò†Â∞Ñ: JSON key -> (‰ΩçÂÆΩÂ±ûÊÄßÂêç, ÂØπÁß∞ÈáèÂåñÂ±ûÊÄßÂêç)
    field_map = {
        "input.x": ("x_", "x_symmetric_"),
        "input.h": ("h_", "h_symmetric_"),
        "weight.W": ("W_", "W_symmetric_"),
        "weight.R": ("R_", "R_symmetric_"),
        "weight.bx": ("bx_", "bx_symmetric_"),
        "weight.br": ("br_", "br_symmetric_"),
        "matmul.Wx": ("Wx_", "Wx_symmetric_"),
        "matmul.Rh": ("Rh_", "Rh_symmetric_"),
        "gate.z_pre": ("z_pre_", "z_pre_symmetric_"),
        "gate.z_out": ("z_out_", "z_out_symmetric_"),
        "gate.r_pre": ("r_pre_", "r_pre_symmetric_"),
        "gate.r_out": ("r_out_", "r_out_symmetric_"),
        "gate.g_pre": ("g_pre_", "g_pre_symmetric_"),
        "gate.g_out": ("g_out_", "g_out_symmetric_"),
        "op.Rh_add_br": ("Rh_add_br_", "Rh_add_br_symmetric_"),
        "op.rRh": ("rRh_", "rRh_symmetric_"),
        "op.old_contrib": ("old_contrib_", "old_contrib_symmetric_"),
        "op.new_contrib": ("new_contrib_", "new_contrib_symmetric_"),
    }

    for json_key, (bw_attr, sym_attr) in field_map.items():
        if json_key in op_config:
            op_cfg = op_config[json_key]
            # ËÆæÁΩÆ‰ΩçÂÆΩ
            bw_val = _get_bitwidth_value(op_cfg)
            setattr(config, bw_attr, bw_val)
            # ËÆæÁΩÆÂØπÁß∞ÈáèÂåñÈÖçÁΩÆ
            sym_val = _get_symmetric_value(op_cfg)
            setattr(config, sym_attr, sym_val)

    return config


def _format_bitwidth(val: int) -> str:
    """Ê†ºÂºèÂåñ‰ΩçÂÆΩÂÄº: 8 -> '8bit'"""
    return f"{abs(val)}bit"


def _format_symmetric(is_symmetric: bool) -> str:
    """Ê†ºÂºèÂåñÂØπÁß∞ÈáèÂåñ: True -> 'ÂØπÁß∞'"""
    return "ÂØπÁß∞" if is_symmetric else "ÈùûÂØπÁß∞"


def apply_bitwidth_config(config: gru_ops.OperatorQuantConfig,
                          config_file: str,
                          verbose: bool = False) -> int:
    """
    ‰ªé JSON Êñá‰ª∂Â∫îÁî®ÈÖçÁΩÆÂà∞Áé∞Êúâ OperatorQuantConfig ÂØπË±°
    
    Args:
        config: Ë¶ÅÊõ¥Êñ∞ÁöÑÈÖçÁΩÆÂØπË±°
        config_file: ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
        verbose: ÊòØÂê¶ÊâìÂç∞ÈÖçÁΩÆËØ¶ÊÉÖ
        
    Returns:
        ÈÖçÁΩÆÁöÑÂ≠óÊÆµÊï∞Èáè
    """
    loaded = load_bitwidth_config(config_file)

    # ‰ΩçÂÆΩÈÖçÁΩÆÂ≠óÊÆµÔºà18 ‰∏™Ôºâ
    bitwidth_attrs = ['x_', 'h_', 'W_', 'R_', 'bx_', 'br_', 'Wx_', 'Rh_',
                      'z_pre_', 'z_out_', 'r_pre_', 'r_out_', 'g_pre_', 'g_out_',
                      'Rh_add_br_', 'rRh_', 'old_contrib_', 'new_contrib_']
    for attr in bitwidth_attrs:
        setattr(config, attr, getattr(loaded, attr))

    # ÂØπÁß∞ÈáèÂåñÈÖçÁΩÆÂ≠óÊÆµÔºà18 ‰∏™Ôºâ
    symmetric_attrs = ['x_symmetric_', 'h_symmetric_', 'W_symmetric_', 'R_symmetric_',
                       'bx_symmetric_', 'br_symmetric_', 'Wx_symmetric_', 'Rh_symmetric_',
                       'z_pre_symmetric_', 'z_out_symmetric_', 'r_pre_symmetric_', 'r_out_symmetric_',
                       'g_pre_symmetric_', 'g_out_symmetric_', 'Rh_add_br_symmetric_', 'rRh_symmetric_',
                       'old_contrib_symmetric_', 'new_contrib_symmetric_']
    for attr in symmetric_attrs:
        setattr(config, attr, getattr(loaded, attr))

    if verbose:
        print("\n" + "=" * 70)
        print("üîß Â∫îÁî® GRU ÈáèÂåñÈÖçÁΩÆÔºà‰ΩçÂÆΩ + ÂØπÁß∞ÈáèÂåñÔºâ")
        print("=" * 70)
        print(f"üìÑ ÈÖçÁΩÆÊñá‰ª∂: {config_file}")
        print("-" * 70)
        print(f"  [ËæìÂÖ•]  x: {_format_bitwidth(config.x_):6s} ({_format_symmetric(config.x_symmetric_)})")
        print(f"          h: {_format_bitwidth(config.h_):6s} ({_format_symmetric(config.h_symmetric_)})")
        print(f"  [ÊùÉÈáç]  W: {_format_bitwidth(config.W_):6s} ({_format_symmetric(config.W_symmetric_)})")
        print(f"          R: {_format_bitwidth(config.R_):6s} ({_format_symmetric(config.R_symmetric_)})")
        print(f"          bx: {_format_bitwidth(config.bx_):6s} ({_format_symmetric(config.bx_symmetric_)})")
        print(f"          br: {_format_bitwidth(config.br_):6s} ({_format_symmetric(config.br_symmetric_)})")
        print(f"  [Áü©Èòµ]  Wx: {_format_bitwidth(config.Wx_):6s} ({_format_symmetric(config.Wx_symmetric_)})")
        print(f"          Rh: {_format_bitwidth(config.Rh_):6s} ({_format_symmetric(config.Rh_symmetric_)})")
        print(f"  [Èó®Êéß]  z_pre: {_format_bitwidth(config.z_pre_):6s} ({_format_symmetric(config.z_pre_symmetric_)})")
        print(f"          z_out: {_format_bitwidth(config.z_out_):6s} ({_format_symmetric(config.z_out_symmetric_)})")
        print(f"          r_pre: {_format_bitwidth(config.r_pre_):6s} ({_format_symmetric(config.r_pre_symmetric_)})")
        print(f"          r_out: {_format_bitwidth(config.r_out_):6s} ({_format_symmetric(config.r_out_symmetric_)})")
        print(f"          g_pre: {_format_bitwidth(config.g_pre_):6s} ({_format_symmetric(config.g_pre_symmetric_)})")
        print(f"          g_out: {_format_bitwidth(config.g_out_):6s} ({_format_symmetric(config.g_out_symmetric_)})")
        print(
            f"  [ËøêÁÆó]  Rh+br: {_format_bitwidth(config.Rh_add_br_):6s} ({_format_symmetric(config.Rh_add_br_symmetric_)})")
        print(f"          rRh: {_format_bitwidth(config.rRh_):6s} ({_format_symmetric(config.rRh_symmetric_)})")
        print(
            f"  [ËæìÂá∫]  old: {_format_bitwidth(config.old_contrib_):6s} ({_format_symmetric(config.old_contrib_symmetric_)})")
        print(
            f"          new: {_format_bitwidth(config.new_contrib_):6s} ({_format_symmetric(config.new_contrib_symmetric_)})")
        print("=" * 70 + "\n")

    return len(bitwidth_attrs) + len(symmetric_attrs)  # 36 ‰∏™Â≠óÊÆµ


# ============================================================
#                      ÊùÉÈáçÊ†ºÂºèËΩ¨Êç¢
# ============================================================

def reorder_weights_pytorch_to_haste(w: torch.Tensor) -> torch.Tensor:
    """
    PyTorch ÊùÉÈáçÊ†ºÂºè (r,z,n) -> Haste Ê†ºÂºè (z,r,n)
    
    Args:
        w: ÂΩ¢Áä∂ [3*H, ...] ÁöÑÊùÉÈáçÂº†Èáè
        
    Returns:
        ÈáçÊéíÂ∫èÂêéÁöÑÂº†ÈáèÔºåÂΩ¢Áä∂‰∏çÂèò
    """
    w = w.contiguous()
    h3 = w.shape[0] // 3
    device = w.device
    # [r, z, n] -> [z, r, n]
    indices = torch.cat([
        torch.arange(h3, 2 * h3, device=device),
        torch.arange(0, h3, device=device),
        torch.arange(2 * h3, 3 * h3, device=device)
    ])
    return w.index_select(0, indices).contiguous()


def reorder_weights_haste_to_pytorch(w: torch.Tensor) -> torch.Tensor:
    """
    Haste ÊùÉÈáçÊ†ºÂºè (z,r,n) -> PyTorch Ê†ºÂºè (r,z,n)
    
    Args:
        w: ÂΩ¢Áä∂ [3*H, ...] ÁöÑÊùÉÈáçÂº†Èáè
        
    Returns:
        ÈáçÊéíÂ∫èÂêéÁöÑÂº†ÈáèÔºåÂΩ¢Áä∂‰∏çÂèò
    """
    w = w.contiguous()
    h3 = w.shape[0] // 3
    device = w.device
    # [z, r, n] -> [r, z, n]
    indices = torch.cat([
        torch.arange(h3, 2 * h3, device=device),
        torch.arange(0, h3, device=device),
        torch.arange(2 * h3, 3 * h3, device=device)
    ])
    return w.index_select(0, indices).contiguous()


def ensure_cuda_float32(tensor: torch.Tensor, device: torch.device) -> torch.Tensor:
    """Á°Æ‰øùÂº†ÈáèÂú® CUDA ‰∏ä‰∏î‰∏∫ float32 Á±ªÂûã"""
    if not tensor.is_cuda:
        tensor = tensor.to(device)
    if tensor.dtype != torch.float32:
        tensor = tensor.float()
    return tensor

# ============================================================
#                      QDQ (Quantize-Dequantize) ËæÖÂä©ÂáΩÊï∞
#                      Áî®‰∫é ONNX ÂØºÂá∫ÁöÑ‰º™ÈáèÂåñÊìç‰Ωú
# ============================================================

def fake_quantize(x: torch.Tensor, exp2_inv: int, zp: int = 0,
                  bitwidth: int = 8, symmetric: bool = True,
                  is_unsigned: bool = False) -> torch.Tensor:
    """
    ‰º™ÈáèÂåñÔºàFake QuantizeÔºâ: ÈáèÂåñÂêéÁ´ãÂç≥ÂèçÈáèÂåñÔºå‰øùÊåÅÊµÆÁÇπÊ†ºÂºè
    
    Áî®‰∫é ONNX ÂØºÂá∫ÔºåÊé®ÁêÜÂºïÊìé‰ºöËØÜÂà´ QDQ Ê®°ÂºèÂπ∂‰ºòÂåñ
    
    [‰∏é CUDA ‰∏ÄËá¥] ÈáèÂåñÂèÇÊï∞ (exp2_inv, zp) ‰∏é CUDA Á´ØÂÆåÂÖ®‰∏ÄËá¥
    [ONNX ÂÖºÂÆπ] ‰ΩøÁî®ÊµÆÁÇπËøêÁÆóÊ®°ÊãüÈáèÂåñÊïàÊûú
    
    Args:
        x: ËæìÂÖ•Âº†Èáè
        exp2_inv: ÈáèÂåñÊåáÊï∞ (scale = 2^(-exp2_inv))
        zp: Èõ∂ÁÇπ
        bitwidth: ‰ΩçÂÆΩ (8/16/32)
        symmetric: ÂØπÁß∞ÈáèÂåñ (ÂΩ±Âìç zp ÁöÑ‰ΩøÁî®ÊñπÂºè)
        is_unsigned: ÊòØÂê¶‰ΩøÁî®Êó†Á¨¶Âè∑ËåÉÂõ¥ (UINT)Ôºå‰∏é symmetric Áã¨Á´ã
                     - False: INT ËåÉÂõ¥ (-128~127, -32768~32767)
                     - True: UINT ËåÉÂõ¥ (0~255, 0~65535)
    """
    # ËÆ°ÁÆó scale
    if exp2_inv >= 0:
        scale = 1.0 / (1 << exp2_inv)
    else:
        scale = float(1 << (-exp2_inv))
    
    # Á°ÆÂÆöÈáèÂåñËåÉÂõ¥ÔºöÁî± is_unsigned ÂÜ≥ÂÆö INT/UINT
    if bitwidth == 8:
        qmin, qmax = (0, 255) if is_unsigned else (-128, 127)
    elif bitwidth == 16:
        qmin, qmax = (0, 65535) if is_unsigned else (-32768, 32767)
    else:
        qmin, qmax = (0, 4294967295) if is_unsigned else (-2147483648, 2147483647)
    
    # ÈáèÂåñ: q = clamp(round(x / scale) + zp, qmin, qmax)
    # Ê≥®ÊÑè: torch.round ‰ΩøÁî®Èì∂Ë°åÂÆ∂ËàçÂÖ•Ôºå‰∏é CUDA ÁöÑ round half up Áï•ÊúâÂ∑ÆÂºÇ
    # ‰ΩÜÂÆûÈôÖÂΩ±ÂìçÊûÅÂ∞è (ÈöèÊú∫Êï∞ÊçÆÂ∑ÆÂºÇÁéá < 0.001%)
    q = torch.clamp(torch.round(x / scale) + zp, qmin, qmax)
    
    # ÂèçÈáèÂåñ: x' = (q - zp) * scale
    x_dequant = (q - zp) * scale
    
    return x_dequant


def fake_quantize_per_channel(x: torch.Tensor, exp2_invs: list, zp: int = 0,
                               bitwidth: int = 8, symmetric: bool = True,
                               is_unsigned: bool = False) -> torch.Tensor:
    """
    Per-channel ‰º™ÈáèÂåñ
    
    [‰∏é CUDA ‰∏ÄËá¥] per-channel ÈáèÂåñÂèÇÊï∞‰∏é CUDA quantificationPerChannel ‰∏ÄËá¥
    [ONNX ÂÖºÂÆπ] ‰ΩøÁî®ÊµÆÁÇπËøêÁÆóÊ®°ÊãüÈáèÂåñÊïàÊûú
    
    Args:
        x: ËæìÂÖ•Âº†Èáè
        exp2_invs: per-channel ÈáèÂåñÊåáÊï∞ÂàóË°®
        zp: Èõ∂ÁÇπ
        bitwidth: ‰ΩçÂÆΩ (8/16/32)
        symmetric: ÂØπÁß∞ÈáèÂåñ
        is_unsigned: ÊòØÂê¶‰ΩøÁî®Êó†Á¨¶Âè∑ËåÉÂõ¥ (UINT)
    """
    # Á°ÆÂÆöÈáèÂåñËåÉÂõ¥ÔºöÁî± is_unsigned ÂÜ≥ÂÆö INT/UINT
    if bitwidth == 8:
        qmin, qmax = (0, 255) if is_unsigned else (-128, 127)
    elif bitwidth == 16:
        qmin, qmax = (0, 65535) if is_unsigned else (-32768, 32767)
    else:
        qmin, qmax = (0, 4294967295) if is_unsigned else (-2147483648, 2147483647)
    
    device = x.device
    result = torch.zeros_like(x)
    channel_size = len(exp2_invs)
    
    for c in range(channel_size):
        exp2_inv = exp2_invs[c]
        if exp2_inv >= 0:
            scale = 1.0 / (1 << exp2_inv)
        else:
            scale = float(1 << (-exp2_inv))
        
        q = torch.clamp(torch.round(x[..., c] / scale) + zp, qmin, qmax)
        result[..., c] = (q - zp) * scale
    
    return result


# ============================================================
#                      GRUFunction (autograd)
# ============================================================

class GRUFunction(torch.autograd.Function):
    """
    GRU Ëá™ÂÆö‰πâ autograd Function
    
    Ë¥üË¥£ PyTorch/Haste Ê†ºÂºèËΩ¨Êç¢„ÄÅË∞ÉÁî® C++ Êé•Âè£„ÄÅÁÆ°ÁêÜÂèçÂêë‰º†Êí≠
    """

    @staticmethod
    def forward(ctx, input, weight_ih, weight_hh, bias_ih, bias_hh, h0, is_training,
                use_quantization=False, quant_params=None):
        """
        ÂâçÂêë‰º†Êí≠
        
        Args:
            input: [T, B, I] ËæìÂÖ•Â∫èÂàó
            weight_ih: [3*H, I] ËæìÂÖ•ÊùÉÈáç (PyTorch r,z,n Ê†ºÂºè)
            weight_hh: [3*H, H] Âæ™ÁéØÊùÉÈáç
            bias_ih, bias_hh: [3*H] ÂÅèÁΩÆÊàñ None
            h0: [B, H] ÂàùÂßãÁä∂ÊÄÅÊàñ None
            is_training: ËÆ≠ÁªÉÊ®°ÂºèÊ†áÂøó
            use_quantization: ÈáèÂåñÂºÄÂÖ≥
            quant_params: ÈáèÂåñÂèÇÊï∞
            
        Returns:
            output: [T, B, H] ËæìÂá∫Â∫èÂàó
            h_n: [1, B, H] ÊúÄÁªàÁä∂ÊÄÅ
        """
        time_steps, batch_size, input_size = input.shape
        hidden_size = weight_hh.shape[1]

        # ‰øùÂ≠òÁª¥Â∫¶‰ø°ÊÅØÂíå None Ê†áÂøó
        ctx.time_steps, ctx.batch_size = time_steps, batch_size
        ctx.input_size, ctx.hidden_size = input_size, hidden_size
        ctx.bias_ih_is_none = (bias_ih is None)
        ctx.bias_hh_is_none = (bias_hh is None)
        ctx.h0_is_none = (h0 is None)

        device = input.device if input.is_cuda else torch.device('cuda')
        input = ensure_cuda_float32(input, device)

        # ÊùÉÈáçÊ†ºÂºèËΩ¨Êç¢: PyTorch (r,z,n) -> Haste (z,r,n)ÔºåÂπ∂ËΩ¨ÁΩÆ
        weight_ih = ensure_cuda_float32(weight_ih, device)
        weight_hh = ensure_cuda_float32(weight_hh, device)
        W = reorder_weights_pytorch_to_haste(weight_ih).t().contiguous()
        R = reorder_weights_pytorch_to_haste(weight_hh).t().contiguous()

        # ÂÅèÁΩÆÂ§ÑÁêÜ
        if bias_ih is not None and bias_hh is not None:
            bias_ih = ensure_cuda_float32(bias_ih, device)
            bias_hh = ensure_cuda_float32(bias_hh, device)
            bx = reorder_weights_pytorch_to_haste(bias_ih).contiguous()
            br = reorder_weights_pytorch_to_haste(bias_hh).contiguous()
        else:
            bx = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)
            br = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)

        # ÂàùÂßãÁä∂ÊÄÅ
        h0_tensor = ensure_cuda_float32(h0, device) if h0 is not None else torch.empty(0, device=device,
                                                                                       dtype=torch.float32)

        # ÈáèÂåñÂèÇÊï∞
        if use_quantization:
            if quant_params is None:
                raise RuntimeError("use_quantization=True Êó∂ÂøÖÈ°ªÊèê‰æõ quant_params")
        else:
            quant_params = gru_ops.GRUQuantitativeParameters()

        # Ë∞ÉÁî® C++ ÂâçÂêëÊé•Âè£
        output_full, v = gru_ops.forward_interface(
            is_training=is_training,
            is_quant=use_quantization,
            time_steps=time_steps,
            batch_size=batch_size,
            input_size=input_size,
            hidden_size=hidden_size,
            W=W,
            R=R,
            bx=bx,
            br=br,
            x=input,
            h0=h0_tensor,
            quant_params=quant_params
        )

        # ÂàÜÁ¶ªËæìÂá∫: output_full[0] ÊòØÂàùÂßãÁä∂ÊÄÅÔºå[1:] ÊòØÊó∂Èó¥Ê≠•ËæìÂá∫
        output = output_full[1:]
        h_n = output_full[-1:]

        # ‰øùÂ≠òÂèçÂêë‰º†Êí≠ÊâÄÈúÄÁöÑ‰∏≠Èó¥ÁªìÊûú
        ctx.save_for_backward(W, R, bx, br, input, output_full, v)

        return output, h_n

    @staticmethod
    def backward(ctx, grad_output, grad_h_n):
        """
        ÂèçÂêë‰º†Êí≠
        
        Args:
            grad_output: [T, B, H] ËæìÂá∫Ê¢ØÂ∫¶
            grad_h_n: [1, B, H] ÊúÄÁªàÁä∂ÊÄÅÊ¢ØÂ∫¶
            
        Returns:
            ÂØπÂ∫î forward ÂêÑÂèÇÊï∞ÁöÑÊ¢ØÂ∫¶
        """
        W, R, bx, br, input, h, v = ctx.saved_tensors
        time_steps, batch_size = ctx.time_steps, ctx.batch_size
        input_size, hidden_size = ctx.input_size, ctx.hidden_size

        # Á°Æ‰øùÊâÄÊúâÂº†ÈáèÂú® CUDA ‰∏ä
        device = grad_output.device
        tensors = [W, R, bx, br, input, h]
        W, R, bx, br, input, h = [t.to(device) if not t.is_cuda else t for t in tensors]
        if v is not None and not v.is_cuda:
            v = v.to(device)
        if not grad_output.is_cuda:
            grad_output = grad_output.to(device)
        if grad_h_n is not None and not grad_h_n.is_cuda:
            grad_h_n = grad_h_n.to(device)

        # ÊûÑÂª∫ÈöêËóèÁä∂ÊÄÅÊ¢ØÂ∫¶
        # C++ Êé•Âè£ÈúÄË¶Å [T+1, B, H] Ê†ºÂºè
        # dh_new[0] ÊòØÂàùÂßãÁä∂ÊÄÅÊ¢ØÂ∫¶Ôºà‰øùÊåÅ‰∏∫ 0ÔºâÔºådh_new[1:] ÊòØÊó∂Èó¥Ê≠•Ê¢ØÂ∫¶
        dh_new = torch.zeros(
            (time_steps + 1, batch_size, hidden_size),
            device=device, dtype=grad_output.dtype
        )
        dh_new[1:] = grad_output

        # Á¥ØÂä†ÊúÄÁªàÁä∂ÊÄÅÊ¢ØÂ∫¶Ôºàoutput[-1] Âíå h_n[0] ÊåáÂêëÂêå‰∏ÄÊó∂Èó¥Ê≠•Ôºâ
        if grad_h_n is not None and grad_h_n.numel() > 0:
            dh_new[-1] = dh_new[-1] + grad_h_n[0]

        # Ë∞ÉÁî® C++ ÂèçÂêëÊé•Âè£ÔºàÁªëÂÆöÂ±Ç‰ºöÂ§ÑÁêÜÊ†ºÂºèËΩ¨Êç¢Ôºâ
        dx, dW, dR, dbx, dbr, dh = gru_ops.haste_gru_backward(
            time_steps=time_steps, batch_size=batch_size,
            input_size=input_size, hidden_size=hidden_size,
            W=W, R=R, bx=bx, br=br, x=input,
            dh_new=dh_new, h=h, v=v
        )

        # Ê¢ØÂ∫¶Ê†ºÂºèËΩ¨Êç¢: Haste (z,r,n) -> PyTorch (r,z,n)
        dW_pytorch = reorder_weights_haste_to_pytorch(dW.t()).contiguous()
        dR_pytorch = reorder_weights_haste_to_pytorch(dR.t()).contiguous()
        dbx_pytorch = reorder_weights_haste_to_pytorch(dbx).contiguous() if not ctx.bias_ih_is_none else None
        dbr_pytorch = reorder_weights_haste_to_pytorch(dbr).contiguous() if not ctx.bias_hh_is_none else None
        grad_h0 = None if ctx.h0_is_none else dh

        # ËøîÂõûÊ¢ØÂ∫¶ÔºàÂØπÂ∫î forward ÁöÑ 9 ‰∏™ÂèÇÊï∞Ôºâ
        return dx, dW_pytorch, dR_pytorch, dbx_pytorch, dbr_pytorch, grad_h0, None, None, None


# ============================================================
#                      QuantGRU Ê®°Âùó
# ============================================================

class QuantGRU(nn.Module):
    """
    ÊîØÊåÅÈáèÂåñÁöÑËá™ÂÆö‰πâ GRU ÂÆûÁé∞ÔºåÂÖºÂÆπ nn.GRU Êé•Âè£
    
    ÁâπÊÄß:
        - Âª∂ËøüÂàùÂßãÂåñ: CUDA handle Âú®È¶ñÊ¨°‰ΩøÁî®Êó∂ÂàùÂßãÂåñ
        - ÂèØÂ∫èÂàóÂåñ: ÊîØÊåÅ pickle/deepcopy
        - ÂèåÂêëÊîØÊåÅ: bidirectional=True Êó∂ËæìÂá∫Áª¥Â∫¶‰∏∫ 2*hidden_size
        - ONNX ÂØºÂá∫: export_mode=True Êó∂‰ΩøÁî®Á∫Ø PyTorch ÂÆûÁé∞

    ÈáèÂåñÊµÅÁ®ã:
        1. gru.load_bitwidth_config("config.json")  # ÂèØÈÄâ
        2. gru.calibrate(data1), gru.calibrate(data2), ...
        3. gru.finalize_calibration()
        4. gru.use_quantization = True
        5. output, h_n = gru(input)
    
    ONNX ÂØºÂá∫ÊµÅÁ®ã:
        1. gru.export_mode = True
        2. torch.onnx.export(model, ...)
        3. gru.export_mode = False  # ÊÅ¢Â§ç CUDA Ê®°Âºè
    
    È´òÁ∫ßÔºöÊåáÂÆöÂØºÂá∫Ê†ºÂºè:
        gru.export_format = 'float'      # ÊµÆÁÇπÔºàÈªòËÆ§Ôºå‰∏é Haste ‰∏ÄËá¥Ôºâ
        gru.export_format = 'qdq'        # QDQ ‰º™ÈáèÂåñÔºàÈáèÂåñÊ®°ÂûãÊé®ËçêÔºâ

    Args:
        input_size: ËæìÂÖ•ÁâπÂæÅÁª¥Â∫¶
        hidden_size: ÈöêËóèÁä∂ÊÄÅÁª¥Â∫¶
        num_layers: Â±ÇÊï∞Ôºà‰ªÖÊîØÊåÅ 1Ôºâ
        bias: ÊòØÂê¶‰ΩøÁî®ÂÅèÁΩÆ
        batch_first: True Êó∂ËæìÂÖ•‰∏∫ [B, T, I]
        dropout: ÊöÇ‰∏çÊîØÊåÅ
        bidirectional: ÊòØÂê¶ÂèåÂêë
    
    Attributes:
        use_quantization: ÈáèÂåñÂºÄÂÖ≥ÔºàÈªòËÆ§ FalseÔºâ
        calibration_method: Ê†°ÂáÜÊñπÊ≥ï ('minmax' Êàñ 'histogram')
        export_mode: ONNX ÂØºÂá∫Ê®°ÂºèÔºàÈªòËÆ§ FalseÔºå‰ΩøÁî® CUDAÔºõTrue Êó∂‰ΩøÁî®Á∫Ø PyTorchÔºâ
    """

    def __init__(
            self,
            input_size: int,
            hidden_size: int,
            num_layers: int = 1,
            bias: bool = True,
            batch_first: bool = False,
            dropout: float = 0.0,
            bidirectional: bool = False,
            use_quantization: bool = False,
    ):
        super(QuantGRU, self).__init__()

        if num_layers != 1:
            raise NotImplementedError("‰ªÖÊîØÊåÅ num_layers=1")
        if dropout > 0:
            raise NotImplementedError("ÊöÇ‰∏çÊîØÊåÅ dropout")

        # Âü∫Êú¨ÈÖçÁΩÆ
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.use_quantization = use_quantization
        self.num_directions = 2 if bidirectional else 1

        # ONNX ÂØºÂá∫ÂºÄÂÖ≥ÔºöTrue Êó∂‰ΩøÁî®Á∫Ø PyTorch ÂÆûÁé∞ÔºåÂèØË¢´ ONNX ËøΩË∏™
        self.export_mode = False
        # ÂØºÂá∫Ê†ºÂºèÔºàÈ´òÁ∫ßÈÄâÈ°πÔºå‰ªÖÂú® export_mode=True Êó∂ÊúâÊïàÔºâ
        # 'float': ÊµÆÁÇπÔºàÈªòËÆ§Ôºå‰∏é Haste GRU Ë°å‰∏∫‰∏ÄËá¥Ôºâ
        # 'qdq': QDQ ‰º™ÈáèÂåñÔºàÊé®ËçêÁî®‰∫éÈáèÂåñÊ®°ÂûãÔºâ
        self._export_format = 'float'

        # ÊùÉÈáçÂèÇÊï∞ÔºàÂëΩÂêç‰∏é nn.GRU ‰∏ÄËá¥Ôºâ
        self.weight_ih_l0 = nn.Parameter(torch.empty(3 * hidden_size, input_size))
        self.weight_hh_l0 = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
        if bias:
            self.bias_ih_l0 = nn.Parameter(torch.empty(3 * hidden_size))
            self.bias_hh_l0 = nn.Parameter(torch.empty(3 * hidden_size))
        else:
            self.register_parameter('bias_ih_l0', None)
            self.register_parameter('bias_hh_l0', None)

        # ÂèçÂêëÊùÉÈáçÔºàÂèåÂêëÊó∂Ôºâ
        if bidirectional:
            self.weight_ih_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size, input_size))
            self.weight_hh_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
            if bias:
                self.bias_ih_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size))
                self.bias_hh_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size))
            else:
                self.register_parameter('bias_ih_l0_reverse', None)
                self.register_parameter('bias_hh_l0_reverse', None)

        self.reset_parameters()

        # ÈáèÂåñÁä∂ÊÄÅÔºàÂª∂ËøüÂàõÂª∫Ôºâ
        self.quant_ranges = None  # calibrate() Êó∂ÂàõÂª∫
        self.quant_params = None  # finalize_calibration() Êó∂ÂàõÂª∫
        if bidirectional:
            self.quant_ranges_reverse = None
            self.quant_params_reverse = None

        self._calibration_dirty = False  # Ê†°ÂáÜÊï∞ÊçÆÊõ¥Êñ∞Ê†áÂøó
        self._bitwidth_config_dict = None  # ‰ΩçÂÆΩÈÖçÁΩÆÔºàPython Â≠óÂÖ∏ÔºåÂèØÂ∫èÂàóÂåñÔºâ
        self._cublas_initialized = False  # CUDA Âª∂ËøüÂàùÂßãÂåñÊ†áÂøó

        # Ê†°ÂáÜÊñπÊ≥ï: 'minmax'ÔºàÂø´ÈÄüÔºâÊàñ 'histogram'ÔºàAIMET È£éÊ†ºÔºåÈ´òÁ≤æÂ∫¶Ôºâ
        self.calibration_method = 'histogram'

        # Áõ¥ÊñπÂõæÊî∂ÈõÜÂô®Ôºàhistogram ÊñπÊ≥ï‰ΩøÁî®Ôºâ
        self.hist_collectors = None
        if bidirectional:
            self.hist_collectors_reverse = None

    def reset_parameters(self):
        """ÊùÉÈáçÂàùÂßãÂåñÔºà‰∏é nn.GRU Áõ∏ÂêåÁöÑÂùáÂåÄÂàÜÂ∏ÉÔºâ"""
        stdv = 1.0 / (self.hidden_size ** 0.5)
        for param in self.parameters():
            nn.init.uniform_(param, -stdv, stdv)

    # -------------------- ÂÜÖÈÉ®ÊñπÊ≥ï --------------------

    def _ensure_cublas_initialized(self):
        """Âª∂ËøüÂàùÂßãÂåñ cublas handle"""
        if not self._cublas_initialized:
            gru_ops.init_gru_cublas()
            self._cublas_initialized = True

    def _load_bitwidth_config_to_dict(self, config_file: str):
        """‰ªé JSON Êñá‰ª∂Âä†ËΩΩÈÖçÁΩÆÂà∞ÂÜÖÈÉ®Â≠óÂÖ∏"""
        if self._bitwidth_config_dict is None:
            self._bitwidth_config_dict = {}

        with open(config_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ËØªÂèñ GRU_config ËäÇÁÇπ‰∏ãÁöÑÈÖçÁΩÆ
        gru_config = data.get('GRU_config', {})

        # ËØªÂèñÂÖ®Â±ÄÈÖçÁΩÆ
        default_config = gru_config.get('default_config', {})
        if 'disable_quantization' in default_config:
            # disable_quantization=true Ë°®Á§∫Á¶ÅÁî®ÈáèÂåñÔºåÊâÄ‰ª• use_quantization ÂèñÂèç
            self.use_quantization = not default_config['disable_quantization']

        op_config = gru_config.get('operator_config', {})

        # Â≠óÊÆµÊò†Â∞Ñ: JSON key -> (‰ΩçÂÆΩÂ±ûÊÄßÂêç, ÂØπÁß∞ÈáèÂåñÂ±ûÊÄßÂêç)
        field_map = {
            "input.x": ("x_", "x_symmetric_"),
            "input.h": ("h_", "h_symmetric_"),
            "weight.W": ("W_", "W_symmetric_"),
            "weight.R": ("R_", "R_symmetric_"),
            "weight.bx": ("bx_", "bx_symmetric_"),
            "weight.br": ("br_", "br_symmetric_"),
            "matmul.Wx": ("Wx_", "Wx_symmetric_"),
            "matmul.Rh": ("Rh_", "Rh_symmetric_"),
            "gate.z_pre": ("z_pre_", "z_pre_symmetric_"),
            "gate.z_out": ("z_out_", "z_out_symmetric_"),
            "gate.r_pre": ("r_pre_", "r_pre_symmetric_"),
            "gate.r_out": ("r_out_", "r_out_symmetric_"),
            "gate.g_pre": ("g_pre_", "g_pre_symmetric_"),
            "gate.g_out": ("g_out_", "g_out_symmetric_"),
            "op.Rh_add_br": ("Rh_add_br_", "Rh_add_br_symmetric_"),
            "op.rRh": ("rRh_", "rRh_symmetric_"),
            "op.old_contrib": ("old_contrib_", "old_contrib_symmetric_"),
            "op.new_contrib": ("new_contrib_", "new_contrib_symmetric_"),
        }

        for json_key, (bw_attr, sym_attr) in field_map.items():
            if json_key in op_config:
                op_cfg = op_config[json_key]
                self._bitwidth_config_dict[bw_attr] = op_cfg.get('bitwidth', 8)
                self._bitwidth_config_dict[sym_attr] = op_cfg.get('is_symmetric', True)

    def _get_cpp_bitwidth_config(self) -> gru_ops.OperatorQuantConfig:
        """‰ªé Python Â≠óÂÖ∏ÂàõÂª∫ C++ OperatorQuantConfig ÂØπË±°"""
        config = gru_ops.OperatorQuantConfig()
        if self._bitwidth_config_dict is not None:
            for attr, value in self._bitwidth_config_dict.items():
                setattr(config, attr, value)
        return config

    def _convert_weights_to_haste_format(self, device: torch.device, reverse: bool = False):
        """
        Â∞ÜÊùÉÈáçËΩ¨Êç¢‰∏∫ Haste Ê†ºÂºè (z,r,n)
        
        Returns:
            W, R, bx, br: Haste Ê†ºÂºèÁöÑÊùÉÈáçÂíåÂÅèÁΩÆ
        """
        if reverse and self.bidirectional:
            weight_ih = ensure_cuda_float32(self.weight_ih_l0_reverse, device)
            weight_hh = ensure_cuda_float32(self.weight_hh_l0_reverse, device)
        else:
            weight_ih = ensure_cuda_float32(self.weight_ih_l0, device)
            weight_hh = ensure_cuda_float32(self.weight_hh_l0, device)

        W = reorder_weights_pytorch_to_haste(weight_ih).t().contiguous()
        R = reorder_weights_pytorch_to_haste(weight_hh).t().contiguous()

        if self.bias:
            if reverse and self.bidirectional:
                bias_ih = ensure_cuda_float32(self.bias_ih_l0_reverse, device)
                bias_hh = ensure_cuda_float32(self.bias_hh_l0_reverse, device)
            else:
                bias_ih = ensure_cuda_float32(self.bias_ih_l0, device)
                bias_hh = ensure_cuda_float32(self.bias_hh_l0, device)
            bx = reorder_weights_pytorch_to_haste(bias_ih).contiguous()
            br = reorder_weights_pytorch_to_haste(bias_hh).contiguous()
        else:
            hidden_size = self.hidden_size
            bx = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)
            br = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)

        return W, R, bx, br

    def _accumulate_calibration_ranges(self, calibration_data: torch.Tensor):
        """Á¥ØÁßØÊ†°ÂáÜËåÉÂõ¥"""
        self._ensure_cublas_initialized()

        device = calibration_data.device if calibration_data.is_cuda else torch.device('cuda')
        if not calibration_data.is_cuda:
            calibration_data = calibration_data.to(device)

        # Á°Æ‰øùÊ®°ÂûãÂú® GPU ‰∏ä
        if not next(self.parameters()).is_cuda:
            for param in self.parameters():
                param.data = param.data.to(device)
            for buffer in self.buffers():
                buffer.data = buffer.data.to(device)

        if self.batch_first:
            calibration_data = calibration_data.transpose(0, 1).contiguous()

        time_steps, batch_size, input_size = calibration_data.shape
        hidden_size = self.hidden_size

        # ÂâçÂêëÊ†°ÂáÜ
        W, R, bx, br = self._convert_weights_to_haste_format(device, reverse=False)
        if self.calibration_method == 'histogram':
            if self.hist_collectors is None:
                self.hist_collectors = gru_ops.GRUHistogramCollectors(hidden_size, num_bins=2048)
            gru_ops.calibrate_gru_histograms(
                time_steps=time_steps, batch_size=batch_size, input_size=input_size, hidden_size=hidden_size,
                W=W, R=R, bx=bx, br=br, x=calibration_data, hist_collectors=self.hist_collectors)
        else:
            if self.quant_ranges is None:
                self.quant_ranges = gru_ops.GRUQuantizationRanges(hidden_size)
            gru_ops.calibrate_gru_ranges(
                time_steps=time_steps, batch_size=batch_size, input_size=input_size, hidden_size=hidden_size,
                W=W, R=R, bx=bx, br=br, x=calibration_data, quant_ranges=self.quant_ranges)

        # ÂèçÂêëÊ†°ÂáÜÔºàÂèåÂêëÊó∂Ôºâ
        if self.bidirectional:
            W_rev, R_rev, bx_rev, br_rev = self._convert_weights_to_haste_format(device, reverse=True)
            calibration_data_reversed = calibration_data.flip(0).contiguous()

            if self.calibration_method == 'histogram':
                if self.hist_collectors_reverse is None:
                    self.hist_collectors_reverse = gru_ops.GRUHistogramCollectors(hidden_size, num_bins=2048)
                gru_ops.calibrate_gru_histograms(
                    time_steps=time_steps, batch_size=batch_size, input_size=input_size, hidden_size=hidden_size,
                    W=W_rev, R=R_rev, bx=bx_rev, br=br_rev, x=calibration_data_reversed,
                    hist_collectors=self.hist_collectors_reverse)
            else:
                if self.quant_ranges_reverse is None:
                    self.quant_ranges_reverse = gru_ops.GRUQuantizationRanges(hidden_size)
                gru_ops.calibrate_gru_ranges(
                    time_steps=time_steps, batch_size=batch_size, input_size=input_size, hidden_size=hidden_size,
                    W=W_rev, R=R_rev, bx=bx_rev, br=br_rev, x=calibration_data_reversed,
                    quant_ranges=self.quant_ranges_reverse)

        # Á°Æ‰øùÊùÉÈáçËøûÁª≠
        self.weight_ih_l0.data = self.weight_ih_l0.data.contiguous()
        self.weight_hh_l0.data = self.weight_hh_l0.data.contiguous()
        if self.bias:
            self.bias_ih_l0.data = self.bias_ih_l0.data.contiguous()
            self.bias_hh_l0.data = self.bias_hh_l0.data.contiguous()
        if self.bidirectional:
            self.weight_ih_l0_reverse.data = self.weight_ih_l0_reverse.data.contiguous()
            self.weight_hh_l0_reverse.data = self.weight_hh_l0_reverse.data.contiguous()
            if self.bias:
                self.bias_ih_l0_reverse.data = self.bias_ih_l0_reverse.data.contiguous()
                self.bias_hh_l0_reverse.data = self.bias_hh_l0_reverse.data.contiguous()

    # -------------------- ÂÖ¨ÂºÄÊé•Âè£ --------------------

    def load_bitwidth_config(self, config_file: str, verbose: bool = False):
        """‰ªé JSON Êñá‰ª∂Âä†ËΩΩ‰ΩçÂÆΩÈÖçÁΩÆ"""
        self._load_bitwidth_config_to_dict(config_file)
        if verbose:
            cpp_config = self._get_cpp_bitwidth_config()
            apply_bitwidth_config(cpp_config, config_file, verbose=True)
            print(f"  [ÂÖ®Â±Ä]  use_quantization: {self.use_quantization}")

    def set_all_bitwidth(self, bitwidth: int = 8, is_symmetric: bool = True, verbose: bool = False):
        """
        ËÆæÁΩÆÊâÄÊúâÁÆóÂ≠êÁªü‰∏ÄÁöÑ‰ΩçÂÆΩÂíåÂØπÁß∞ÈáèÂåñÈÖçÁΩÆ
        
        Args:
            bitwidth: ‰ΩçÂÆΩ (8/16/32)
            is_symmetric: ÊòØÂê¶ÂØπÁß∞ÈáèÂåñÔºà‰ªÖÂØπÊøÄÊ¥ªÂÄºÁîüÊïàÔºåÊùÉÈáç/ÂÅèÁΩÆÂßãÁªàÂØπÁß∞Ôºâ
            verbose: ÊòØÂê¶ÊâìÂç∞‰ø°ÊÅØ
        """
        if bitwidth not in (8, 16, 32):
            raise ValueError(f"bitwidth must be 8, 16 or 32, got {bitwidth}")

        # ÂàùÂßãÂåñÈÖçÁΩÆÂ≠óÂÖ∏
        if self._bitwidth_config_dict is None:
            self._bitwidth_config_dict = {}

        # ‰ΩçÂÆΩÂ±ûÊÄßÂàóË°®
        bitwidth_attrs = [
            'x_', 'h_', 'W_', 'R_', 'bx_', 'br_', 'Wx_', 'Rh_',
            'z_pre_', 'z_out_', 'r_pre_', 'r_out_', 'g_pre_', 'g_out_',
            'Rh_add_br_', 'rRh_', 'old_contrib_', 'new_contrib_'
        ]

        # ÊùÉÈáç/ÂÅèÁΩÆÂØπÁß∞ÈáèÂåñÂ±ûÊÄßÔºàÂßãÁªà‰∏∫ TrueÔºå‰∏çÂèØÈÖçÁΩÆÔºâ
        weight_symmetric_attrs = [
            'W_symmetric_', 'R_symmetric_', 'bx_symmetric_', 'br_symmetric_'
        ]

        # ÊøÄÊ¥ªÂÄºÂØπÁß∞ÈáèÂåñÂ±ûÊÄßÔºàÂèØÈÖçÁΩÆÔºâ
        activation_symmetric_attrs = [
            'x_symmetric_', 'h_symmetric_', 'Wx_symmetric_', 'Rh_symmetric_',
            'z_pre_symmetric_', 'z_out_symmetric_', 'r_pre_symmetric_', 'r_out_symmetric_',
            'g_pre_symmetric_', 'g_out_symmetric_', 'Rh_add_br_symmetric_', 'rRh_symmetric_',
            'old_contrib_symmetric_', 'new_contrib_symmetric_'
        ]

        # ËÆæÁΩÆÊâÄÊúâ‰ΩçÂÆΩ
        for attr in bitwidth_attrs:
            self._bitwidth_config_dict[attr] = bitwidth

        # ÊùÉÈáç/ÂÅèÁΩÆÂßãÁªà‰ΩøÁî®ÂØπÁß∞ÈáèÂåñ
        for attr in weight_symmetric_attrs:
            self._bitwidth_config_dict[attr] = True

        # ÊøÄÊ¥ªÂÄºÂØπÁß∞ÈáèÂåñÈÖçÁΩÆÁî±ÂèÇÊï∞ÊéßÂà∂
        for attr in activation_symmetric_attrs:
            self._bitwidth_config_dict[attr] = is_symmetric

        if verbose:
            sym_str = "ÂØπÁß∞" if is_symmetric else "ÈùûÂØπÁß∞"
            print(f"\n[QuantGRU] ËÆæÁΩÆÊâÄÊúâÁÆóÂ≠ê: {bitwidth}bit, ÊøÄÊ¥ªÂÄº{sym_str}ÈáèÂåñ, ÊùÉÈáç/ÂÅèÁΩÆÂØπÁß∞ÈáèÂåñ")

    def is_calibrated(self) -> bool:
        """Ê£ÄÊü•ÊòØÂê¶Â∑≤ÂÆåÊàêÊ†°ÂáÜ"""
        if self.bidirectional:
            return self.quant_params is not None and self.quant_params_reverse is not None
        return self.quant_params is not None

    def calibrate(self, calibration_data: torch.Tensor):
        """
        Á¥ØÁßØÊ†°ÂáÜÊï∞ÊçÆ
        
        Args:
            calibration_data: [T, B, I] Êàñ [B, T, I] (batch_first) ÁöÑÊï∞ÊçÆ
        
        Note:
            ÊîØÊåÅÂ¢ûÈáèÊ†°ÂáÜÔºåÂÆåÊàêÂêéÈúÄË∞ÉÁî® finalize_calibration()
        """
        self._accumulate_calibration_ranges(calibration_data)
        self._calibration_dirty = True

    def finalize_calibration(self, verbose: bool = False):
        """
        ÂÆåÊàêÊ†°ÂáÜÔºåËÆ°ÁÆóÈáèÂåñÂèÇÊï∞Âπ∂ÂàùÂßãÂåñ LUT
        
        Args:
            verbose: ÊòØÂê¶ÊâìÂç∞Ê†°ÂáÜ‰ø°ÊÅØ
            
        Raises:
            RuntimeError: Êú™Ë∞ÉÁî®Ëøá calibrate()
        """
        use_histogram = (self.calibration_method == 'histogram')

        # Ê£ÄÊü•Ê†°ÂáÜÊï∞ÊçÆ
        if use_histogram:
            if self.hist_collectors is None or not self.hist_collectors.is_valid():
                raise RuntimeError("Êú™Êî∂ÈõÜÁõ¥ÊñπÂõæÊï∞ÊçÆÔºåËØ∑ÂÖàË∞ÉÁî® calibrate()")
        else:
            if self.quant_ranges is None:
                raise RuntimeError("Êú™Êî∂ÈõÜÊ†°ÂáÜÊï∞ÊçÆÔºåËØ∑ÂÖàË∞ÉÁî® calibrate()")

        cpp_config = self._get_cpp_bitwidth_config()

        if verbose:
            method_name = {'minmax': 'MINMAX', 'histogram': 'HISTOGRAM'}.get(
                self.calibration_method, self.calibration_method.upper())
            print(f"\n[QuantGRU] Ê†°ÂáÜÊñπÊ≥ï: {method_name}")

        # ÂâçÂêëÊñπÂêë
        if use_histogram:
            self.quant_params = gru_ops.calculate_gru_quantitative_parameters_from_histograms(
                hist_collectors=self.hist_collectors, bitwidth_config=cpp_config, verbose=verbose)
        else:
            self.quant_params = gru_ops.calculate_gru_quantitative_parameters(
                quant_ranges=self.quant_ranges, bitwidth_config=cpp_config)
        gru_ops.initialize_quantization_lut(quant_params=self.quant_params)

        # ÂèçÂêëÊñπÂêëÔºàÂèåÂêëÊó∂Ôºâ
        if self.bidirectional:
            if use_histogram:
                if self.hist_collectors_reverse is None or not self.hist_collectors_reverse.is_valid():
                    raise RuntimeError("ÂèåÂêë GRU ÂèçÂêëÁõ¥ÊñπÂõæÊï∞ÊçÆÂºÇÂ∏∏")
                self.quant_params_reverse = gru_ops.calculate_gru_quantitative_parameters_from_histograms(
                    hist_collectors=self.hist_collectors_reverse, bitwidth_config=cpp_config, verbose=verbose)
            else:
                if self.quant_ranges_reverse is None:
                    raise RuntimeError("ÂèåÂêë GRU ÂèçÂêëÊ†°ÂáÜÊï∞ÊçÆÂºÇÂ∏∏")
                self.quant_params_reverse = gru_ops.calculate_gru_quantitative_parameters(
                    quant_ranges=self.quant_ranges_reverse, bitwidth_config=cpp_config)
            # Ê†áËÆ∞‰∏∫ÂèçÂêëÊñπÂêëÔºåÂàùÂßãÂåñÂèçÂêë LUT
            self.quant_params_reverse.is_reverse_ = True
            gru_ops.initialize_quantization_lut(quant_params=self.quant_params_reverse)

        self._calibration_dirty = False

    def reset_calibration(self):
        """ÈáçÁΩÆÊ†°ÂáÜÁä∂ÊÄÅÔºåÊ∏ÖÈô§ÊâÄÊúâÁ¥ØÁßØÁöÑËåÉÂõ¥ÂíåÂèÇÊï∞"""
        self.quant_ranges = None
        self.quant_params = None
        self.hist_collectors = None
        self._calibration_dirty = False
        if self.bidirectional:
            self.quant_ranges_reverse = None
            self.quant_params_reverse = None
            self.hist_collectors_reverse = None

    # -------------------- ONNX ÂØºÂá∫Ê®°ÂºèÔºöÁ∫Ø PyTorch ÂÆûÁé∞ --------------------

    def _get_quant_param(self, param_name: str, quant_params) -> Tuple[int, int]:
        """Ëé∑ÂèñÈáèÂåñÂèÇÊï∞ (exp2_inv, zero_point)"""
        exp2_inv = getattr(quant_params, f'exp2_inv_{param_name}_', 0)
        zp = getattr(quant_params, f'zp_{param_name}_', 0)
        return exp2_inv, zp

    def _get_bitwidth(self, op_name: str) -> int:
        """Ëé∑ÂèñÊåáÂÆöÊìç‰ΩúÁöÑ‰ΩçÂÆΩ"""
        if self._bitwidth_config_dict is not None:
            return self._bitwidth_config_dict.get(f'{op_name}_', 8)
        return 8

    def _get_symmetric(self, op_name: str) -> bool:
        """Ëé∑ÂèñÊåáÂÆöÊìç‰ΩúÊòØÂê¶ÂØπÁß∞ÈáèÂåñ"""
        if self._bitwidth_config_dict is not None:
            return self._bitwidth_config_dict.get(f'{op_name}_symmetric_', True)
        return True

    @property
    def export_format(self) -> str:
        """
        Ëé∑ÂèñÂØºÂá∫Ê†ºÂºèÔºàÈ´òÁ∫ßÈÄâÈ°πÔºå‰ªÖÂú® export_mode=True Êó∂ÊúâÊïàÔºâ
        
        Returns:
            'float': ÊµÆÁÇπÊ†ºÂºèÔºàÈªòËÆ§Ôºå‰∏é Haste GRU Ë°å‰∏∫‰∏ÄËá¥Ôºâ
            'qdq': QDQ ‰º™ÈáèÂåñÊ†ºÂºèÔºàÊé®ËçêÁî®‰∫éÈáèÂåñÊ®°Âûã ONNX ÂØºÂá∫Ôºâ
        """
        return self._export_format
    
    @export_format.setter
    def export_format(self, mode: str):
        """
        ËÆæÁΩÆÂØºÂá∫Ê†ºÂºèÔºàÈ´òÁ∫ßÁî®Ê≥ïÔºåÂ§ßÂ§öÊï∞Áî®Êà∑‰∏çÈúÄË¶Å‰øÆÊîπÔºâ
        
        Args:
            mode: 'qdq' | 'float'
        """
        valid_modes = ('qdq', 'float')
        if mode not in valid_modes:
            raise ValueError(f"Invalid export_format: '{mode}'. Use one of {valid_modes}")
        self._export_format = mode

    def _forward_python_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
            quant_params
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Á∫Ø PyTorch ÂÆûÁé∞ÁöÑÂçïÂêë GRU ÂâçÂêë‰º†Êí≠ÔºàÂèØË¢´ ONNX ËøΩË∏™Ôºâ

        GRU ÂÖ¨ÂºèÔºàHaste Ê†ºÂºèÔºåÈó®È°∫Â∫è‰∏∫ z, r, gÔºâÔºö
            z = sigmoid(W_z @ x + R_z @ h + bx_z + br_z)  # update gate
            r = sigmoid(W_r @ x + R_r @ h + bx_r + br_r)  # reset gate
            g = tanh(W_g @ x + r * (R_g @ h + br_g) + bx_g)  # candidate gate
            h' = z * h + (1 - z) * g

        ÈáèÂåñÊ®°Âºè‰∏ãÊ†πÊçÆ ONNX ÂØºÂá∫Ê®°ÂºèÈÄâÊã©ÂÆûÁé∞Ôºö
            - 'qdq': QDQ Ê†ºÂºèÔºå‰ΩøÁî®Ê†áÂáÜÁÆóÂ≠ê + ‰º™ÈáèÂåñ
            - 'float': Ê†áÂáÜÊµÆÁÇπËÆ°ÁÆóÔºàHaste Ê†ºÂºèÔºâ

        Args:
            input: [T, B, I] ËæìÂÖ•Â∫èÂàó
            h0: [B, H] ÂàùÂßãÈöêËóèÁä∂ÊÄÅ Êàñ None
            weight_ih: [3*H, I] ËæìÂÖ•ÊùÉÈáç (PyTorch r,z,n Ê†ºÂºèÔºåÂÜÖÈÉ®Ëá™Âä®ËΩ¨Êç¢)
            weight_hh: [3*H, H] Âæ™ÁéØÊùÉÈáç (PyTorch r,z,n Ê†ºÂºèÔºåÂÜÖÈÉ®Ëá™Âä®ËΩ¨Êç¢)
            bias_ih: [3*H] ËæìÂÖ•ÂÅèÁΩÆ Êàñ None (PyTorch Ê†ºÂºèÔºåÂÜÖÈÉ®Ëá™Âä®ËΩ¨Êç¢)
            bias_hh: [3*H] Âæ™ÁéØÂÅèÁΩÆ Êàñ None (PyTorch Ê†ºÂºèÔºåÂÜÖÈÉ®Ëá™Âä®ËΩ¨Êç¢)
            quant_params: ÈáèÂåñÂèÇÊï∞ÔºàÊù•Ëá™ finalize_calibrationÔºâ

        Returns:
            output: [T, B, H] ËæìÂá∫Â∫èÂàó
            h_n: [1, B, H] ÊúÄÁªàÈöêËóèÁä∂ÊÄÅ
        """
        # Ê†πÊçÆ export_format ÈÄâÊã©ÂÆûÁé∞
        if self._export_format == 'float':
            # ÊµÆÁÇπÊ®°ÂºèÔºöÁõ¥Êé•‰ΩøÁî®ÊµÆÁÇπÂÆûÁé∞
            return self._forward_python_float_single_direction(
                input, h0, weight_ih, weight_hh, bias_ih, bias_hh
            )
        
        # qdq ÈúÄË¶ÅÈáèÂåñÂèÇÊï∞
        if quant_params is None:
            raise RuntimeError(
                f"export_format='{self._export_format}' ÈúÄË¶ÅÈáèÂåñÂèÇÊï∞Ôºå"
                f"ËØ∑ÂÖàË∞ÉÁî® calibrate() Âíå finalize_calibration()"
            )
        
        if self._export_format == 'qdq':
            return self._forward_onnx_qdq_single_direction(
                input, h0, weight_ih, weight_hh, bias_ih, bias_hh, quant_params
            )

    def _forward_python_float_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ÊµÆÁÇπÂÆûÁé∞ÁöÑÂçïÂêë GRU ÂâçÂêë‰º†Êí≠ÔºàHaste Ê†ºÂºèÔºâ
        
        ‰∏é HasteGRU CUDA ÊµÆÁÇπÊé®ÁêÜË°å‰∏∫‰∏ÄËá¥
        Èó®ÊéßÈ°∫Â∫èÔºöHaste Ê†ºÂºè (z, r, g)
        
        ÂÖ¨ÂºèÔºà‰∏é gru_forward_gpu.cu ‰∏ÄËá¥ÔºâÔºö
            z = sigmoid(Wx_z + Rh_z + bx_z + br_z)
            r = sigmoid(Wx_r + Rh_r + bx_r + br_r)
            g = tanh(Wx_g + r * (Rh_g + br_g) + bx_g)
            h_new = z * h_old + (1 - z) * g
        
        Args:
            input: [T, B, I] ËæìÂÖ•Â∫èÂàó
            h0: [B, H] ÂàùÂßãÈöêËóèÁä∂ÊÄÅ Êàñ None
            weight_ih: [3*H, I] ËæìÂÖ•ÊùÉÈáç (PyTorch r,z,n Ê†ºÂºèÔºåÂÜÖÈÉ®ËΩ¨Êç¢)
            weight_hh: [3*H, H] Âæ™ÁéØÊùÉÈáç (PyTorch r,z,n Ê†ºÂºèÔºåÂÜÖÈÉ®ËΩ¨Êç¢)
            bias_ih: [3*H] ËæìÂÖ•ÂÅèÁΩÆ Êàñ None (PyTorch Ê†ºÂºèÔºåÂÜÖÈÉ®ËΩ¨Êç¢)
            bias_hh: [3*H] Âæ™ÁéØÂÅèÁΩÆ Êàñ None (PyTorch Ê†ºÂºèÔºåÂÜÖÈÉ®ËΩ¨Êç¢)
            
        Returns:
            output: [T, B, H] ËæìÂá∫Â∫èÂàó
            h_n: [1, B, H] ÊúÄÁªàÈöêËóèÁä∂ÊÄÅ
        """
        T, B, I = input.shape
        H = self.hidden_size
        device = input.device
        dtype = input.dtype

        # ÂàùÂßãÂåñÈöêËóèÁä∂ÊÄÅ
        if h0 is None:
            h = torch.zeros(B, H, device=device, dtype=dtype)
        else:
            h = h0

        # ÊùÉÈáçÊ†ºÂºèËΩ¨Êç¢ÔºöPyTorch (r,z,n) -> Haste (z,r,g)
        W = reorder_weights_pytorch_to_haste(weight_ih)  # [3*H, I]
        R = reorder_weights_pytorch_to_haste(weight_hh)  # [3*H, H]

        # Â§ÑÁêÜÂÅèÁΩÆÂπ∂ËΩ¨Êç¢Ê†ºÂºè
        if bias_ih is None:
            bx = torch.zeros(3 * H, device=device, dtype=dtype)
        else:
            bx = reorder_weights_pytorch_to_haste(bias_ih)
        if bias_hh is None:
            br = torch.zeros(3 * H, device=device, dtype=dtype)
        else:
            br = reorder_weights_pytorch_to_haste(bias_hh)

        # ========== Âæ™ÁéØÂ§ñ‰∏ÄÊ¨°ÊÄßËÆ°ÁÆó Wx GEMMÔºà‰∏é CUDA ‰∏ÄËá¥Ôºâ==========
        # input: [T, B, I] -> x_flat: [T*B, I]
        # W: [3*H, I] -> W.t(): [I, 3*H]
        # Wx_all: [T*B, 3*H] -> reshape: [T, B, 3*H]
        x_flat = input.reshape(T * B, I)
        Wx_all = torch.mm(x_flat, W.t())  # [T*B, 3*H]
        Wx_all = Wx_all.reshape(T, B, 3 * H)  # [T, B, 3*H]

        # È¢ÑÂàÜÂâ≤ÂÅèÁΩÆÔºàÂæ™ÁéØÂ§ñÂÆåÊàêÔºâ
        bx_z, bx_r, bx_g = bx.chunk(3)
        br_z, br_r, br_g = br.chunk(3)

        outputs = []

        for t in range(T):
            # Ëé∑ÂèñÂΩìÂâçÊó∂Èó¥Ê≠•ÁöÑ WxÔºàÂ∑≤Âú®Âæ™ÁéØÂ§ñËÆ°ÁÆóÂ•ΩÔºâ
            Wx = Wx_all[t]  # [B, 3*H]
            
            # Rh = h @ R.T, shape [B, 3H]Ôºà‰æùËµñ‰∏ä‰∏ÄÊ≠•ÁöÑ hÔºåÂøÖÈ°ªÂú®Âæ™ÁéØÂÜÖÔºâ
            Rh = torch.mm(h, R.t())

            # ÂàÜÂâ≤Èó®ÊéßÔºàHaste Ê†ºÂºèÔºöz, r, gÔºâ
            Wx_z, Wx_r, Wx_g = Wx.chunk(3, dim=1)
            Rh_z, Rh_r, Rh_g = Rh.chunk(3, dim=1)

            # Update gate (z)
            z = torch.sigmoid(Wx_z + Rh_z + bx_z + br_z)

            # Reset gate (r)
            r = torch.sigmoid(Wx_r + Rh_r + bx_r + br_r)

            # Candidate gate (g): r Âè™‰πò‰ª• (Rh_g + br_g)
            Rh_add_br_g = Rh_g + br_g
            g = torch.tanh(Wx_g + r * Rh_add_br_g + bx_g)

            # Êñ∞ÈöêËóèÁä∂ÊÄÅ: h_new = z * h_old + (1 - z) * g
            h = z * h + (1 - z) * g

            outputs.append(h)

        # Â†ÜÂè†ËæìÂá∫: [T, B, H]
        output = torch.stack(outputs, dim=0)
        h_n = h.unsqueeze(0)  # [1, B, H]

        return output, h_n

    # -------------------- ONNX ÂØºÂá∫ÁâàÊú¨ÔºàQDQ Ê†ºÂºèÔºâ--------------------
    
    def _forward_onnx_qdq_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
            quant_params
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Áî®‰∫é ONNX ÂØºÂá∫ÁöÑ QDQ Ê†ºÂºèÂâçÂêë‰º†Êí≠
        
        ‰ΩøÁî®‰º™ÈáèÂåñÔºàFake QuantizeÔºâÂú®ÂÖ≥ÈîÆÁÇπÊèíÂÖ• Q/DQ Êìç‰ΩúÔºå
        Êé®ÁêÜÂºïÊìé‰ºöËØÜÂà´ QDQ Ê®°ÂºèÂπ∂Ëá™Âä®‰ºòÂåñ‰∏∫ÈáèÂåñÁÆóÂ≠ê„ÄÇ
        
        ËÆæËÆ°ÂéüÂàôÔºö
        ==========
        [‰∏é CUDA ‰∏ÄËá¥]
          - ÈáèÂåñÂèÇÊï∞Ôºàscale/zpÔºâÂÆåÂÖ®‰∏ÄËá¥
          - ËÆ°ÁÆóÂõæÁªìÊûÑ‰∏ÄËá¥ÔºàÈó®È°∫Â∫è„ÄÅËÆ°ÁÆóÈ°∫Â∫èÔºâ
          - ÊùÉÈáç/ÂÅèÁΩÆÁöÑ per-channel ÈáèÂåñÂèÇÊï∞‰∏ÄËá¥
          
        [ONNX ÂÖºÂÆπ - ‰∏é CUDA ÂÆûÁé∞‰∏çÂêå]
          - GEMM: ‰ΩøÁî®Ê†áÂáÜ torch.mmÔºàÊé®ÁêÜÂºïÊìé‰ºöÁî® MatMulIntegerÔºâ
          - sigmoid/tanh: ‰ΩøÁî®Ê†áÂáÜ torch.sigmoid/tanhÔºàÊé®ÁêÜÂºïÊìé‰ºö‰ºòÂåñÔºâ
          - rescale: ÈÄöËøá QDQ ÂÆûÁé∞Ôºà‰∏çÁî®ÊòæÂºè rshift_roundÔºâ
        
        Args:
            input: [T, B, I] ËæìÂÖ•Â∫èÂàó
            h0: [B, H] ÂàùÂßãÈöêËóèÁä∂ÊÄÅ Êàñ None
            weight_ih: [3*H, I] ËæìÂÖ•ÊùÉÈáç
            weight_hh: [3*H, H] Âæ™ÁéØÊùÉÈáç
            bias_ih: [3*H] ËæìÂÖ•ÂÅèÁΩÆ Êàñ None
            bias_hh: [3*H] Âæ™ÁéØÂÅèÁΩÆ Êàñ None
            quant_params: ÈáèÂåñÂèÇÊï∞
            
        Returns:
            output: [T, B, H] ËæìÂá∫Â∫èÂàó
            h_n: [1, B, H] ÊúÄÁªàÈöêËóèÁä∂ÊÄÅ
        """
        T, B, I = input.shape
        H = self.hidden_size
        device = input.device
        dtype = input.dtype
        
        # ========== ÈáèÂåñÂèÇÊï∞ÊèêÂèñ ==========
        # [‰∏é CUDA ‰∏ÄËá¥] ‰ΩøÁî®Áõ∏ÂêåÁöÑÈáèÂåñÂèÇÊï∞
        exp2_x = quant_params.exp2_inv_x_
        zp_x = quant_params.zp_x_
        exp2_h = quant_params.exp2_inv_h_
        zp_h = quant_params.zp_h_
        exp2_Wx = quant_params.exp2_inv_Wx_
        zp_Wx = quant_params.zp_Wx_
        exp2_Rh = quant_params.exp2_inv_Rh_
        zp_Rh = quant_params.zp_Rh_
        
        # ÊøÄÊ¥ªÂáΩÊï∞ÈáèÂåñÂèÇÊï∞
        exp2_z_pre = quant_params.exp2_inv_z_pre_
        zp_z_pre = quant_params.zp_z_pre_
        exp2_z_out = quant_params.exp2_inv_z_out_
        zp_z_out = quant_params.zp_z_out_
        
        exp2_r_pre = quant_params.exp2_inv_r_pre_
        zp_r_pre = quant_params.zp_r_pre_
        exp2_r_out = quant_params.exp2_inv_r_out_
        zp_r_out = quant_params.zp_r_out_
        
        exp2_g_pre = quant_params.exp2_inv_g_pre_
        zp_g_pre = quant_params.zp_g_pre_
        exp2_g_out = quant_params.exp2_inv_g_out_
        zp_g_out = quant_params.zp_g_out_
        
        # per-channel ÈáèÂåñÂèÇÊï∞
        exp2_W = list(quant_params.exp2_inv_W_)
        exp2_R = list(quant_params.exp2_inv_R_)
        exp2_bx = list(quant_params.exp2_inv_bx_)
        exp2_br = list(quant_params.exp2_inv_br_)
        
        # ========== ÊùÉÈáçÈáçÊéíÂ∫è ==========
        # [‰∏é CUDA ‰∏ÄËá¥] PyTorch Ê†ºÂºè (r, z, n) -> Haste Ê†ºÂºè (z, r, n)
        W_reordered = reorder_weights_pytorch_to_haste(weight_ih)  # [3*H, I]
        R_reordered = reorder_weights_pytorch_to_haste(weight_hh)  # [3*H, H]
        
        if bias_ih is not None:
            bx_reordered = reorder_weights_pytorch_to_haste(bias_ih)  # [3*H]
        else:
            bx_reordered = torch.zeros(3 * H, device=device, dtype=dtype)
            
        if bias_hh is not None:
            br_reordered = reorder_weights_pytorch_to_haste(bias_hh)  # [3*H]
        else:
            br_reordered = torch.zeros(3 * H, device=device, dtype=dtype)
        
        # ========== ÊùÉÈáç‰º™ÈáèÂåñ ==========
        # [‰∏é CUDA ‰∏ÄËá¥] per-channel ÈáèÂåñ
        # [ONNX ÂÖºÂÆπ] ‰ΩøÁî® fake_quantize ‰øùÊåÅÊµÆÁÇπÊ†ºÂºè
        W_q = fake_quantize_per_channel(W_reordered.t(), exp2_W, zp=0,
                                        bitwidth=self._get_bitwidth('W'),
                                        symmetric=self._get_symmetric('W')).t()
        R_q = fake_quantize_per_channel(R_reordered.t(), exp2_R, zp=0,
                                        bitwidth=self._get_bitwidth('R'),
                                        symmetric=self._get_symmetric('R')).t()
        # ÂÅèÁΩÆ‰ΩøÁî®ÈÖçÁΩÆÁöÑ‰ΩçÂÆΩÔºàÊ≥®ÊÑèÔºöÂÅèÁΩÆÂßãÁªà‰ΩøÁî®ÂØπÁß∞ÈáèÂåñÔºâ
        bx_q = fake_quantize_per_channel(bx_reordered.unsqueeze(0), exp2_bx, zp=0,
                                         bitwidth=self._get_bitwidth('bx'),
                                         symmetric=self._get_symmetric('bx')).squeeze(0)
        br_q = fake_quantize_per_channel(br_reordered.unsqueeze(0), exp2_br, zp=0,
                                         bitwidth=self._get_bitwidth('br'),
                                         symmetric=self._get_symmetric('br')).squeeze(0)
        
        # ÂàÜÂâ≤ÂÅèÁΩÆÔºàHaste Ê†ºÂºèÔºöz, r, nÔºâ
        bx_z, bx_r, bx_n = bx_q.chunk(3)  # ÂêÑ [H]
        br_z, br_r, br_n = br_q.chunk(3)  # ÂêÑ [H]
        
        # ========== ÂàùÂßãÂåñÈöêËóèÁä∂ÊÄÅ ==========
        if h0 is None:
            h = torch.zeros(B, H, device=device, dtype=dtype)
        else:
            h = h0
        
        # [‰∏é CUDA ‰∏ÄËá¥] ÈáèÂåñÂàùÂßãÁä∂ÊÄÅ
        h = fake_quantize(h, exp2_h, zp_h, bitwidth=self._get_bitwidth('h'),
                          symmetric=self._get_symmetric('h'))
        
        # ========== ËæìÂÖ•‰º™ÈáèÂåñ ==========
        # [‰∏é CUDA ‰∏ÄËá¥] ÊâÄÊúâÊó∂Èó¥Ê≠•‰∏ÄËµ∑ÈáèÂåñ
        x_q = fake_quantize(input, exp2_x, zp_x, bitwidth=self._get_bitwidth('x'),
                            symmetric=self._get_symmetric('x'))
        
        # ========== Wx GEMMÔºàÂæ™ÁéØÂ§ñ‰∏ÄÊ¨°ÊÄßËÆ°ÁÆóÔºâ==========
        # [‰∏é CUDA ‰∏ÄËá¥] ËÆ°ÁÆóÈ°∫Â∫è‰∏ÄËá¥
        # [ONNX ÂÖºÂÆπ] ‰ΩøÁî®Ê†áÂáÜ matmulÔºåÊé®ÁêÜÂºïÊìé‰ºöÊõøÊç¢‰∏∫ MatMulInteger
        # x_q: [T, B, I], W_q: [3*H, I] -> Wx: [T, B, 3*H]
        Wx_all = torch.matmul(x_q, W_q.t())  # [T, B, 3*H]
        
        # [‰∏é CUDA ‰∏ÄËá¥] GEMM ËæìÂá∫ÈáèÂåñ
        Wx_all = fake_quantize(Wx_all, exp2_Wx, zp_Wx, bitwidth=self._get_bitwidth('Wx'),
                               symmetric=self._get_symmetric('Wx'))
        
        # È¢ÑÂàÜÈÖçËæìÂá∫Âº†ÈáèÔºàONNX ÂèãÂ•ΩÔºåÈÅøÂÖçÂä®ÊÄÅÂàóË°®Ôºâ
        outputs = torch.zeros(T, B, H, device=device, dtype=dtype)
        
        for t in range(T):
            Wx = Wx_all[t]  # [B, 3*H]
            
            # ========== Rh GEMM ==========
            # [‰∏é CUDA ‰∏ÄËá¥] ÊØè‰∏™Êó∂Èó¥Ê≠•ËÆ°ÁÆó Rh
            # [ONNX ÂÖºÂÆπ] ‰ΩøÁî®Ê†áÂáÜ matmul
            Rh = torch.mm(h, R_q.t())  # [B, 3*H]
            
            # [‰∏é CUDA ‰∏ÄËá¥] GEMM ËæìÂá∫ÈáèÂåñ
            Rh = fake_quantize(Rh, exp2_Rh, zp_Rh, bitwidth=self._get_bitwidth('Rh'),
                               symmetric=self._get_symmetric('Rh'))
            
            # ========== ÂàÜÂâ≤Èó®Êéß ==========
            # [‰∏é CUDA ‰∏ÄËá¥] Haste Ê†ºÂºè (z, r, n)
            Wx_z, Wx_r, Wx_n = Wx.chunk(3, dim=1)  # ÂêÑ [B, H]
            Rh_z, Rh_r, Rh_n = Rh.chunk(3, dim=1)  # ÂêÑ [B, H]
            
            # ========== z Èó®ÔºàUpdate GateÔºâ==========
            # [‰∏é CUDA ‰∏ÄËá¥] z = sigmoid(Wx_z + Rh_z + bx_z + br_z)
            z_pre = Wx_z + Rh_z + bx_z.unsqueeze(0) + br_z.unsqueeze(0)
            
            # [‰∏é CUDA ‰∏ÄËá¥] ÊøÄÊ¥ªÂâçÈáèÂåñ
            z_pre = fake_quantize(z_pre, exp2_z_pre, zp_z_pre,
                                  bitwidth=self._get_bitwidth('z_pre'),
                                  symmetric=self._get_symmetric('z_pre'))
            
            # [ONNX ÂÖºÂÆπ] ‰ΩøÁî®Ê†áÂáÜ sigmoidÔºàÊé®ÁêÜÂºïÊìé‰ºöÁî®ÈáèÂåñÁâàÊú¨Êàñ LUTÔºâ
            z = torch.sigmoid(z_pre)
            
            # [‰∏é CUDA ‰∏ÄËá¥] sigmoid ËæìÂá∫Âº∫Âà∂‰ΩøÁî® UINT ËåÉÂõ¥ÔºåÂØπÁß∞ÊÄß‰ªéÈÖçÁΩÆËØªÂèñ
            # [‰∏é CUDA ‰∏ÄËá¥] sigmoid ËæìÂá∫Âõ∫ÂÆö‰ΩøÁî® UINT (Á°¨ÁºñÁ†ÅÔºå‰∏çÂèØÈÖçÁΩÆ)
            z = fake_quantize(z, exp2_z_out, zp_z_out,
                              bitwidth=self._get_bitwidth('z_out'),
                              symmetric=self._get_symmetric('z_out'),
                              is_unsigned=True)
            
            # ========== r Èó®ÔºàReset GateÔºâ==========
            # [‰∏é CUDA ‰∏ÄËá¥] r = sigmoid(Wx_r + Rh_r + bx_r + br_r)
            r_pre = Wx_r + Rh_r + bx_r.unsqueeze(0) + br_r.unsqueeze(0)
            
            r_pre = fake_quantize(r_pre, exp2_r_pre, zp_r_pre,
                                  bitwidth=self._get_bitwidth('r_pre'),
                                  symmetric=self._get_symmetric('r_pre'))
            
            # [ONNX ÂÖºÂÆπ] ‰ΩøÁî®Ê†áÂáÜ sigmoid
            r = torch.sigmoid(r_pre)
            
            # [‰∏é CUDA ‰∏ÄËá¥] sigmoid ËæìÂá∫Âº∫Âà∂‰ΩøÁî® UINT ËåÉÂõ¥ÔºåÂØπÁß∞ÊÄß‰ªéÈÖçÁΩÆËØªÂèñ
            # [‰∏é CUDA ‰∏ÄËá¥] sigmoid ËæìÂá∫Âõ∫ÂÆö‰ΩøÁî® UINT (Á°¨ÁºñÁ†ÅÔºå‰∏çÂèØÈÖçÁΩÆ)
            r = fake_quantize(r, exp2_r_out, zp_r_out,
                              bitwidth=self._get_bitwidth('r_out'),
                              symmetric=self._get_symmetric('r_out'),
                              is_unsigned=True)
            
            # ========== g Èó®ÔºàNew Gate / CandidateÔºâ==========
            # [‰∏é CUDA ‰∏ÄËá¥] g = tanh(Wx_n + r * (Rh_n + br_n) + bx_n)
            Rh_add_br = Rh_n + br_n.unsqueeze(0)
            
            # [‰∏é CUDA ‰∏ÄËá¥] ‰∏≠Èó¥ÁªìÊûúÈáèÂåñÔºà‰ªéÈÖçÁΩÆËØªÂèñ‰ΩçÂÆΩÔºâ
            Rh_add_br = fake_quantize(Rh_add_br, quant_params.exp2_inv_Rh_add_br_,
                                      quant_params.zp_Rh_add_br_,
                                      bitwidth=self._get_bitwidth('Rh_add_br'),
                                      symmetric=self._get_symmetric('Rh_add_br'))
            
            rRh = r * Rh_add_br
            
            # [‰∏é CUDA ‰∏ÄËá¥] ‰πòÁßØÈáèÂåñÔºà‰ªéÈÖçÁΩÆËØªÂèñ‰ΩçÂÆΩÔºâ
            rRh = fake_quantize(rRh, quant_params.exp2_inv_rRh_,
                                quant_params.zp_rRh_,
                                bitwidth=self._get_bitwidth('rRh'),
                                symmetric=self._get_symmetric('rRh'))
            
            g_pre = Wx_n + rRh + bx_n.unsqueeze(0)
            
            g_pre = fake_quantize(g_pre, exp2_g_pre, zp_g_pre,
                                  bitwidth=self._get_bitwidth('g_pre'),
                                  symmetric=self._get_symmetric('g_pre'))
            
            # [ONNX ÂÖºÂÆπ] ‰ΩøÁî®Ê†áÂáÜ tanh
            g = torch.tanh(g_pre)
            
            # [‰∏é CUDA ‰∏ÄËá¥] ÊøÄÊ¥ªÂêéÈáèÂåñÔºåÂØπÁß∞ÊÄß‰ªéÈÖçÁΩÆËØªÂèñ
            g = fake_quantize(g, exp2_g_out, zp_g_out,
                              bitwidth=self._get_bitwidth('g_out'),
                              symmetric=self._get_symmetric('g_out'))
            
            # ========== Êñ∞ÈöêËóèÁä∂ÊÄÅ ==========
            # [‰∏é CUDA ‰∏ÄËá¥] h_new = z * h + (1 - z) * g
            # CUDA computeH ÂàÜÂà´ËÆ°ÁÆóÂπ∂ÈáèÂåñ old_contrib Âíå new_contrib
            
            # old_contrib = z * hÔºà‰ªéÈÖçÁΩÆËØªÂèñ‰ΩçÂÆΩÔºâ
            old_contrib = z * h
            old_contrib = fake_quantize(old_contrib, quant_params.exp2_inv_old_contrib_,
                                        quant_params.zp_old_contrib_,
                                        bitwidth=self._get_bitwidth('old_contrib'),
                                        symmetric=self._get_symmetric('old_contrib'))
            
            # new_contrib = (1 - z) * gÔºà‰ªéÈÖçÁΩÆËØªÂèñ‰ΩçÂÆΩÔºâ
            new_contrib = (1 - z) * g
            new_contrib = fake_quantize(new_contrib, quant_params.exp2_inv_new_contrib_,
                                        quant_params.zp_new_contrib_,
                                        bitwidth=self._get_bitwidth('new_contrib'),
                                        symmetric=self._get_symmetric('new_contrib'))
            
            # h_new = old_contrib + new_contrib
            h_new = old_contrib + new_contrib
            
            # [‰∏é CUDA ‰∏ÄËá¥] ËæìÂá∫ÈáèÂåñ
            h_new = fake_quantize(h_new, exp2_h, zp_h,
                                  bitwidth=self._get_bitwidth('h'),
                                  symmetric=self._get_symmetric('h'))
            
            h = h_new
            
            # ‰ΩøÁî®Á¥¢ÂºïËµãÂÄºÂ≠òÂÇ®ÔºàONNX ÂèãÂ•ΩÔºâ
            outputs[t] = h
        
        # ========== ËæìÂá∫ ==========
        output = outputs  # [T, B, H]ÔºåÂ∑≤È¢ÑÂàÜÈÖç
        h_n = h.unsqueeze(0)  # [1, B, H]
        
        return output, h_n

    def _forward_python(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Á∫Ø PyTorch ÂÆûÁé∞ÁöÑ GRU ÂâçÂêë‰º†Êí≠ÔºàÁî®‰∫é ONNX ÂØºÂá∫Ôºâ

        ÊîØÊåÅÂçïÂêëÂíåÂèåÂêëÊ®°Âºè
        """
        if self.batch_first:
            input = input.transpose(0, 1).contiguous()

        T, B, I = input.shape
        H = self.hidden_size
        device = input.device

        # ÂàùÂßãÁä∂ÊÄÅÂ§ÑÁêÜ
        h0_forward, h0_reverse = None, None
        if hx is not None:
            expected_layers = self.num_layers * self.num_directions
            expected_shape = (expected_layers, B, H)
            if hx.shape != expected_shape:
                raise ValueError(f"hx ÂΩ¢Áä∂Â∫î‰∏∫ {expected_shape}ÔºåÂÆûÈôÖ {hx.shape}")
            h0_forward = hx[0]
            if self.bidirectional:
                h0_reverse = hx[1]

        # ÂâçÂêëÊñπÂêë
        output_forward, h_n_forward = self._forward_python_single_direction(
            input, h0_forward,
            self.weight_ih_l0, self.weight_hh_l0,
            self.bias_ih_l0 if self.bias else None,
            self.bias_hh_l0 if self.bias else None,
            self.quant_params
        )

        if self.bidirectional:
            # ÂèçÂêëÊñπÂêëÔºàËæìÂÖ•ÈúÄË¶ÅÁøªËΩ¨Ôºâ
            output_reverse, h_n_reverse = self._forward_python_single_direction(
                input.flip(0), h0_reverse,
                self.weight_ih_l0_reverse, self.weight_hh_l0_reverse,
                self.bias_ih_l0_reverse if self.bias else None,
                self.bias_hh_l0_reverse if self.bias else None,
                self.quant_params_reverse
            )

            # ÂèçËΩ¨ÂèçÂêëËæìÂá∫‰ª•ÂØπÈΩêÊó∂Èó¥Ê≠•
            output_reverse = output_reverse.flip(0)
            # ÊãºÊé•ËæìÂá∫: [T, B, H] + [T, B, H] -> [T, B, 2H]
            output = torch.cat([output_forward, output_reverse], dim=-1)
            # ÊãºÊé•ÈöêËóèÁä∂ÊÄÅ: [1, B, H] + [1, B, H] -> [2, B, H]
            h_n = torch.cat([h_n_forward, h_n_reverse], dim=0)
        else:
            output = output_forward
            h_n = h_n_forward

        if self.batch_first:
            output = output.transpose(0, 1).contiguous()

        return output, h_n

    # -------------------- ‰∏ª forward ÊñπÊ≥ï --------------------

    def forward(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ÂâçÂêë‰º†Êí≠
        
        Args:
            input: [T, B, I] Êàñ [B, T, I] (batch_first) ÁöÑËæìÂÖ•
            hx: ÂàùÂßãÈöêËóèÁä∂ÊÄÅÔºåÂçïÂêë [1, B, H]ÔºåÂèåÂêë [2, B, H]
            
        Returns:
            output: [T, B, H] Êàñ [T, B, 2H] (ÂèåÂêë)
            h_n: [1, B, H] Êàñ [2, B, H] (ÂèåÂêë)

        Note:
            - export_mode=False (ÈªòËÆ§): ‰ΩøÁî® CUDA C++ ÂÆûÁé∞ÔºàÈ´òÊÄßËÉΩÔºâ
            - export_mode=True: ‰ΩøÁî®Á∫Ø PyTorch ÂÆûÁé∞ÔºàÂèØË¢´ ONNX ËøΩË∏™Ôºâ
        """
        # ===== ONNX ÂØºÂá∫Ê®°ÂºèÔºö‰ΩøÁî®Á∫Ø PyTorch ÂÆûÁé∞ =====
        if self.export_mode:
            return self._forward_python(input, hx)

        # ===== Ê≠£Â∏∏Ê®°ÂºèÔºö‰ΩøÁî® CUDA C++ ÂÆûÁé∞ =====
        self._ensure_cublas_initialized()

        # ÈáèÂåñÊ®°Âºè‰∏ãÊ£ÄÊü•Ê†°ÂáÜÁä∂ÊÄÅ
        if self.use_quantization:
            if self._calibration_dirty:
                # Ê†°ÂáÜÊï∞ÊçÆÂ∑≤Êõ¥Êñ∞ÔºåÈúÄË¶ÅÈáçÊñ∞ËÆ°ÁÆóÈáèÂåñÂèÇÊï∞
                self.finalize_calibration()
            elif not self.is_calibrated():
                # Ê£ÄÊü•ÊòØÂê¶ÊúâÊú™ÂÆåÊàêÁöÑÊ†°ÂáÜÊï∞ÊçÆÔºàÊîØÊåÅ minmax Âíå histogram ‰∏§ÁßçÊñπÊ≥ïÔºâ
                if self.quant_ranges is not None or self.hist_collectors is not None:
                    # Â∑≤Á¥ØÁßØÊï∞ÊçÆ‰ΩÜÊú™ÂÆåÊàêÊ†°ÂáÜÔºåËá™Âä®Ë∞ÉÁî® finalize
                    self.finalize_calibration()
                else:
                    raise RuntimeError("ÈáèÂåñÂ∑≤ÂêØÁî®‰ΩÜÊú™Ê†°ÂáÜÔºåËØ∑ÂÖàË∞ÉÁî® calibrate() Âíå finalize_calibration()")

        if self.batch_first:
            input = input.transpose(0, 1).contiguous()

        seq_len, batch_size, input_size = input.shape
        hidden_size = self.hidden_size

        device = input.device if input.is_cuda else torch.device('cuda')
        input = ensure_cuda_float32(input, device)

        # ÂàùÂßãÁä∂ÊÄÅÂ§ÑÁêÜ
        h0_forward, h0_reverse = None, None
        if hx is not None:
            expected_layers = self.num_layers * self.num_directions
            expected_shape = (expected_layers, batch_size, hidden_size)
            if hx.shape != expected_shape:
                raise ValueError(f"hx ÂΩ¢Áä∂Â∫î‰∏∫ {expected_shape}ÔºåÂÆûÈôÖ {hx.shape}")
            h0_forward = ensure_cuda_float32(hx[0], device)
            if self.bidirectional:
                h0_reverse = ensure_cuda_float32(hx[1], device)

        # ÂâçÂêëÊñπÂêë
        output_forward, h_n_forward = GRUFunction.apply(
            input, self.weight_ih_l0, self.weight_hh_l0,
            self.bias_ih_l0 if self.bias else None,
            self.bias_hh_l0 if self.bias else None,
            h0_forward, self.training, self.use_quantization, self.quant_params)

        if self.bidirectional:
            # ÂèçÂêëÊñπÂêë
            output_reverse, h_n_reverse = GRUFunction.apply(
                input.flip(0), self.weight_ih_l0_reverse, self.weight_hh_l0_reverse,
                self.bias_ih_l0_reverse if self.bias else None,
                self.bias_hh_l0_reverse if self.bias else None,
                h0_reverse, self.training, self.use_quantization, self.quant_params_reverse)

            # ÂèçËΩ¨ÂèçÂêëËæìÂá∫‰ª•ÂØπÈΩêÊó∂Èó¥Ê≠•
            output_reverse = output_reverse.flip(0)
            # ÊãºÊé•ËæìÂá∫: [T, B, H] + [T, B, H] -> [T, B, 2H]
            output = torch.cat([output_forward, output_reverse], dim=-1)
            # ÊãºÊé•ÈöêËóèÁä∂ÊÄÅ: [1, B, H] + [1, B, H] -> [2, B, H]
            h_n = torch.cat([h_n_forward, h_n_reverse], dim=0)
        else:
            output = output_forward
            h_n = h_n_forward

        if self.batch_first:
            output = output.transpose(0, 1).contiguous()

        return output, h_n


# ============================================================
#                      Ë∞ÉËØïÂ∑•ÂÖ∑ÂáΩÊï∞
# ============================================================

def print_quant_params(gru: QuantGRU):
    """
    ÊâìÂç∞ QuantGRU ÁöÑÈáèÂåñÂèÇÊï∞

    Args:
        gru: Â∑≤ÂÆåÊàêÊ†°ÂáÜÁöÑ QuantGRU ÂÆû‰æã
    """
    if not gru.is_calibrated():
        raise RuntimeError("ËØ∑ÂÖàË∞ÉÁî® finalize_calibration()")

    params = gru.quant_params
    print("=" * 60)
    print("GRUQuantitativeParameters (ÈáèÂåñÂèÇÊï∞)")
    print("=" * 60)
    print(f"  hidden_ = {params.hidden_}")
    print(f"  [x]  exp2_inv={params.exp2_inv_x_:3d}, zp={params.zp_x_}")
    print(f"  [h]  exp2_inv={params.exp2_inv_h_:3d}, zp={params.zp_h_}")
    print(f"  [Wx] exp2_inv={params.exp2_inv_Wx_:3d}, zp={params.zp_Wx_}")
    print(f"  [Rh] exp2_inv={params.exp2_inv_Rh_:3d}, zp={params.zp_Rh_}")
    print("-" * 60)
    print(f"  [z_pre] exp2_inv={params.exp2_inv_z_pre_:3d}, zp={params.zp_z_pre_}")
    print(f"  [r_pre] exp2_inv={params.exp2_inv_r_pre_:3d}, zp={params.zp_r_pre_}")
    print(f"  [g_pre] exp2_inv={params.exp2_inv_g_pre_:3d}, zp={params.zp_g_pre_}")
    print(f"  [z_out] exp2_inv={params.exp2_inv_z_out_:3d}, zp={params.zp_z_out_}")
    print(f"  [r_out] exp2_inv={params.exp2_inv_r_out_:3d}, zp={params.zp_r_out_}")
    print(f"  [g_out] exp2_inv={params.exp2_inv_g_out_:3d}, zp={params.zp_g_out_}")
    print("-" * 60)
    print(f"  [Rh_add_br_g]        exp2_inv={params.exp2_inv_Rh_add_br_:3d}, zp={params.zp_Rh_add_br_}")
    print(f"  [rRh]              exp2_inv={params.exp2_inv_rRh_:3d}, zp={params.zp_rRh_}")
    print(f"  [new_contrib]      exp2_inv={params.exp2_inv_new_contrib_:3d}, zp={params.zp_new_contrib_}")
    print(f"  [old_contrib]      exp2_inv={params.exp2_inv_old_contrib_:3d}, zp={params.zp_old_contrib_}")
    print("-" * 60)
    if params.exp2_inv_W_:
        print(f"  [W] exp2_inv (first 5): {list(params.exp2_inv_W_[:5])} ...")
    if params.exp2_inv_R_:
        print(f"  [R] exp2_inv (first 5): {list(params.exp2_inv_R_[:5])} ...")
    if params.exp2_inv_bx_:
        print(f"  [bx] exp2_inv (first 5): {list(params.exp2_inv_bx_[:5])} ...")
    if params.exp2_inv_br_:
        print(f"  [br] exp2_inv (first 5): {list(params.exp2_inv_br_[:5])} ...")
    print("=" * 60)


def print_quant_ranges(gru: QuantGRU):
    """
    ÊâìÂç∞ QuantGRU ÁöÑÈáèÂåñËåÉÂõ¥

    Args:
        gru: Â∑≤Ë∞ÉÁî® calibrate() ÁöÑ QuantGRU ÂÆû‰æã
    """
    if gru.quant_ranges is None:
        raise RuntimeError("ËØ∑ÂÖàË∞ÉÁî® calibrate()")

    r = gru.quant_ranges
    print("=" * 60)
    print("GRUQuantizationRanges (ÈáèÂåñËåÉÂõ¥)")
    print("=" * 60)
    print(f"  hidden_ = {r.hidden_}")
    print(f"  [x]  min={r.min_x_:12.6f}, max={r.max_x_:12.6f}")
    print(f"  [h]  min={r.min_h_:12.6f}, max={r.max_h_:12.6f}")
    print(f"  [Wx] min={r.min_Wx_:12.6f}, max={r.max_Wx_:12.6f}")
    print(f"  [Rh] min={r.min_Rh_:12.6f}, max={r.max_Rh_:12.6f}")
    print("-" * 60)
    print(f"  [z_pre] min={r.min_z_pre_:12.6f}, max={r.max_z_pre_:12.6f}")
    print(f"  [r_pre] min={r.min_r_pre_:12.6f}, max={r.max_r_pre_:12.6f}")
    print(f"  [g_pre] min={r.min_g_pre_:12.6f}, max={r.max_g_pre_:12.6f}")
    print(f"  [z_out] min={r.min_z_out_:12.6f}, max={r.max_z_out_:12.6f}")
    print(f"  [r_out] min={r.min_r_out_:12.6f}, max={r.max_r_out_:12.6f}")
    print(f"  [g_out] min={r.min_g_out_:12.6f}, max={r.max_g_out_:12.6f}")
    print("-" * 60)
    print(f"  [Rh_add_br_g]        min={r.min_Rh_add_br_g_:12.6f}, max={r.max_Rh_add_br_g_:12.6f}")
    print(f"  [rRh]              min={r.min_rRh_:12.6f}, max={r.max_rRh_:12.6f}")
    print(f"  [new_contrib]      min={r.min_new_contrib_:12.6f}, max={r.max_new_contrib_:12.6f}")
    print(f"  [old_contrib]      min={r.min_old_contrib_:12.6f}, max={r.max_old_contrib_:12.6f}")
    print("=" * 60)
